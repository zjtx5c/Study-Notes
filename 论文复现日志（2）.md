#  论文复现日志（2）

这里记录的主要是一些本人思考 idea 时借鉴的文章。

## 随笔思考

我洞察到了一个点就是，以前在 NIE 的工作GNN包括现在的 transformer 架构都只是在基于目标节点的局部邻域上处理的（几跳的范围内），包括像预处理的节点结构特征也都是基于随机游走的思想得到目标节点的局部子图从而形成的嵌入。这在同构图上当然没问题，但是我们处理的是异构图！假如说我们的目标节点 $A$ 类型为 $type_a$，但是在其采样的子图上可能不会出现也为 $type_a$ 类型的节点 $B$。事实上，节点 $A$ 与节点 $B$ 应该是有一些共性存在的，虽然消息传递机制是基于递归处理的，所以理论上即便两个节点相距多远也还是会有一些信息传递给对方（只是可能被稀释掉了），所以异质节点的这些共性我们是不是不应该放弃应该利用起来？（还是说有这一方面的研究已经展开了）

但是解决这个问题在操作上很麻烦，我们无法获取节点的具体数据类型（得花很大的功夫）而且这充其量只能算个 insight



考虑设置一种框架，使用两个维度的对齐。一种是异质类型层次的对齐，一种是多模态特征层次的对齐（图结构和文本信息）貌似可行？我们能否针对图设置出新一代的结构感知型的LLM

但是图结构和文本信息真的要对齐吗？？？



graphformer 为我们提供了一点思路？引入三种**空间编码**方式来**弥补Transformer感知图结构能力的不足**。

## idea（update）

事实上，既然已经明确==**自注意力需要的显存其实只与 $E$ 呈线性关系而不是和 $n^2$ 正相关**==，那么很遗憾，我之前的 idea 计划的动机就已经被打翻了。我以为 g_trans 是针对全图的 g_trans 因而有了将图分成子图训练的想法，但实际上 g_trans 就是针对的一跳子图即**基于邻居进行聚合**，所以上述的做法其实并不需要了。这波很伤。

那么我们还可以往哪里思考突破呢？

1. NIE 方向上的 g_trans 相关论文确实可能没有充分考虑到图的异质性 $\rightarrow$ 如何充分挖掘图中的异质性（**从异构图上突破**）

   1. 新框架？（重新设计一个 HGT 还是做一些预处理之类的工作）

      当下的 HGT 存在表达能力[35]、**过度平滑**[5]和**过度压缩**[1]等挑战，如果选择往这个方向做那就是要换赛道了...

      或者说我们**图补强一波**，发挥一下 transformers 的优势？引入图补强 + 对比学习？（事实上 HHGT 这篇文章在做 transformer 的时候就是在做图补强，Type-level 级别）

      HHGT 这篇文章的 Figure-4 也启发我们异质性区分的重要性，然而目前的 NIE 工作上对异质性的利用还是比较欠缺的。

      以往的文章对边类型的嵌入都是随机一个 `nn.Embedding` ，HHGT 这篇文章甚至都**没有对异构图的边类型**进行很好地处理（这一点我们也可以好好利用）

   2. 新理论？

      在 EASING 这个实验上，貌似在减少 hidden-dim 但是在启用批出批处理的情况下，跑出来的效果要更好？并且异构图感觉就是可以天然地去分图

2. 我们将目标瞄到异构图的大规模图的训练上，那么势必要用到图采样 + 批处理技术。之前的做法是随机采样目标节点，但是我们是在大规模的异构图上，所以直觉上感觉这种做法其实是比较莽撞的，原因如下（可能有误）：

   1. （训练稳定性角度）打乱节点顺序和在小批量中处理节点，可能会导致每次训练的梯度计算**基于不同的子图**。这样会使得训练过程中的梯度波动较大，因为每个 mini-batch 内的节点信息局部性较强，**无法全面反映全图结构**。因此，训练的收敛速度可能较慢，需要更多的训练轮次来达到与 full-batch 相同的效果。训练的效果也可能受到批次大小和打乱顺序的影响，我们可能需要一定的策略来保证每个 batch 能代表整体图的结构。
   2. （异构图复杂性角度）在 mini-batch 中，每个 batch 的节点和其邻居可能来自图的不同部分，并且包含不同类型的节点和边。因此，打乱节点顺序可能导致某些类型的节点信息不足，影响异构图中各类信息的均衡学习。为此，一些方法会使用采样策略（如基于节点类型或边类型的采样）来保持训练中各类节点的代表性。

   我们是否可以考虑在这一个方向优化，请仔细思考

3. 从半监督学习上的伪标签上突破，EASING 那篇文章上，学长和老师都敏锐地==更想要关注训练得到的伪标签的质量，我们是否可以从这里寻找创新点和突破？==

   针对异构图构造分层图？在这些分层图上进行学习 batch，然后利用一种模型将这些 batch 上的东西进行融合和对齐，学习到更吊的嵌入？（口胡的）

   > **伪标签生成 + 一致性正则化（Consistency Regularization）**
   >
   > ==**教师-学生框架（Mean Teacher, Co-training）**==（蒸馏有没有说法？）
   >
   > **基于置信度或异方差建模的伪标签筛选**

   学习一下生成式方法（Generative Modeling）

   ==个人感觉使用 LLM 方法来提高为标签的质量是十分可行的，因为很多 GNN 节点存在“冷启动”问题，这时文本就会成为重要补充！且 LLM **具有很强的泛化能力与预训练能力**。==

   想到的做法如下：

   1. 通过 LLM 与 GNN 的**双向对齐/互生成伪标签**，提高伪监督质量
   2. 

4. 还有一个更大胆的 Idea，利用生成式。利用文本描述信息结合 LLM 瞎搞一波

   这个通过阅读 LLM & Graph 的综述，是完全可行的

   ~~==**考虑使用 Bert，利用现有的文本数据生成更强的语义嵌入数据？可以先开始进行实验！先小消融一波，换一手生成的语义嵌入，用这个作为语义特征去跑，看看和之前的结果有没有提升。**==~~

   或者：

   > 从 LLM 自动生成高质量节点描述（数据增强）
   >
   > Textual Prompting：为图节点设计 prompt 模板，用 GPT/T5 补全上下文
   >
   > Graph2Text（借助 GPT 自动生成上下游关系描述）再反编码为嵌入

   但是这个东西只是在预处理范畴。感觉创新性其实是欠缺的。

   事实上 《A Fine-Tuning Approach for T5 Using Knowledge  Graphs to Address Complex Tasks》 这篇文章给了我们处理 将图的结构化知识更好地融入模型中的手段，目的是可以让模型从文本信息中结合图更好地理解图的结构信息。

   这样的话我们考虑做一个基于 Bert 模型增强图的结构与语义嵌入的框架 用来增强 NIE 任务？？

   **能不能做到在生成更高质量的节点特征嵌入和节点文本嵌入的同时，将这两个语义空间也对齐啊**但是我们首先得明确这两个嵌入之前是怎么得到的？

   文中的相关段落是这样的

   > 节点结构特征是从node2vec[9]中获得的，其目的是捕捉相邻节点的共现，并为GNN编码器提供更大的图中接受域。对于节点描述，Transformer XL[6]用于将文本嵌入到节点语义特征中。在通过两个图编码器后，采用共同关注融合机制来交互结构特定特征和语义特定特征。然后将这些特征分别投影到重要性值中，并与注意力权重聚合，得到节点的最终重要性值。最后，使用学习排序（LTR）损失和均方根误差（RMSE）来训练整个模型。
   >
   > 我们首先为输入知识图制作两个副本，如图2所示，一个使用node2vec中关注图拓扑的特征X（s）作为节点初始化特征，另一个使用Transformer XL编码的关注节点语义的文本特征X（c）作为初始化特征。谓词特征是随机初始化的，表示为P。

   等等！这个文本嵌入是不是就是通过 LLM 的结果获取的。。。。。并不是，这只是一个改进版的 transformer 框架。

   > Transformer-XL 是一种支持跨 segment 的上下文建模能力的 Transformer，通过引入“循环记忆机制”和“相对位置编码”，可以处理比标准 Transformer 更长的序列。

   而 node2vec 是

   > **Node2Vec = 图上的随机游走 + Skip-Gram 对比学习**，用**局部共现结构训练节点向量**，是一种无监督图嵌入方法。

5. 提出更强的对图进行建模的范式

   GNN 对图的结构信息（**拓扑结构**）捕捉能力更强，但是不善于处理图的文本信息。LLM 对文本信息（**序列结构**）处理能力更强，但是不善于处理图的一些拓扑结构。搞出一种框架，使得该模型既能够轻易理解图中额文本内容，又能轻易理解图中的拓扑结构，**然后将这两个语义空间对齐或者做一些别的操作**

   结合两者可以提升**表示能力**和**推理能力**

6. LLM + GNN 结合的创新点（GPT 画饼版）

   > 非常有！将 **GNN-LLM 对齐** 应用于你的 **节点重要性评估（Node Importance Estimation, NIE）** 任务，不仅有理论意义，也有实用价值。下面我结合你的场景详细分析它的**启发与帮助**：
   >
   > ------
   >
   > ## 🧠 一、NIE 面临的挑战回顾
   >
   > NIE（尤其在异构图上）本质上是在评估某个节点在图结构中的重要性，比如代表用户、内容、实体等的关键性。你之前提到过的问题包括：
   >
   > | 挑战       | 描述                                                 |
   > | ---------- | ---------------------------------------------------- |
   > | 结构偏弱   | 图结构可能稀疏、局部连接性差，GNN 单独建模能力有限   |
   > | 异质性强   | 节点类型多样，语义差异大                             |
   > | 监督信号弱 | NIE 往往标注稀少，依赖伪标签与无监督信息             |
   > | 表征孤岛   | **不同模态（如图结构 vs 文本属性）表征无法统一处理** |
   > | 泛化困难   | 新节点或少连接节点很难估计其重要性                   |
   >
   > ------
   >
   > ## 🚀 二、GNN-LLM 对齐如何缓解这些挑战？
   >
   > | 挑战             | GNN-LLM 对齐的帮助                                          |
   > | ---------------- | ----------------------------------------------------------- |
   > | **结构偏弱**     | LLM 可通过节点文本/属性语义弥补结构缺失的上下文             |
   > | **异质性强**     | 统一图与文本空间后，异构节点可以在语义空间内比较重要性      |
   > | **伪标签难生成** | 通过 LLM 与 GNN 的**双向对齐/互生成伪标签**，提高伪监督质量 |
   > | **节点冷启动**   | LLM 可以仅依赖文本生成重要性估计，提供 cold-start 能力      |
   > | **表达力不足**   | 对齐 LLM 表征后，GNN 获得更强泛化与语义建模能力             |
   > | **可解释性弱**   | LLM 表征可驱动自然语言解释（如：“因为内容 A 的高关注度…”）  |
   >
   > ------
   >
   > ## 🔧 三、在 NIE 中如何具体应用对齐？
   >
   > 你可以考虑如下几种方式：
   >
   > ### 1. **表示对齐式监督**
   >
   > - 构造 $L_{\text{align}} = \text{KL}(P_{\text{GNN}}(v) \parallel P_{\text{LLM}}(v))$
   > - 强化节点表征的一致性，尤其是对重要节点的建模
   > - 对未标记节点可互为伪监督源 → 增强伪标签质量
   >
   > ### 2. **双模态伪标签生成（Iterative Alignment）**
   >
   > - LLM 给出文本属性推导的重要性伪标签，监督 GNN
   > - GNN 根据结构给出 importance 排名，引导 LLM 表征更新
   > - 双向交互 → 提高对未标记节点的预测质量
   >
   > ### 3. **冷启动节点的重要性估计**
   >
   > - 新节点（结构缺失）可由 LLM 估计初始重要性值（文本、属性生成）
   > - 与结构节点融合，完成 GNN 重要性打分
   >
   > ### 4. **异构类型的对齐建模**
   >
   > - 对于异构节点（如用户、内容、商品），构建 GNN 表征 + 文本嵌入
   > - 通过对齐学习到统一重要性判断尺度
   >
   > ------
   >
   > ## 💡 四、可以考虑的研究创新点
   >
   > | 创新点方向                                   | 说明                                            |
   > | -------------------------------------------- | ----------------------------------------------- |
   > | ✨ 异构图上的 GNN-LLM 对齐框架                | 基于异构类型自适应对齐策略                      |
   > | ✨ 图结构驱动的语言提示优化（Prompt for NIE） | 用图重要性提示 LLM 生成文本或判断重要性         |
   > | ✨ 跨模态伪标签互补机制                       | GNN 与 LLM 互生成伪标签，提高无监督精度         |
   > | ✨ 可解释性增强                               | 利用 LLM 解释 GNN 的判断，如“此节点重要是因为…” |
   >
   > ## ✅ 总结一句话：
   >
   > > **GNN-LLM 对齐为你的 NIE 任务提供了强大的语义补充能力，尤其能提升伪标签质量、冷启动节点估计、异构图建模与模型泛化能力，是值得深入研究的融合方向。**
   >
   > 





## HHGT

[链接](https://dl.acm.org/doi/pdf/10.1145/3701551.3703511)

2024.7

### ==思考与感悟==

#### idea

这篇文章给我和我之前的一个想法有点相似：先局部 transformer 再全局 transformers



==我发现针对 HGT （异构图 transformer）的方法所遵循的原则和 dgl 的 block 机制（图采样 + 批处理手段）的底层逻辑几乎是一样的==。最大的区别是：HGT 强调 **异构类型**，DGL Block 是框架机制，但支持异构图处理。

那么针对我的 NIE 方向，我是不是可以通过**改进预处理**图的结构来使整个任务性能变得更好？

这篇文章指出当下 HGT 的手段与缺陷（1）不区分距离（2）不区分类型，所以我们需要关注的是：transformers 对异构信息的识别能力如何？直接喂给它一个异构图它能否准确地判断异构信息？若我们使用某种策略来人为地区分图上的异构信息（一种预处理手段），transformer的能力是否会增强。还是说我们直接再次设计一种能够天然识别针对图异构信息的 transformers 框架？（但是这项工作前人已经做过了）。

事实上，我们可以将目标放在大规模图上，那么进行训练的话就离不开 dgl 的 block 机制，这和之前HGT遵循的准则是一样的，这个操作顺便还能解决显存问题。所以我们可以考虑将重心放在预处理的框架上？？



transformers 能否自己学习到异构结构？

下面两个公式分别是 transformers 和 GAT 的异构图聚合和注意力计算公式（NIE 方向上）

下面这个公式是第 $h$ 个头的两点之间的注意力公式的计算

RGTN、SKES、EASING 均使用了下式这种聚合注意力计算公式（transformers）
$$
w^{(l),h}_{y \rightarrow x} = 
\frac{
\left( W^{(l),h}_Q H^{(l)}_x \cdot \left( W^{(l),h}_K H^{(l)}_y \right)^\top \right)
\cdot W^{(l),h}_E E^{(l)}_{y \rightarrow x}
}{
\sqrt{d}
}
$$

$$
\alpha_{y \to x}^{(l),h} = 
\sum_{e \in \mathcal{E}(y, x)} 
\frac{
    \exp\left( w_{y \xrightarrow{e} x}^{(l),h} \right)
}{
    \sum\limits_{y' \in \mathcal{N}(x)} \sum\limits_{e' \in \mathcal{E}(y', x)} \exp\left( w_{y' \xrightarrow{e'} x}^{(l),h} \right)
}
$$

___

GENI、MultiImport、EASING均使用了下面面这种聚合注意力计算公式（GAT）

> $\mathbf{h}_i$ 和 $\mathbf{h}_j$ 分别表示汇点和源点的隐向量， $\alpha$ 是一个可学习的注意力权重系数，学习并度量目标节点对该边的依赖强度

$$
\omega_l(i, j, m) = 
\frac{
    \exp\left( \mathrm{LeakyReLU}\left( \mathbf{a}_l^\top [ \mathbf{h}_{i}^{l-1} \| \pi(\rho_{ij}^{m}) \| \mathbf{h}_{j}^{l-1} \right]) \right)
}{
    \sum_{(k, n) \in \mathcal{N}(i)} 
    \exp\left( \mathrm{LeakyReLU}\left( \mathbf{a}_l^\top [ \mathbf{h}_{i}^{l-1} \| \pi(\rho_{ik}^{n}) \| \mathbf{h}_{k}^{l-1} \right]) \right)
}
$$

前者是**点积注意力**，用两个变换（Query/Key）产生向量，然后直接计算相似性，**能够自然捕捉高阶结构相似性、语义相似性、空间相似性**，并且**在理论上是通用函数逼近器**

后者是**加性注意力**，用一个向量 $\mathbf{a}$ 和拼接后的表示做一次投影 + 激活函数，属于浅层打分方式（Single-layer MLP 近似），**表达能力弱、结构简单**。

上面的就是 GPT 口胡的，两者的区别和联系的深刻理解 我具体也还不知道





transformers 的优越性有

**表达能力更强**（点积注意力能建模更复杂的语义关系）；

GAT 是**加性注意力**，用一个向量 $\mathbf{a}$ 和拼接后的表示做一次投影 + 激活函数，属于浅层打分方式（**Single-layer MLP 近似**），**表达能力弱、结构简单**。

Transformers 是**点积注意力**，用两个变换（Query/Key）产生向量，然后直接计算相似性（该相似性是基于这一跳子图中目标节点及其邻居入点的”全局“视角的），**能够自然捕捉高阶结构相似性、语义相似性、空间相似性**，并且**在理论上是通用函数逼近器**

尽管 Transformer-style 注意力机制在理论上具有较强的表达能力，但在图神经网络中，它并不具备 LLM 中 Transformer 的那种显著的计算效率和扩展性优势。在 dgl 框架下，其原因在于，图结构是天然稀疏的，注意力仅在目标节点与其邻居之间计算，而非全图 token 之间的全连接计算。即使采用 full-batch 训练，DGL 等稀疏图框架下的计算也采用基于边的消息传递机制，不会构造显式的 $N \times N$自注意力矩阵，因此无法像 LLM 中那样使用高效的矩阵乘法加速（如 FlashAttention）。这一结构限制使得图上的 Transformer 无法在并行性和计算效率上获得与 NLP 中 Transformer 相同的优势。但是这不重要。既然是这样，那我们为什么会爆显存？仔细分析 `EASING` 中的 `GTLayer` 代码模块可以知道，内存复杂度随节点和边数是**线性增长**的，所以即便是在全图视角下怎么会爆内存呢？首先需要明确一点的是：**数据中所有的节点都会用到！因为我们为所有的节点都创建了结构特征与语义特征，但是只有少部分节点有标签。**

不懂啊，为什么会爆显存啊。刚测试了一下，使用自己能在服务器上跑的参数与FB15k数据集去训练模型，发现一个模型第一次反向传播后会用到 2G，继续深究

边[607164, 10]和节点[14951, 64]特征加起来约为 24MB

q, k, v[14951, 4, 8] 加起来大约 10M

`graph.apply_edges(fn.v_dot_u('q', 'k', 't'))` 约 9M（4B存）

`attn_score = graph.edata.pop('t').sum(-1) / self.sqrt_dim  # [n_edge, n_head]` 约 9M（4B存）

```python
attn_score = self.attn_drop(edge_softmax(graph, attn_score.unsqueeze(-1), norm_by='dst'))

graph.edata.update({'t': attn_score})
```

  30MB

后续的 前馈神经网络 一共也就加了 20MB，不仔细细算了。

反正截至到计算 struct_h 的编码器的两层一共是223.01MB正常

content_h 则是 500.90MB



解码器部分一计算 x_m 和 x_v 直接变成 2361 MB，接下来细究

```python
s = torch.bmm(self.p_s, q_input) # [B, N, 2d]
u = torch.bmm(self.p_u, q_input) # [B, N, 2d]
```

花费了 70MB（4B），这里是 `[14951, 10, 64]`，我们将 N 设置为10， 我们的 num_hidden 设置为 8 ，heads 设置为 4，这里的 2d 就是 4 * 8 * 2 = 64



计算完下述这6个矩阵，增加了216MB

```python
qs = self.w_qs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
ks = self.w_ks(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
vs = self.w_vs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]

qu = self.w_qu(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
ku = self.w_ku(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
vu = self.w_vu(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
```

[14951, 4, 10, 16]



60MB

```
ua_s_attn = torch.matmul(qs / self.sqrt_d, ks.transpose(-2,-1)) # [B, heads_num, N, N]
```

[14951, 4, 10, 10]



后面的不细算了，反正一层前向 DJE 大概是700MB。我们还用了两个模型，每次采样还要再重复一整个前向过程。这么一累计确实大了。所以问题终于得到了解决，和 transformers 的自注意力机制无关（==**自注意力需要的显存其实只与 $E$ 呈线性关系而不是和 $n^2$ 正相关**==），就是单纯的模型参数多（尤其是解码部分） +  数据量大（整图 + 大图） + 常数大（模型多用）



上述这种transformers 的 attention 构造方式**能有效建模==单跳==邻接关系中的异构性**，适合做类型感知的局部信息聚合；但它的表达能力受限，**不能捕捉复杂、跨边/跨跳/多元异构路径下的语义交互**。（**我所见的论文中 NIE 方向相关的 transformer 建模在聚合层面确实只关注一层的信息，但不排除其他 HIG 方向的论文有跨边/跨跳/多元异构路径下的建模**）

```python
# 对异构图边注意力机制的处理
if self.edge_mode == 'DOT':
    rel_feat = self.e_linear(edge_feat).reshape(-1, self.n_heads, self.out_dim)
    graph.edata.update({'rel': rel_feat})
    graph.apply_edges(lambda edges: {'e': edges.src['q'] * edges.data['rel']})
    rel_attn = graph.edata.pop('e').sum(-1) / self.sqrt_dim
    attn_score = attn_score + rel_attn
elif self.edge_mode == 'MUL':
    edge_attn = self.e_linear(edge_feat)  # [n_edge, n_head]
    attn_score = attn_score * edge_attn
elif self.edge_mode == 'ADD':
    edge_attn = self.e_linear(edge_feat)  # [n_edge, n_head]
    attn_score = attn_score + edge_attn
elif self.edge_mode == 'GATE':
    edge_attn = self.e_linear(edge_feat)
    attn_score = attn_score * edge_attn
    attn_score = self.attn_drop(edge_softmax(graph, attn_score.unsqueeze(-1), norm_by='dst'))
```



理论上 transformer 有 capacity，但存在问题：

| 问题               | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| **异构类型弱建模** | 多数 HGT 架构中 type embedding 只是一个加性偏置？（反正我见到的都是乘一个系数即直接对边类型嵌入得到一个乘法系数加入到自注意力的计算当中），事实上，做法有很多具体见上面的公式和代码。是否能建模复杂关系呢（在 NIE 这个方向上的 Gtrans 确实是聚合时只关注周围一层，而没有捕捉复杂、跨边/跨跳/多元异构路径下的语义交互） |
| **距离信息缺失**   | 由于 attention 本身不感知 hop distance，需要额外引入 positional bias |
| **图结构信息稀疏** | **大图场景中，局部结构不完整，Transformer 表现力过剩，容易过拟合或失效** |

我们具体来理解一下这个问题==大图场景中，局部结构不完整，Transformer 表现力过剩，容易过拟合或失效==

> 这句话出现在图神经网络（GNN）或图Transformer等相关研究中，传达了一个**图结构建模中的常见问题**。我们逐句解释：
>
> ✅ 原句拆解：
>
> > **图结构信息稀疏**
>
> 👉 指的是图中**边的连接较少**，或者**节点之间的关系不完整**。
>  例如，在一个社交网络中，如果你只能看到一个用户的极少朋友，你就很难推断他的行为或兴趣。
>
> ------
>
> > **大图场景中，局部结构不完整**
>
> 👉 在大规模图（如百万级节点的图）中，对每个节点你通常**只能观测其局部的邻居**，而且这些邻居可能本身也缺乏足够信息（例如标签或特征），导致这个节点的“局部结构”是不完整的。
>
> ------
>
> > **Transformer 表现力过剩**
>
> 👉 指的是 Transformer 模型的表达能力非常强（**因为它可以建模任意两个节点之间的关系**），但在图结构中如果数据本身很“稀疏”、信息很少，这种强模型就**没有足够的数据支撑其学习能力**，反而容易出问题。
>
> ------
>
> > **容易过拟合或失效**
>
> 👉 因为训练数据中的图结构太稀疏、局部信息不完整，导致模型学到的是**噪声而不是规律**，最终在测试时表现差。这叫做 **过拟合**。
>  更极端的情况是模型根本没学到东西 —— 这就叫 **失效**。
>
> 📌 举个通俗的例子：
>
> 你有一张社交图（图结构），每个人是一个节点，他们之间有“认识”这个边。
>
> - 如果你只能看到某个人的 1～2 个朋友（信息稀疏），
> - 而你用一个复杂的 Transformer 模型试图去推理他的兴趣（表现力过剩），
> - 最终可能模型会胡乱“猜”或者只记住训练集中的特例（过拟合），在新的人身上完全失败（失效）。（**这部分存疑，transformer 性能很强，这并不能说明它在稀疏图上表现很弱**）
>
> ✅ 总结：
>
> 这句话的意思是：
>
> > 在大规模图中，由于每个节点的邻居信息很有限（稀疏/不完整），直接使用高表达能力的模型（如 Transformer）反而可能适得其反，容易陷入过拟合或学习失败。
>
> ------
>
> 如果你是做图学习相关的任务（比如图推荐、节点分类、节点排序等），这句话提醒你要**注意模型复杂度与图数据结构之间的匹配**，必要时可能需要：
>
> - 图结构增强（如图补全）
> - 模型简化（降低参数量）
> - 增强监督信号（如伪标签、蒸馏）
> - 局部感受野限制（如采用 GNN 或局部 attention）



事实上，既然已经明确==**自注意力需要的显存其实只与 $E$ 呈线性关系而不是和 $n^2$ 正相关**==，那么很遗憾，我之前的 idea 计划的动机就已经被打翻了。我以为 g_trans 是针对全图的 g_trans 因而有了将图分成子图训练的想法，但实际上 g_trans 就是针对的一跳子图即**基于邻居进行聚合**，所以上述的做法其实并不需要了。这波很伤。

那么我们还可以往哪里思考突破呢？

1. NIE 方向上的 g_trans 相关论文确实可能没有充分考虑到图的异质性 $\rightarrow$ 如何充分挖掘图中的异质性（**从异构图上突破**）

   1. 新框架？（重新设计一个 HGT 还是做一些预处理之类的工作）

      当下的 HGT 存在表达能力[35]、过度平滑[5]和过度压缩[1]等挑战，如果选择往这个方向做那就是要换塞到了...

      或者说我们**图补强一波**，发挥一下 transformers 的优势？引入图补强 + 对比学习？（事实上 HHGT 这篇文章在做 transformer 的时候就是在做图补强，Type-level 级别）

      HHGT 这篇文章的 Figure-4 也启发我们异质性区分的重要性，然而目前的 NIE 工作上对异质性的利用还是比较欠缺的。

      HHGT 这篇文章貌似并没有对异构图的边类型进行很好地处理（这一点我们也可以好好利用）

   2. 新理论？

      在 EASING 这个实验上，貌似在减少 hidden-dim 但是在启用批出批处理的情况下，跑出来的效果要更好？并且异构图感觉就是可以天然地去分图

2. 我们将目标瞄到异构图的大规模图的训练上，那么势必要用到图采样 + 批处理技术。之前的做法是随机采样目标节点，但是我们是在大规模的异构图上，所以直觉上感觉这种做法其实是比较莽撞的，原因如下（可能有误）：

   1. （训练稳定性角度）打乱节点顺序和在小批量中处理节点，可能会导致每次训练的梯度计算**基于不同的子图**。这样会使得训练过程中的梯度波动较大，因为每个 mini-batch 内的节点信息局部性较强，**无法全面反映全图结构**。因此，训练的收敛速度可能较慢，需要更多的训练轮次来达到与 full-batch 相同的效果。训练的效果也可能受到批次大小和打乱顺序的影响，我们可能需要一定的策略来保证每个 batch 能代表整体图的结构。
   2. （异构图复杂性角度）在 mini-batch 中，每个 batch 的节点和其邻居可能来自图的不同部分，并且包含不同类型的节点和边。因此，打乱节点顺序可能导致某些类型的节点信息不足，影响异构图中各类信息的均衡学习。为此，一些方法会使用采样策略（如基于节点类型或边类型的采样）来保持训练中各类节点的代表性。

   我们是否可以考虑在这一个方向优化，请仔细思考

3. 从半监督学习上的伪标签上突破，EASING 那篇文章上，学长和老师都敏锐地更想要关注训练得到的伪标签的质量，我们是否可以从这里寻找创新点和突破？

   针对异构图构造分层图？在这些分层图上进行学习 batch，然后利用一种模型将这些 batch 上的东西进行融合和对其，学习到更吊的嵌入？（口胡的）

   > **伪标签生成 + 一致性正则化（Consistency Regularization）**
   >
   > ==**教师-学生框架（Mean Teacher, Co-training）**==（蒸馏有没有说法？）
   >
   > **基于置信度或异方差建模的伪标签筛选**

   学习一下生成式方法（Generative Modeling）

4. 还有一个更大胆的 Idea，利用生成式。利用文本描述信息结合 LLM 瞎搞一波

   这个通过阅读 LLM & Graph 的综述，是完全可行的

5. 提出更强的对图进行建模的范式

   GNN 对图的结构信息（**拓扑结构**）捕捉能力更强，但是不善于处理图的文本信息。LLM 对文本信息（**序列结构**）处理能力更强，但是不善于处理图的一些拓扑结构。搞出一种框架，使得该模型既能够轻易理解图中额文本内容，又能轻易理解图中的拓扑结构，**然后将这两个语义空间对齐或者做一些别的操作**



目前最新的思考如下：

* 洞察点1

1. 首先是有一个洞察点，就是在针对异构图的 NIE 的任务中，每个目标节点在更新信息时（无论是使用 GNN 框架还是 transformers 框架），均未利用到全局中同类型节点的信息，并且可能有些目标节点其同配性较低，也就是说该结点与其邻居特性较为吻合（事实上，这个早在 2021 年就有人提出来了[知乎链接](https://zhuanlan.zhihu.com/p/380639504)，这项工作的核心思想是：**通过改变异配性高的图的拓扑结构，根据节点的相似度生成一个新的邻接矩阵**，**帮助 GNN 从同一类节点中获取信息**。）

   > 也就是说，目标节点的更新仅依赖于局部节点，若是对于同构图来说，则是很合理的，但是针对异构图来说。类型不同的节点需要进行区分并且类型相同的节点需要进行比较？
   >
   > 同时我们还要注意区分**异质性与异配性的区别**

2. 将上述点精简和融合一下可以这样说

   在异构图中的节点重要性估计（NIE）任务中，我们希望捕捉：

   * 同类型节点之间的**语义共性**
   * 超越局部邻域的**结构性依赖**（即使得 GNN 能够捕获长距离外的节点（不仅仅局限于两跳以内））

   > “现有异构图模型仅利用局部邻居更新，缺失同类节点的语义共性表达与非邻接结构依赖建模，限制了节点重要性评估的准确性。”

   传统的 GNN （或者说 graph transformer）的消息传递受限于“邻接图的递归聚合”，而这个共性信息往往存在于图中远距离甚至无边链接的节点之间。如何理解这段话呢？

   我们举一个例子就能理解

   > 假设我在研究一个**学术异构图**，其中节点包括：作者（Author）、论文（Paper）、会议（Venue）等。
   >
   > 目标节点是一个作者 A。现在传统 GNN 模型只会聚合 A 的邻居节点（例如他写的论文、合作者、投过的会议等），也许还会通过两跳邻居看到间接合作的人。
   >
   > 但是：
   >
   > - 另一位作者 B，虽然没有与 A 合作过，也没有任何路径连接；
   > - 但 B 研究的主题与 A 非常相似，发在相同的会议，引用过相似的论文；
   > - 从语义上来看，他们是“同类人”，他们之间具有同类型节点之间的**语义共性**但 GNN 没法捕捉到；
   > - **他们在图结构上“很远”，但在功能上“很近”**——这就属于“**超越局部邻域的结构性依赖**”。
   >
   > 而传统的框架，受限于内存资源以及图的结构，它们仅会捕捉到目标节点一两条邻域内的信息，这对同构图当然是自然的，但是在异构图上并没有充分利用异构图的信息。==**并且 HHGT 这篇文章也指出现有的基于HGT的方法倾向于混合不同类型的节点，并在邻居聚合过程中统一处理k-hop邻域内的所有节点，从而导致潜在的语义混淆。**==
   >
   > ==所以我们不仅要关注同类节点，对异类节点也要做一些处理==
   >
   > ==如何区分同类节点与异类节点？使用轻便的 prompt 进行，让 LLM 进行学习？==
   >
   > ==总的来讲，我们需要解决的是拓扑 + 异构信息==

   这两个满足条件的同类型节点应该是能够再次挖掘它们之间的信息的，比如我们可以在训练时加入一个“同类一致性损失”或“语义对齐损失”。甚至结合蒸馏或对比学习的策略。（但是这样是不是会往分类任务上引导而非回归任务上引导？）

   > * 同类一致性损失：
   >
   > 对于**同一类型（同一类）节点/样本/模态内部的表示**，我们希望它们在嵌入空间中表现出一致性或接近性。在异构图中：同类型节点（如两个作者节点）在结构上或语义上相似，那么它们的表示应该**相近**。否则就会出现：作者A和作者B明明都是重要作者，但编码结果差异很大，影响下游任务
   >
   > * 语义对齐损失
   >
   > 对于不同模态之间**语义相同的表示**，希望它们的向量嵌入彼此对齐（即，跨模态对齐）。
   >
   > 就是同一个节点的 struct_feat 要尽量和 content_feat 对齐（这一点存疑，需要进行实验验证）

3. 那么如何寻找拥有同样类型节点呢（假如在节点类型标签丢失但是节点描述存在的情况下）并且如何将 LLM 和我们的具体任务与我们思考出的 Idea 结合利用起来呢？（这部分得看点文章啊）

   我们可以考虑使用 LLM 。利用 LLM 建立语义相似图？？

   把 LLM 当做标签补全器或伪标签 teacher，辅助训练 GNN？

   或者考虑使用==虚拟节点==（这貌似是一个常见的 trick），将具有同类型节点的点连向一个对应的虚拟节点？（该节点聚合同类型的节点）然后呢？不知道了。。。

   使用虚拟节点后，图的两跳领域范围内的信息将会被较大地扩充，即信息会变得更多，更关键的一点是，虚拟节点的引入会使得原本只关注邻域的 GNN 在一定程度上拥有全局的视野！我大胆认为这是革命性的改变（只是不知道有没有工作提出过跟我类似的观点，早就提出来过了，而且别人是直接连全部节点）。
   
   ```
   某节点描述如下：xxx  
   它的连接关系如下：a -> b, a -> c, ...  
   请根据你的知识判断它的重要性等级（1-5）？
   ```

   
   
   但是我们还是遇到了问题，就是目前的数据集 TMDB 貌似只有一种节点类型（电影）,imdb有两种节点类型（电影、演员），FB15K则非常多。节点类型数量太少，我们使用虚拟节点的话不会出现太多的虚拟节点，这样的做法是不是太泛化了，我们可能区分不出同类型节点之间的差异了。（这部分留着）
   
   
   
   **还有一个点就是我们不让 LLM 做所谓的嵌入编码器，而是直接让他作为决策者参与到整个任务过程当中**。

* 洞察点2

1. 假如一个节点A 和 节点B 。这两个节点的形状（比如说指包含自己两跳范围内的邻域子图几乎一样）但是两个节点的语义完全不一样，节点A代表的是演员 节点 B 代表的是电影。 对它们做语义对齐是有意义的吗？

   很显然，直观上： **直接做语义对齐是没有意义的，甚至是有害的，除非对齐方式能区分节点类型。**

   > 在图结构学习中，尤其是 node2vec、GCN 这类结构为主的模型里，两个节点只要**邻接模式**相似，它们就会被编码成相似的向量。
   >
   > 此时如果你进行 embedding 的“**结构语义对齐**”，会面临：
   >
   > - 对齐方式无法区分 actor/movie，导致语义污染
   > - 会错误强化结构主导的 embedding，相当于忽视语义角色
   > - 最终 NIE 模型会失去**类别敏感性（type-awareness）** 模型会“认为” A 和 B 是可替换或类似的节点，严重影响基于语义的任务

   GPT 给了一个做法，使用交叉注意力机制 `cross_attention`

   > - 使用结构嵌入作为 query，语义作为 key/value：
   >
   > ```python
   > z_fused = cross_attention(z_struct, z_sem)
   > ```
   >
   > - 可以抑制掉不相关语义，而不是直接拼接导致混淆

   或者说我们更希望将具有同样结构并且具有大致语义（节点类型差不多）的节点嵌入对齐

2. 当下我看到的前沿文章 比如 EASING、SKES 有没有注意到或者说解决这个问题

   EASING 在处理时进行的是拼接操作，这样做是不好的。但不确定后续的编码或者解码结构能否将它们纠正过来。

   SKES忘了，明天去补了

3. 其实我们人类判断 NIE 的逻辑是。先根据这个图的结构做初步判断，然后再结合语义做调整。所以在异构的意义下，我认为相较于图结构，这个节点本身的背景信息也是相当重要的。那么我思考了一下，尝试以图结构为辅助，直接将该图的邻域内所有的边 + 节点看成一组文本。如果能够实现的话就能将图的拓扑结构转化为序列结构（这更适合 LLM 进行处理），**难的正是如何将图的拓扑结构转化为序列结构，建立一种上下文关系**



* 洞察点3

我们将注意力聚焦到对节点语义特征嵌入与结构嵌入的做法上

1. 之前的工作是怎样将结构特征与语义特征扔进 transformer 作为初始输入的呢？

   EASING 是直接将两者 cat 拼接。感觉这样的做法并没有考虑到结构特征与语义特征其实是两个多模态特征，**不知道 transformer 在工作时能否将它们区分（存疑）**

   RGTN 的做法比较合理一些，但他是在使用两个模型共同工作（就是浪费了一点空间资源），对这个结构进行了特征增强的处理（其实就是做了**交叉注意力**），只不过是在编码完成之后提到了后面进行。

2. 我想表达的是能不能对这两个特征进行合理地处理之后在扔入 LLM / transformer？讲得更具体一点是利用结构信息辅助，获取更多的文本信息。理想的状态是 结构来引导语义（因为语义本身就含有异构信息，可以让 LLM 自己识别）且结构辅助语义





==但是有一个很色的点就是做着做着我发现貌似数据用的居然是同构数据吗（EASING！！！）如果是同构数据那么全部炸开==核心是数据。

大概知道了，EASING那篇文章所用的数据都是电影节点的数据（也就是说这个TMDB所构的图其实是**同构图**）FB15K倒是数据很齐全。但是没用啊。TMDB的数据可以自己去提取（也可以直接不管）IMDB只有电影和人物两种类型的数据。其实还是可做的感觉。

我们考虑的做法是不定量做，而是定性选择 k 或者其他的什么操作处理。而是先根据语义节点进行划分 k 个虚点，然后进行相连。（这样也能增强一些泛化性）这样的话两跳邻域内的这个图就会变得很密集，比较适应 transformers 适用的紧凑性（但不知道有没有效）

真没想到要死在数据上了。。

另外，LLM还能在已有 Transformer-based 图嵌入方法上带来创新吗？ **LLM 仍有创新空间，但方向需要避开简单的“语义编码器替换”，而聚焦更高层次的能力**。

上述的做法这仍然只是将 Transformer 作为“高级编码器”，而不是“决策参与者”或“推理参与者”。需要理解的是 LLM 不等于 Transformer

> 虽然 LLM 架构上是 Transformer，但 **能力层级** 并不一样。
>
> | 模型类型                                      | 功能                           | 局限                                                         |
> | --------------------------------------------- | ------------------------------ | ------------------------------------------------------------ |
> | **Transformer-XL, BERT, T5**                  | 局部编码器：为文本生成句子向量 | 仅在“向量空间”建模，无法主动推理、排序或比较                 |
> | **LLM（如 ChatGPT, GPT-4, LLaMA-3, Claude）** | 推理者、排序者、判断者         | 具备 few-shot、CoT、对比判断能力，可以主动生成排序、路径、重要性解释 |

* 洞察点 4 （从 P2TAG 这篇文章得到的启发：提高预处理工作的质量）

1. 最核心的一点就是此工作直接在**文本编码**阶段也即所谓的预处理阶段直接利用了图结构信息。以往的工作包括最前沿的 NIE 的做法也是在文本编码时只考虑了**当前节点文本特征**。这种做法仅是基于上下文的短期依赖，它不会利用周遭节点的特征，缺乏图的结构信息。

   当然，在下游过程的编码-解码处理过程中还是会充分利用图结构信息的。

2. 关于 struct_feat。由于它是使用 node2vec 编码，因此在编码过程（预处理过程）就已经充分利用了图的结构信息。但事实上我们完全可以考虑在节点结构编码时利用一些异构信息！！！

3. 所以这个工作对我们的启发就是利用 LM 与 GNN 在编码的预处理阶段入手！提高静态的输入特征（嵌入的质量）。但是我们还得证明在 NIE 这个问题上，花精力提升上游给下游的输入质量确实比提高下游的输出质量有效！

   P2TAG 的想法是：

   1. 在 LM 阶段就让 token 表示吸收邻域结构信息，**避免 token-level 信息在节点聚合前就被压缩/丢失**。（这个还是很重要的，因为下游 GNN 的传播时**节点粒度**的，它对每个 token 内部的语义细节（例如文本里的关键实体、上下文细节）已经无能为力了）
   2. 这样一来，进入 GNN 阶段的节点表示已经是“结构增强版”，传播时更容易学习全局模式。

   这种方法在少样本（few-shot）和稀疏图上优势尤其明显，因为下游的学习机会少，初始特征质量的提升能直接带来泛化提升。



##### 书签





#### 思考

* 在输出层方面

**Ring-level 级别是做拼接 + 投影 + 归一化得到注意力系数；而 Type-level 级别是做 向量点积 + 归一化得到注意力系数。这两者有什么区别.**

首先尝试自己思考，思考不出来...（这个问题应该是比较深刻的）

> > **Type-level 注意力用的是点积 + softmax，而 Ring-level 用的是拼接 + 投影 + softmax，它们本质上有什么区别？**
>
> 我们来从**数学结构、表达能力、建模偏好**三个方面剖析它们的**注意力计算方法**本身的差异。
>
> 🔹1. 数学结构上的区别
>
> |          | 点积注意力（Type-level） | 拼接+投影注意力（Ring-level）  |
> | -------- | ------------------------ | ------------------------------ |
> | 形式     | 公式（4）                | 公式（6）                      |
> | 操作     | **内积**（不加权）       | **拼接再线性投影**（加权组合） |
> | 权重学习 | 无（只用输入特征）       | 有 learnable 参数 $W$          |
> | 特征维度 | 需对齐，必须维度一致     | 拼接后维度为 $2d$，更自由      |
>
> 🔸解释：
>
> - **点积注意力**是一种**无参数的相似性度量机制**：越相似（内积越大）→ 权重越高。
> - **拼接+投影注意力**是**可学习的匹配函数**：它通过参数 $W$ 来适配不同维度或关系的融合，表达能力更强，但更复杂。
>
> 🔹2. 表达能力差异
>
> |            | 点积（Type）                        | 拼接+投影（Ring）          |
> | ---------- | ----------------------------------- | -------------------------- |
> | 可表达函数 | 类似 cosine / L2 的距离度量（固定） | 可以学习非线性偏好（可变） |
> | 灵活性     | 较低                                | 高，可编码复杂交互关系     |
> | 稳定性     | 高，训练容易收敛                    | 低，依赖初始化与正则化     |
>
> > 📌 总结：拼接+投影机制能学到复杂的“偏好权重”，比如“靠近+远处组合权重最高”，而点积只能“相似度大就重要”。
>
> 🔹3. 建模偏好（适合处理的语义）
>
> | 任务                       | 点积注意力适合       | 拼接+投影注意力适合           |
> | -------------------------- | -------------------- | ----------------------------- |
> | 处理实体之间的相似性       | ✅ 是                 | ❌ 否                          |
> | 需要学习不同尺度的组合影响 | ❌ 否                 | ✅ 是                          |
> | 构造局部对比               | ✅ 是（每个 ring 内） | ❌ 否                          |
> | 融合跨尺度/层次结构        | ❌ 否                 | ✅ 是（例如 0-ring vs 2-ring） |
>
> ✅ 举个直观例子：
>
> 假设我们有中心节点 $p_1$，它的 1-ring 有 3 个类型：作者、主题、引用的论文：
>
> - **Type-level 的点积注意力**就像问：“这些类型谁跟我语义最像？” → 一个更**对称、静态的度量**；
> - **Ring-level 的拼接+投影注意力**就像问：“1-ring、2-ring加权后谁整体更重要？” → 是个更**复杂且可调的评估函数**。
>
> ✅ 总结一句话：
>
> > **点积注意力**是一种简单有效的“相似度选择机制”，适合**局部语义对比**；而**拼接+投影注意力**是一种带参数的“加权表达机制”，适合==**跨尺度融合与层次建模**。==
>
> 你可以根据你的任务需求来选择：
>
> - 追求简洁、稳定 → 点积；
> - 需要灵活建模非线性层次依赖 → 拼接 + 投影。



* 这篇文章貌似没有充分利用边的异构性



### 摘要

表现力限制和过度平滑促使研究人员探索图形变换器（GT）以增强 HIN **表示学习**。





### 引言

近年来，关于异构图的表示学习激增，这项技术可以**将节点嵌入到低维表示**中，同时**保留图结构和异构性**。

#### HGCN

基于HGNN的方法[11,19]通常利用**邻居聚合**策略，在HIN中的不同类型节点之间有效地捕获和传播信息。例如，R-GCN[23]通过结合关系特定的权重矩阵，在HIN中捕获各种关系，扩展了传统的图卷积网络（GCN）[17]。Fu等人[11]提出沿元路径合并中间节点，使用元路径内和元路径间信息进行高阶语义信息聚合。

尽管HGNN在模拟现实世界的HIN方面取得了成功，**但表达能力[35]、过度平滑[5]和过度压缩[1]等挑战的存在促使研究人员研究图形变换器（GT）[40]，以增强HIN表示学习**。例如，Hu等人[14]提出了一种**用于邻居聚合的异构Transformer式注意力架构**。Mao等人[21]利用**局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息**。一般来说，现有的基于GT的HIN表示学习方法，即图1（b）所示的基于HGT  （异构图 transformer） 的方法，遵循一个典型的原则：**给定一个目标节点，首先提取其k跳邻域（即距离目标节点≤k的可达距离内的节点）。然后，Transformer[29]将用于将信息从这些节点传播到目标节点。**（==其实就是做一个子图的处理==）这不就是一个 dgl 的 block 机制

比如 `figure 1` 中的 `b Existing HGT-based methods` 就揭露了当下的 HGT 的手段与缺陷！！！：

> 将 P1 的 2-hop 邻居（P2、A1、A3、S1、P3、P4）**全部统一处理**；
>
> 也就是说：
>
> - **不区分距离**：P2 是直接邻居（1-hop），P3 是间接邻居（2-hop），都混在一起；
> - **不区分类型**：论文、作者、主题的表示**放到一个 attention 序列中统一计算**；
>
> 缺点：语义容易混淆，难以理解不同节点“在结构中的不同语义角色”。



论文中也指出了了当下 HGT 的痛点、缺陷与不足（==这一块务必仔细阅读，好好理解==）

> 然而，现有的基于HGT的方法**倾向于混合不同类型的节点，并在邻居聚合过程中统一处理k-hop邻域内的所有节点，从而导致潜在的语义混淆。**特别是，（1）限制1：HIN中不同距离的目标节点的邻居具有不同的语义。以图1（a）为例，论文P1的直接邻居论文P2表示引用关系。相反，p1的间接邻居p3暗示了一种没有直接引用关系的主题联系，展示了不同的内涵。遗憾的是，现有的策略通过统一处理距离k内的每个邻居来忽略这些区别，即将P1、P2、P3**打包成一个序列**并统一聚合它们。这是不理想的，因为这些节点服务于不同的功能。（2）限制2：不同类型的目标节点的邻居也具有不同的语义。以图1（a）为例，论文P1的直接邻居包括论文P2、作者A1、A3和受试者S1。这里，P2表示引用关系，A1、A3表示作者关系，S1表示主题对齐关系。虽然现有的基于HGT的方法**考虑了节点类型**，**但它们通常将P2、A1、A3、S1打包在一起作为一个统一的序列**（**这是 HGT 所没有考虑到的**）。这种方法是不可取的，**因为它在邻居聚合过程中混合了不同类型的节点，模糊了论文、作者和受试者的不同功能。**

总结来说就是异构图中存在距离和邻居的异质性，而之前的 HGT 都没有考虑到



下面列举了一些**以前的**代表性工作

* R-GCN

  其实就是**最原始的**异构图的处理手段，对不同类型的边（关系）使用不同的变换矩阵，从而实现“关系感知”的聚合。

  > 我们以一个学术图（Academic Graph）为例，图中包含：
  >
  > - **三种节点类型**：Paper（P）、Author（A）、Venue（V）
  > - **几种关系类型**：
  >   - Paper ←write-by→ Author
  >   - Paper ←published-in→ Venue
  >   - Paper ←cite→ Paper
  >
  > 例如：
  >
  > - P1 是由 A1 和 A2 撰写的论文，发表于 V1；
  > - P2 是由 A2 撰写的论文，引用了 P1；
  > - P3 是由 A3 撰写的，和 P1 都发表于 V1。
  >
  > 图自己画一下就行了（很简单）
  >
  > 
  >
  > **方法核心：**
  >
  > ==R-GCN 对不同类型的边（关系）使用不同的**变换矩阵**，从而实现“关系感知”的聚合。==
  >
  > 
  >
  >  **举例：**
  >
  > 假设我们要更新节点 P1 的表示：
  >
  > - P1 的邻居包括：
  >   - A1, A2（write 关系）
  >   - V1（published-in）
  >   - P2（被 P2 引用）
  >
  > R-GCN 的聚合逻辑是：
  >
  > - 使用写作者的表示向量，通过参数矩阵 **W_write** 变换；
  > - 使用 Venue 的表示向量，通过 **W_publish** 变换；
  > - 使用被引用的论文的表示向量，通过 **W_cite** 变换；
  > - 聚合时不同关系分别处理，不会“混淆”。

  

* MAGNN [csdn阅读链接](https://blog.csdn.net/byn12345/article/details/105101492) （**暂时搁置**。。）反正这里这种方法是基于元路径的多种聚合手段

  基于元路径的手段，使用元路径合并中间节点，使用元路径内和元路径间信息**进行高阶语义信息聚合**。聚合整个路径的表示

  > 现有的基于元路径的嵌入学习方法有以下局限性：
  >
  > （1）忽略节点的内容特征（属性信息），不能很好地处理节点属性特征丰富的异质图。例如 metapath2vec, ESim, HIN2vec, HERec。
  >
  > （2）舍弃了元路径内部的节点信息，只考虑元路径的起始节点和末尾节点，造成信息损失。例如 HERec, HAN。
  >
  > （3）只依赖于单个元路径，因此需要人工选择元路径，丢失了来自其他元路径的部分信息，导致性能不佳。例如 metapath2vec。
  >
  > 为了解决上述问题，本文提出MAGNN（Metapath Aggregated Graph Neural Network ）。
  >
  > MAGNN由三个部分组成：
  >
  > （1）节点内容转换(node content transformation )，将异质的节点属性信息映射到同一个**隐层的向量空间**；
  >
  > （2）元路径**内部聚合**(intra-metapath aggregation )，使用注意力机制将元路径**内部的语义信息**纳入考虑；
  >
  > （3）元路径**间的聚合**(inter-metapath aggregation )，使用**注意力机制从多个元路径聚合信息**。
  > 
  >



* HGT （贡献很大的一篇文章）

  > 将标准 Transformer 改造为**异构图专用模型**，支持多种节点类型与边类型。
  >
  > 可以理解为：**注意力不仅考虑邻接关系，还区分每种异构边、异构节点对的交互模式。**
  >
  > **2. 时间编码（可选）：**
  >
  > 他们还为动态图（如引用网络）引入时间编码，可以处理时间序列。
  >
  > **3. 基于 Transformer 的堆叠式结构：**
  >
  > 和标准 GNN 一样，多层 HGT 会逐层融合多跳邻居信息，但内部机制是带类型感知的 Transformer。



* HINormer （2023年 80 引用）

  利用局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息。（貌似和我最开始的想法很像，可以找个时间去研读一下）

  进一步改进 HGT，缓解计算量大、信息干扰的问题，同时提升表达能力。

  > 1. **Local Structure Encoder**
  >
  > - 构造目标节点的局部结构视图，比如：
  >   - 局部子图中有哪些节点、类型、边；
  >   - 用轻量注意力机制捕捉结构模式。
  >
  > 2. **Heterogeneous Relation Encoder**
  >
  > - 更精细建模不同关系的作用（类似于 HGT 但更高效）；
  > - 使用了结构编码 + 边编码的组合策略；
  > - 学习关系类型与语义的交互表示。
  >
  > 3. **优势：性能更优，计算更快**
  >
  > - 他们主张结构编码和语义融合可以减少**不必要的全 attention**；
  > - 因此**更适合大图、低资源环境**；
  > - 实验中比 HGT 更快，效果也更强。

#### HHGT 的设计

##### 设计1

**为了区分不同距离的节点邻居**，我们引入了一种称为kring邻域的创新结构。这种结构具体指的是与目标节点的距离恰好为k的节点，将其与众所周知的k-hop邻域区分开。本质上，我们将k跳邻域划分为k+1个不重叠的k环邻域，其中每个k环邻域中的节点与目标节点的距离相同。如图1（c）所示，考虑k=2，对于纸张P1，其距离为2的邻居可以分解为三个不同的k环邻居：0环邻居{P1}、1环邻居{P2，A1，A3，S1}和2环邻居{P3，P4}。基于这种新结构，**我们为每个节点提取了不同的k环邻域，这可以自然地辨别不同的功能，从而防止语义混淆**（但事实上，个人认为即便是在同一环领域的节点，其语义信息也并不一致）。**然后，设计一个环级变换器，分别聚合不同的k环邻域，聚合基于每个k环邻域与目标节点的相关性和重要性。**（相当于在距离这一层上直接编码区分）

##### 设计2

**为了避免在每个k环结构中混合不同类型的节点（这里是考虑在同一环形邻域内的情况）**，我们进一步提出了一种新的（k，t）环结构，根据每个k环的类型将节点排列成不同的组。基于这种邻域划分，考虑到每种类型对目标节点的重要性，提出了一种类型级变换器，用于分别聚合每个k环结构内目标节点的不同类型的邻域。在图1（a）中，考虑节点P1的1-环邻域（即P2、A1、A3、S1），其中不同类型的节点共存。我们根据节点类型将这个1-环邻域分为三组，即论文P2、作者A1、A3和主题S1，**每组都携带唯一的函数**。然后，我们应用一个类型级Transformer来单独聚合每个组，而不是像现有的基于HGT的方法那样将它们视为一个统一的序列。这种方法使我们能够模拟不同类型节点的不同角色。



总结：

对于每个目标节点，我们从不同的k环邻域中提取其邻居，其中每个环内的节点根据其类型进一步分组，形成创新的（k，t）**环**    **邻域**    结构。基于这种结构，我们引入了一种新的分层异构图变换器（HHGT）模型。该模型无缝集成了一个类型级转换器，**用于分别聚合每个k环邻域内不同类型的节点**，然后是一个**环形转换器**，用于以**分层方式聚合不同的k环邻域**。



##### 贡献

* 我们首次为HIN表示学习设计了一种创新的（k，t）环邻域结构，强调了HIN中距离和类型的异质性。（这里的类型可以更加激进地理解为同一环形邻域内的不同节点类型）
* 据我们所知，我们是**第一个使用分层的图 transformers 模型，来进行异构图中的节点表征学习任务**，该模型无缝集成了类型级变换器，用于分别聚合每个k环结构中不同类型的节点，然后利用环形级变换器对不同的k环邻域进行分层聚合。
* 在两个真实世界的HIN基准数据集上的实验结果表明，我们的HHGT在两个典型的下游任务上明显优于14种基准方法。此外，消融研究验证了考虑HIN距离和类型异质性的优势和意义。



### 相关工作

#### HIN嵌入的浅层模型

近年来，许多图嵌入技术应运而生，旨在将节点或子结构映射到低维空间，同时保留图中的连接结构。由于现实世界中的网络通常包含多种类型的节点和关系，针对异构信息网络（HIN）嵌入的浅层模型研究引起了广泛关注。浅层模型主要分为基于随机游走的方法和基于一阶/二阶邻近性的 methods。这些方法利用**元路径**或**类型感知的网络邻近约束**来利用网络的异构性进行HIN嵌入。尽管这些方法有所贡献，但它们未能有效捕捉HIN中的复杂关系和语义，导致表示学习效果不理想。



#### HIN嵌入的深度模型

1. **HINs的研究背景**：
   - 深度学习模型在同质图中的成功激发了对异构信息网络（HINs）研究的关注。HINs涉及不同类型的节点和边，要求模型能够处理这种复杂的结构。
2. **两类HIN嵌入模型**：
   - **基于元路径的深度模型**：这些模型使用**元路径**来聚合特定类型邻居的信息，能够捕捉更高阶的语义信息。一个例子是**HAN**模型，它采用了层次注意力机制来学习节点和元路径的重要性。缺点是**需要专家知识来选择元路径**，这对模型性能有较大影响。
   - **无元路径的深度模型**：这些模型（如**Schlichtkrull**的关系感知图卷积层和**Hu**的基于Transformer的注意力机制）不依赖于元路径，**旨在通过==邻居聚合==来进行表示学习**。虽然无元路径的模型减少了人工选择元路径的需求，但存在两个问题：
     1. **节点类型混合**：在邻居聚合过程中，不同类型的节点被混合处理，导致无法充分捕捉不同类型节点之间的关联。
     2. **距离语义混淆**：忽视了HIN中不同距离邻居所携带的不同语义信息，导致邻居聚合时出现语义混淆，从而影响下游任务的性能。

HIN嵌入有两种主要方法，并指出了无元路径模型在处理节点关系和语义信息时的不足之处。

有时间去了解一下这两类处理方法的代表性方法



### 准备工作

* 问题定义
* transformers 编码器



### 方法论

该模型由两个重要模块组成：Ring2Token和TRGT。总体框架如图2（a）所示。给定一个HIN和一个整数K，对于每个目标节点，我们最初利用Ring2Token提取多个K环邻域（K∈[0，K]），从0环邻域到K环邻域，在每个K环结构中按类型划分组织良好的节点，形成（k，t）-环邻域结构。在邻域划分之后，**我们使用TRGT模块基于这些提取的（k，t）环邻域通过GT层学习节点表示**。这涉及一个类型级转换器来聚合每个k环邻域内不同类型的节点，然后是一个环级转换器来分层聚合不同的k环邻域。



#### Ring2Token

==如何有效地将**邻居的信息聚合到节点**中，对于设计强大的HIN表示学习模型至关重要==。

这个模块好理解，看 Figure 3 就能理解了

* k-ring Neighborhood

* (k, t)-ring Neighborhood

  具体来说，给定整数K和节点类型号T，对于节点u，它具有长度为K+1的K环邻域序列。在每个k环邻域内，它可以进一步划分为一系列长度为t的（k，t）环邻域。这些（k，t）环组将被送入TRGT模块进行模型训练。



#### TRGT Module

##### Type-level Transformer

**T 一致，不够补 0** （也就是说每一跳的 T 都是0，不足的使用 0 向量代替）

HIN中的节点有各种类型，每种类型都代表不同的概念。然而，现有的基于HGT的方法倾向于将不同类型的节点混合在一起，将所有节点类型打包成一个序列，并**在邻居聚合过程中统一关注它们**，未能像前面讨论的那样对各种类型的节点的不同角色进行建模。

$x_k = \{x_{K,1},x_{k,2}, \cdots, x_{k,T}\} \in \mathbb{R}^{T \times d}$ 在第 k 层上，针对这些嵌入/特征向量做 transformer，这样可以在每一跳之间区分节点类型（==这里还有一个比较吊的一点是，每个 k 层之间的节点之间是不存在边相连的，它在这一层做了 transformer 相当于直接建边了，增强了图的结构==）

不够补 0 向量

对于 0 环特征 $x_0$，我们使用多层感知器（MLP）将嵌入维数从 $\mathbb{R}^{T \times d}$ 转换为 $\mathbb{R} ^ {1 \times d}$

* Type-level Attention Mechanism.

  其实就是在解释公式（4）和（5），按照我的理解，这里其实可以看做 Type-level Transformer 的**输出层的处理**。（想通了之后完全合理）

  这个注意力的计算其实也是很自然能想到的，结合 Figure 2（b）那张图以及以往的经验

##### Ring-level Transformer

还没精读论文，感觉就是针对 Type-level Transformer 那一阶段得到的嵌入，作为输入在做一轮 Ring - level 级别的 transformer。

Figure 2（b）讲得很清楚

每个k环邻域都贡献了一层新的信息，它们的组合提供了不同的视角，这对于全面理解HIN至关重要。因此，从这些克林社区有效收集信息至关重要。为了应对这一挑战，我们开发了环级转换器，用于跨不同k环邻域进行全局聚合。对于每个节点，给定从类型级Transformer获得的k环令牌序列 $h_0,h_1, \cdots, h_k$，每个令牌都总结了属于特定k环结构的邻居的唯一信息，环形级Transformer首先利用Transformer编码器来学习节点表示。堆叠L个Transformer层后，我们得到每个节点的表示，即表示序列 $z_0, z_1, \cdots, z_k$，其中 $z_k \in \mathbb{R}^d$。

* Ring-level Attention Mechanism.

  同样的，这也可以看做是 Ring-level trainsformer 的**输出层的处理**

  ==Ring-level 级别是做拼接 + 投影 + 归一化得到注意力系数；而 Type-level 级别是做 向量点积 + 归一化得到注意力系数。这两者有什么区别.==





### 目标函数

做的是节点分类的任务，使用的损失函数是交叉熵损失。

那我们可以考虑将这个东西迁移到 NIE 上做一个回归任务



### 实验

我们进行实验来回答以下研究问题：

* RQ1:HHGT能否在下游任务中超越基线？

  HHGT实现了最佳的整体性能，这表明其具有卓越的有效性。观察到的改进的主要原因可能包括：

  （1）类型级转换器在邻居聚合过程中最佳地利用了节点类型信息，能够有效地捕获不同类型节点之间的适当相关性；

  （2）环级转换器考虑了不同距离的邻居之间的区别，促进了跨不同层次的强大邻居聚合。

  此外，在许多情况下，基于同构GT的方法比基于异构GT的方法表现更差，这验证了在HIN中考虑关系异构性的重要性。我们的模型也从这方面受益，通过设计（k，t）环邻域结构来强调HIN内距离和类型的内在异质性。

* RQ2：学习到的节点嵌入代表什么？他们能捕捉到HIN内部复杂的结构和异质性吗？

  为了进行更直观的比较，我们进行了嵌入可视化，以在低维空间中表示HIN。目标是使用HIN表示学习模型学习节点嵌入，并将其投影到二维空间中。我们采用t-SNE[9]技术进行可视化，特别关注两个数据集上的论文表示。根据[39]，这里的节点分别根据ACM和MAG的字段和已发布的场所进行颜色编码。

  结果见 Figure 4

  1. 基于浅层模型的方法总是在来自不同场所的论文之间显示出混合模式，缺乏明确的聚类边界。例如，HIN2Vec将所有论文混合在一起，限制了它在HIN中捕获复杂结构和语义关系的能力。
  2. 基于HGNN的模型，比如GTN和R-GCN提供了更合理的可视化结果，但它们的聚类更分散，不那么紧凑。
  3. 相比之下，基于异质GT的模型（包括HINormer、HGT和我们的HHGT）的可视化始终表现出高度的类内相似性

* RQ3：HHGT的不同模块如何有助于提高模型性能？

  为了了解拟议框架内不同组件对两项任务整体性能的影响，我们通过在两个数据集上删除或替换HHGT的关键建模模块来进行消融研究。具体来说，我们关注三个关键模块：（1）用于距离异构建模的k环邻域结构及其相应的环级变换器，（2）用于进一步类型异构建模的（k，t）-环邻域结构和其相应的类型级变换器。（3）基于两个变换器编码器的**基于注意力的读出功能**。通过移除或替换这些模块，我们可以获得HHGT的不同变体，如下所示：

* RQ4：超参数如何影响HHGT的性能？

  * 环数的影响：

    我们将K的范围从1到10来分析环数的影响，节点分类结果如图8所示。正如观察到的那样，该模型在不同数据集上使用不同的K达到了最佳效果，因为各种HIN显示了不同的邻域配置。此外，随着K的增加，所有数据集的性能逐渐提高，随后随着进一步的增加，性能略有下降。**虽然更大的K意味着节点考虑更广泛的邻域，但太大的K可能会覆盖整个网络，导致节点的邻域包含大量不相关的信息，甚至过度拟合。**（这对我们也有一定的启发）

    





## A Survey of Graph Meets Large Language Model: Progress and Future Directions（图论和LLM的综述）

### ==思考与感悟==

增强器、预测器和对齐组件

* 有一个点我们需要明确，GNN 对图的结构信息（**拓扑结构**）捕捉能力更强，但是不善于处理图的文本信息。LLM 对文本信息（**序列结构**）处理能力更强，但是不善于处理图的一些拓扑结构。我们可以在这一块上往下深挖。

  拓扑结构是一种非欧结构，常常蕴含高阶信息

  * 每个节点的邻居个数不固定、连接方式任意
  * 没有全局坐标系，没有规律性

* **LLMs 作为预测器的优势**：在**处理图的文本属性时表现出优越性**，与传统图神经网络（GNNs）相比，尤其在**零样本场景**下能取得显著性能。



### 其他补充

* 如何理解图生成学习（Generative Learning）

  最终目标是：训练出一个能够**理解图结构和节点语义**的强大图编码器（Encoder），**即便是在缺失信息时，也能恢复信息。**（这和 EASING 提出的缺少人工标注信息对应起来了）

  ==**既然之前说到过半监督学习可能和伪标签的质量有关，那么我们能不能考虑通过生成学习的方法生成更高质量的伪标签（感觉很靠谱）**==

  妈的貌似有人做过类似的了：

  > ENG[Yu等人，2023]授权LLM作为**样本生成器生成带有标签的额外训练样本，为GNN提供足够的监督信号。**

* 调试会涉及到之前预训练好的大模型？那我们如果在预训练好的大模型上微调，那岂不是又要重新训练，花费时间和资源。

  你这个问题问得非常关键，很多人初学 LLM 微调时都会有这个疑惑：

  > ❓“微调不是又要重新训练大模型吗？那还怎么叫‘微’调？不是又巨贵？”

  其实，**“微调 ≠ 从头训练大模型”**，而是在已有的**预训练模型参数基础上**，**对其中的一部分或全部进行小规模的“继续训练”**。下面我来详细解释。

  预训练过程花了巨大的资源，是一次性的，比如 GPT-3、T5、LLaMA、GLM 这些模型已经训练好了，模型参数你可以**直接下载使用**，不需要重头来过。

  | 微调方式      | 改动的参数（具体）                | 参数量（相对） | 成本 | 精度适配 |
  | ------------- | --------------------------------- | -------------- | ---- | -------- |
  | 全参数微调    | 模型所有参数                      | 💥 高           | 💸 高 | ✅ 最好   |
  | LoRA          | 每层中的 `W_q`, `W_v` 插入模块    | ⭐ 低（<1%）    | ✅ 中 | ✅ 很好   |
  | Adapter       | 每层插入小 MLP 模块               | ⭐ 低           | ✅ 中 | ✅ 好     |
  | Prompt Tuning | 输入 prompt embedding（不改模型） | 🌱 极低         | ✅ 低 | ❌ 较差   |
  | 只调任务 MLP  | Transformer 固定，仅调 head       | 🌱 极低         | ✅ 低 | ❌ 差     |



### 摘要

**背景重要性**：

- 图结构（Graph）在许多现实任务中扮演关键角色，如引文网络、社交网络、生物信息网络等。
- 图神经网络（GNN）一直是解决这些问题的主流方法，利用消息传递机制建模图结构。

**新趋势：引入大语言模型（LLMs）**：

- LLMs（如 GPT、BERT）在自然语言处理领域取得巨大成功，最近也被引入到图任务中，取得了超越传统 GNN 的性能。
- LLM 与图的结合能更好地处理**文本属性丰富的节点**、增强节点表示能力。

**文章内容**：

- 首先，**提出一个新的分类体系**：根据 LLM 在图任务中的角色，分为增强器（Enhancer）、预测器（Predictor）、对齐模块（Alignment Component）三类。
- 然后，**系统综述**这三类下的代表性工作。
- 最后，**讨论现有方法的不足与未来研究方向**。

**附加资源**：

- 提供了一个持续更新的项目列表（GitHub 链接），整理了相关论文和资源。



### 引言

> 这篇文章的**引言（Introduction）**部分主要围绕以下四个核心展开：
>
> 📌 一、研究背景与动机
>
> ✅ 图在现实世界中的重要性：
>
> - 图是一种基本的数据结构，被广泛用于建模**复杂关系**，如：
>   - **引文网络**（Citation Networks）
>   - **社交网络**（Social Networks）
>   - **分子结构**（Molecular Graphs）
> - 图数据通过节点之间的连接关系展现结构特性，能揭示系统中实体之间的依赖。
>
> ✅ 图神经网络（GNN）作为主要处理工具：
>
> - GNNs 是处理图数据的主流方法，如 GCN、GAT。
> - 通过**递归的消息传递机制**（确实哦，消息传递机制是有递归的感觉的），实现对节点、边、图级别的表示学习。
>
> 📌 二、大语言模型（LLMs）的崛起
>
> ✅ LLMs 在文本任务中大放异彩：
>
> - Transformer、BERT、GPT 等 LLM 模型在自然语言处理任务中取得巨大成功。
> - 拥有强大的上下文建模与迁移能力，能用于情感分析、翻译、分类等任务。
>
> ✅ 趋势：将 LLMs 引入图任务：
>
> - 越来越多研究探索将 LLM 与图结构数据融合，尤其是图中含有**文本属性**（如论文标题、用户简介等）时。
> - **互补优势**（==**我们从这里可以得到启发**==）：
>   - ==**GNN 善于建模结构信息，但对节点语义信息建模能力弱。**==
>   - ==**LLM 善于理解文本信息（序列信息），但缺乏对结构的建模能力。**==
>   - ==**结合两者可以提升表示能力和推理能力。**==
>
> 📌 三、典型案例介绍（说明 LLM 的三种作用）
>
> 文章引出了三个结合方式的代表性案例：
>
> 1. ==**TAPE：利用 LLM 生成与节点相关的语义解释作为 GNN 的输入增强。（2023）**==
> 2. ==**InstructGLM：用指令提示让 LLM 直接替代 GNN 的预测器。（2023）**==
> 3. ==**MoleculeSTM：将图嵌入与文本嵌入对齐，增强分子结构的推理能力。（2022）**==
>
> 这些例子展现了 LLM 在图学习中可以扮演多种角色。
>
> 📌 四、文章贡献与结构框架
>
> 📊 提出新的研究分类体系：
>
> - 根据 LLM 的角色，划分为：
>   - ==**Enhancer**（增强器）==
>   - ==**Predictor**（预测器）==
>   - ==**Alignment Component**（对齐模块）==
> - 并细化这些类别，梳理已有方法。
>
> 📚 系统综述：
>
> - 全面回顾并总结现有工作，覆盖主流方法与代表模型。
>
> 🔮 指出未来研究方向：
>
> - 包括：非文本图数据、迁移能力、解释性、计算效率、agent 化等挑战与机遇。

| 模块类型         | 类比作用                                    | 本质目标                           |
| ---------------- | ------------------------------------------- | ---------------------------------- |
| 增强器 Enhancer  | **替代/补充输入 embedding**                 | 强化节点/边的语义表征              |
| 预测器 Predictor | **替代 GNN 输出层（readout + classifier）** | 直接由 LLM 进行预测                |
| 对齐器 Alignment | **连接/协调两种编码器的表示空间**           | 融合结构信息与语言信息，提升表现力 |

我们可以**重点关注增强器与对齐器**



### 预备知识

#### GNN

大多数现有的GNN都遵循**消息传递范式**，其中包含**消息聚合**和**特征更新**，如GCN[Kipf和Welling，2016]和GAT[Velickovic等人，2018]。它们通过迭代地聚合邻居的信息并用非线性函数更新它们来生成节点表示。（这部分完全懂了）

前向过程的范式是
$$
h^{(l)}_i = U \left( h^{(l-1)}_i, M\left( \{ h^{(l-1)}_i, h^{(l-1)}_j | v_j \in \mathcal{N}_i \} \right) \right)
$$
很好理解

* **Graph pre-training and prompting**

  > **一、Graph Pre-training 图预训练**
  >
  > 📌 问题背景：
  >
  > - GNN 在新任务或新图上表现受限，且需要大量人工标注。
  > - 图预训练（Graph Pre-training）是图领域中的“迁移学习”方案，目标是让模型学到通用图表示。
  >
  > ✅ 两类主流方法：
  >
  > 1. **对比学习（Contrastive Learning）**：
  >    - 思路：构造两个图的不同视图，训练模型让它们的表示更接近。
  >    - 代表方法：
  >      - GraphCL（2020）
  >      - GCA（2021）：也扩展到了高阶结构（如超图）
  > 2. **生成学习（Generative Learning）**：
  >    - 思路：对图进行“遮盖”，然后训练模型恢复被遮盖的部分。
  >    - 代表方法：
  >      - GraphMAE（2022）
  >      - S2GAE（2023）
  >      - WGDN（2023）
  >
  > 🧠 技术假设：
  >
  > - 下游任务与预训练任务有共享的表示空间。
  > - 类似 NLP 中的“先 pretrain 再 fine-tune”。
  >
  > **二、Graph Prompting 图提示学习**
  >
  > 📌 新趋势：
  >
  > - 在 NLP 领域，越来越多工作从“预训练 + 微调”转向“预训练 + 提示 + 微调（prompt-tuning）”。
  > - Prompting = 利用“人工或自动构造的输入形式”，激发模型内在能力。
  >
  > ✅ 代表性方法：
  >
  > 1. **GPPT（Graph Pretraining and Prompt Tuning）**：
  >    - 第一阶段：用边遮盖任务进行预训练。
  >    - 第二阶段：将节点任务重新表述为“边预测任务”，即把节点转成一对 token，再用边分类方式解决。
  > 2. **All in One**（Prompt统一化框架）：
  >    - 把**所有图任务和语言任务**都统一成“提示 + 输入 + 输出”的格式。
  >    - 可以在多个任务上做统一训练，提高多任务泛化能力。

#### LLM

> 1️⃣ LLM 的定义（Definitions）
>
> > ❗注意：目前并没有一个公认统一的 LLM 定义。
>
> 📌 本文采用的定义标准如下：
>
> - 借鉴两个重要综述 [Zhao et al., 2023d] 和 [Yang et al., 2023] 的观点：
>   - **LLM（Large Language Model）** 通常指参数量达到**十亿级别（billion-level）**的模型，经过大规模语料预训练。
>   - **PLM（Pretrained Language Model）** 泛指早期的、参数量较小的模型（百万级别），可以被轻量微调到特定任务上。
>
> 🤝 在本论文中：
>
> > ==**将 LLM 和 PLM 一并统称为“LLM”**==
> >  因为在图学习中，实际使用的大多是如 BERT、T5 这样参数较小、可嵌入的语言模型，而非超大模型（如 GPT-4），所以论文放宽了 LLM 的定义范围，便于分类分析。

* **Evolution**

  LLM 可分为非自回归（如 BERT）和自回归模型（如 GPT-3），分别用于理解和生成任务。随着模型规模扩大，它们表现出强大的 zero/few-shot 能力（可能依赖 prompt）和推理能力，是图学习任务中非常宝贵的语义增强工具。

#### Proposed Taxonomy

本节提出了一个围绕 LLM 在图任务中作用的三层分类体系：增强器、预测器、对齐器。该体系清晰地区分了 LLM 参与图任务的方式，有助于系统化理解研究趋势，也为未来方法设计提供了参考。对于不能直接归类的创新方法，也额外提出了“其他”类，以展示 LLM 更广泛的图任务潜力。

> | 类别                     | 含义                                 | 核心任务             | 示例                   |
> | ------------------------ | ------------------------------------ | -------------------- | ---------------------- |
> | **1. LLM as Enhancer**   | LLM 增强 GNN 的输入或中间表示        | 增强节点/边嵌入      | TAPE, GIANT            |
> | **2. LLM as Predictor**  | LLM 替代传统 GNN 的预测模块          | 分类、推理、生成输出 | GPT4Graph, InstructGLM |
> | **3. GNN–LLM Alignment** | 对齐两个模型的表征空间，实现协同训练 | 模态对齐、跨模态检索 | MoMu, GLEM             |

详见论文 Figure-2



### LLM 作为增强器

为什么需要 Enhancer？

许多图任务中的节点具有文本属性（如论文标题、简介等），但传统做法（如 TF-IDF、word2vec）无法充分表达语义，导致 GNN 的输入特征信息稀疏、语义弱。

> LLM-as-Enhancer的目标
>
> 让 LLM 提供更高质量、更语义丰富的节点嵌入，**用于增强 GNN 的输入**（或中间层），从而提升最终表现。

分为两大类增强方式：

1. **Explanation-based Enhancement**（解释式增强）
2. **Embedding-based Enhancement**（嵌入式增强）



#### 基于解释的增强

需要彻底理解 Figure 3 - (a) 这张图

* 理解 LLM 和 LM 在论文中的区别与联系

  | 维度                    | LLM                                  | LM                             |
  | ----------------------- | ------------------------------------ | ------------------------------ |
  | **是否可训练 / 可微调** | ❌ 不可训练，视为黑盒（如 GPT-4 API） | ✅ 可训练（如 BERT、T5）        |
  | **使用方式**            | 用 prompt + 输入生成“解释文本”       | 把文本编码为向量嵌入           |
  | **输出类型**            | 文本（解释 / 标签）                  | 向量（embedding）              |
  | **代表模型**            | ChatGPT, GPT-4, Claude（API）        | BERT, SciBERT, RoBERTa（开源） |
  | **调用开销**            | 高（逐节点生成）                     | 低（可批量编码）               |

* 其范式为

  ```bash
  原始文本 ti + prompt p
         │
       调用 LLM（生成 ei：解释、知识等）
         │
  xi = fLM(ei, ti)  ← 组合原文本 + LLM输出，转成向量
         │
    输入 GNN（X + A）进行图学习
  ```

* 代表方法

  **TAPE**（He et al., 2023）：

  - 给节点（论文）输入 prompt：“请解释这篇论文主要讲了什么？”
  - LLM 生成伪标签或解释文本，再训练轻量模型提取向量表示。

  **KEA**（Chen et al., 2023a）：

  - 让 LLM 为节点**生成**相关的知识实体+文本描述（如“这篇论文与图注意力网络相关”），再编码为 embedding。

  **LLM4Mol**（Qian et al., 2023）：

  - 对分子结构的 SMILES 表达式，生成“分子作用、用途”等语言描述。

  **LLMRec**（Wei et al., 2023）：

  - 用 LLM 为用户和物品生成额外文本特征（如兴趣/属性），**缓解推荐场景中的稀疏性问题**（还真是）。

* 优势

  适用于封闭源 LLM（如 ChatGPT）

  解释信息对**分类、推荐**等任务尤为有效

* 局限

  每个节点都要请求一次 LLM → **成本高、慢**

  依赖 prompt 工程，控制力较弱（就是说我们生成的补充文本、知识片段、伪标签全都强烈依赖这个 prompt 的设计）



#### 基于嵌入的增强

直接将 LLM 作为编码器，**把文本属性转为向量嵌入 xi**，作为 GNN 的输入节点特征。

这种方法需要使用嵌入可见或开源LLM，**因为它需要立即访问文本嵌入**，并使用结构信息微调LLM。

如何理解论文中提到的级联形式？

> 具体而言，在嵌入性增强中，研究人员直接将大语言模型（LLMs）输出的文本嵌入作为图神经网络（GNNs）的初始节点嵌入。为了让 LLMs 更好地捕捉图的结构特征，这类方法通常会将图的结构信息（如节点间的连接关系、邻域信息等）融入 LLMs 的预训练或微调阶段，形成 “结构信息辅助语言模型训练→语言模型生成嵌入→GNNs 利用嵌入进行图学习” 的级联流程。

* 其范式为

  ```bash
  原始文本 ti
       │
  xi = fLLM(ti)   ← 直接调用 LLM 的 encoder，得到嵌入
       │
  GNN(X, A)       ← 正常图神经网络训练
  ```

* 代表方法（一看一个不吱声~~，完全不了解细节）

  **GIANT**（Chien et al., 2021）：

  - 用 PLM（如 BERT）编码文本特征，通过多视图训练得到更鲁棒的表示。

  **SimTeG**（Duan et al., 2023）：

  - 用链接预测任务微调 BERT，使其能感知图结构信息。

  **TouchUp-G**（Zhu et al., 2023）：

  - 增强结构感知性，用负采样强化训练。

  **G-Prompt**（Huang et al., 2023b）：

  - 在 BERT 后添加一个图适配模块（adapter），并加入提示向量以支持多任务学习。

  **WalkLM**（Tan et al., 2023b）：

  - 把图转成“随机游走 + 描述句子”，再送入 LLM 训练，提取表示。

  **LEADING**（Xue et al., 2023）：

  - 将 LLM 迁移到图表示学习，通过轻量微调方式提升可迁移性。

* 优势

  更系统化、自动化、端到端训练可能性更高

  开源 LLM（如 BERT）可直接嵌入 GNN 管道

*  局限

  需要嵌入式访问 LLM（必须能拿到 embedding）

  GPT-4 这类闭源模型无法使用该方式



#### 总结

| 对比维度     | Explanation-based              | Embedding-based         |
| ------------ | ------------------------------ | ----------------------- |
| 使用闭源 LLM | ✅ 支持（如 ChatGPT）           | ❌ 需要开放模型结构      |
| 节点处理成本 | 高（每个节点都需调用一次生成） | 低（批量编码）          |
| 表达能力     | 强，含人类解释                 | 强，但依赖 LLM 训练阶段 |
| 灵活性       | 高，可与任意 GNN 组合          | 高，但需嵌入访问权      |
| 可扩展性     | 差，大图不适合                 | 好，可用于大规模图      |



### LLM 作为预测器（主要用于解决分类和推理任务）

这一类别背后的核心思想是利用LLM在统一的生成范式内对广泛的图形相关任务（如**分类和推理**）进行预测。然而，将LLM应用于图形模态带来了独特的挑战，**主要是因为图形数据通常缺乏直接转换为连续文本的方法**，**因为不同的图以不同的方式定义结构和特征**。在本节中，我们将模型大致分为基于扁平化和基于GNN的预测，**具体取决于它们是否使用GNN来提取LLM的结构特征**。

#### Flatten-based Prediction

Flatten-based Prediction 是 “LLM as Predictor” 类别下的一种核心方法，其核心思路是将图结构转化为**文本序列**，使大语言模型（LLMs）能够直接处理图数据并完成预测任务1。该方法主要包含两个关键步骤：图的扁平化转换和预测结果解析。

* **图的扁平化（Graph Flattening）**

  通过一个扁平化函数 $\text{Flat}(\cdot)$ 将图的节点、边、节点文本属性、边文本属性等要素转换为文本序列 $G_{seq}$
  $$
  G_{seq} = \text{Flat}(\mathcal{V}, \mathcal{E}, \mathcal{T}, \mathcal{J})
  $$
  比如，将一个图转成类似如下形式的输入：

  ```css
  Node A: "Apple", Node B: "Fruit", Edge: A -> B, Relation: "is a"
  ```

  或者生成“游走序列”：

  ```css
  A → B → C → D（再通过节点属性+边关系生成文本描述）
  ```

* **预测与解析（Prediction & Parsing）**

  将文本序列 $G_{seq}$ 与任务相关的指令提示 $p$ 输入 LLMs，再通过解析函数 $\text{Parse}(\cdot)$ 从 LLMs 的输出中提取预测标签 $\overline{Y}$ ，公式为
  $$
  \overline{Y} = \text{Parse}(f_{\text{LLM}}(G_{seq},p))
  $$

* 之前工作的解析策略

  鸽了，不列举了



* 两者区别

  与解析策略相比，**扁平化策略可以表现出显著的差异**。在下文中，我们根据LLM的参数是否更新来组织扁平化方法。

  1. **扁平化策略**：聚焦于将图结构（节点、边、属性等）转化为 LLMs 可处理的文本序列，是 “**输入转化**” 阶段的操作。**其目的是解决图数据与 LLMs 输入格式不兼容的问题，让 LLMs 能够 “读懂” 图的结构和信息**。
  2. **解析策略**：针对 LLMs 生成的输出结果，提取出最终预测标签，是 “**输出处理**” 阶段的操作。其目的是从 LLMs 可能包含推理过程的自然语言输出中，精准获取任务所需的预测结果（如分类标签、推理结论等）。
  3. **扁平化策略**：确保图数据能被 LLMs 有效 “理解”，重点在于保留图的结构和属性信息。
  4. **解析策略**：确保从 LLMs 的输出中准确 “提取” 预测结果，重点在于适配下游任务对输出格式的要求。

   简言之，扁平化策略是 “如何让 LLMs 看懂图”，解析策略是 “如何从 LLMs 的回答中得到答案”，二者分别对应图输入到预测输出的两个关键环节。



##### LLM 冻结

指在将图结构转化为文本序列输入大语言模型（LLMs）时，不更新 LLMs 的参数，即保持模型参数冻结状态。

该策略的核心是通过特定方式将图结构转化为 LLMs 可处理的文本序列，而不对 LLMs 本身进行微调。具体的扁平化方式多样，例如：

- GPT4Graph 利用 GML、GraphML 等图描述语言来表示 graphs，这些语言为图中的节点和边提供了标准化的语法和语义；
- GraphText 受**语言语法树**启发，利用图语法树将图结构转换为节点序列，进而输入 LLMs 进行无训练的图推理；
- ReLM 使用简化分子输入线输入系统（SMILES）字符串来提供分子图结构的一维线性化表示；
- 还有一些方法直接采用数值组织的节点和边列表，以纯文本形式描述图数据；
- 此外，也可通过**自然叙述**来表达图结构，如 Chen 等人和 Hu 等人的研究**将引用网络的结构信息整合到提示中**，**用 “cite” 一词明确表示边关系，并用论文索引或标题表示节点**；==**Huang 等人则通过枚举当前节点的随机选择的 k 跳邻居来描述关系，而不使用 “cite” 一词。**==

这些方式均在不调整 LLMs 参数的前提下，实现了图结构向文本序列的转化，使 LLMs 能够处理图数据并完成相关任务。



##### LLM 微调

这类策略通过特定方式**将图结构信息融入 LLMs 的微调过程，使模型更适配图相关任务**。例如：

- GIMLET 采用基于距离的位置嵌入，**将两个节点在图中的最短距离定义为相对位置**，以此**扩展 LLMs 感知图结构的能力**，这种方式在图 Transformer 相关研究中被广泛应用。
- InstructGLM 则基于**最大跳数**设计了一系列可扩展提示，==**通过自然语言描述连接关系，让中心节点能与任意跳数的邻居建立直接关联，从而帮助 LLMs 更好地理解图中节点间的多跳连接。**==

通过微调，LLMs 能够更有效地处理经扁平化后的图文本序列，提升在图相关任务中的预测性能，**且部分模型经微调后可直接输出==预测标签==，减少后续解析步骤的成本。**



#### GNN-based Prediction

在 “LLM as Predictor” 框架下，“GNN-based Prediction”（GNN 辅助预测）是与 “Flatten-based Prediction” 并列的另一类核心方法。**其核心思路是利用图神经网络（GNNs）擅长捕捉图结构信息的优势，先提取图的结构特征，再将这些特征输入大语言模型（LLMs）进行预测，使 LLMs 具备结构感知能力。**



##### 核心流程

* **图特征学习**：通过 GNNs 处理节点嵌入矩阵 （$X$） 和邻接矩阵（$A$），生成包含结构信息的嵌入（$H$）,公式为：
  $$
  H= f_{GNN}(X,A)
  $$

* **预测与解析**：将结构感知嵌入 $H$ 与任务提示（$p$）输入 LLMs，再通过解析函数从 LLMs 的输出中提取**预测结果**
  $$
  \tilde{Y} = \text{Parse}(f_{LLM}(H,p))
  $$

与扁平化预测不同，这类方法**依赖 GNNs 提取结构特征，且由于需要将 GNN 表示融入 LLMs，通常需要进行微调，以便在训练中标准化 LLMs 的预测格式**。

当然，我们也可以将 GNN 提取的图结构信息表示（如 `H` 或 `graph_repr`）**通过一定处理，转化成自然语言形式的 prompt**，作为 LLM 的输入。

> 如果我们希望更自动化一些，可以：
>
> 1. 用 GNN 得到结构向量表示 `graph_repr`
> 2. 用一个轻量的**解码器**（如小型 LLM、T5）做结构文本生成：
>
> ```python
> graph_repr = f_GNN(X, A)  # → shape [1, d]
> graph_text = TextDecoder(graph_repr)  # 自然语言结构描述
> ```
>
> 这个 `TextDecoder` 可以是一个微调过的 T5、小的 Transformer 或 LSTM。
>
> 这就是所谓的：
>
> > 用一个“结构解释模块”（structure-aware text generator）来做结构 → 文本的 bridge。
> >
> > 这种做法我们需要自己训练好一个“结构-to-text”生成器



##### 特征融合策略

现有研究提出了多种融合 GNN 结构特征与 LLMs 文本信息的策略，例如：

* **跨模态投射器**：GIT-Mol 和 MolCA 采用 BLIP-2 的 QFormer 作为跨模态投射器，将 GNN 编码器的输出映射到 LLM 的输入文本空间，并通过多目标注意力掩码策略实现有效的图 - 文本交互。
* **前缀调优**：GraphLLM 在 prefix tuning 过程中，通过线性投影将图表示转化为图增强前缀，使 LLMs 能与图 Transformer 协同融合图推理所需的结构信息。
* **线性层对齐**：GraphGPT 和 InstructMol 使用简单的线性层作为轻量级对齐投射器，将编码后的图表示映射为 “图令牌”，使 LLMs 能将这些令牌与各类文本信息对齐。
* **分层注入**：DGTL 将解耦的图嵌入直接注入 LLMs 的每一层，突出图的拓扑和语义的不同方面。

#### 总结

这部分内容围绕将大语言模型（LLMs）直接作为预测器处理图相关任务的优势、目标及两种预测方式（扁平化预测和 GNN 辅助预测）的特点与局限展开，具体如下：

- **LLMs 作为预测器的优势**：在**处理图的文本属性时表现出优越性**，与传统图神经网络（GNNs）相比，尤其在**零样本场景**下能取得显著性能。

  > 具体到图相关任务中，当将大语言模型（LLMs）作为预测器时，其在零样本场景下表现出色，即不需要针对某类图任务（如特定类型的图分类、推理等）进行专门的训练，仅通过输入任务描述或相关提示，就能利用自身预训练过程中习得的知识完成任务，**这与传统图神经网络（GNNs）需要大量标注数据进行训练才能较好地处理特定任务形成对比**。
  >
  > 
  >
  > **这种能力得益于 LLMs 在大规模文本语料上预训练所获得的强大泛化能力和知识储备**，使其能够在未见过特定任务样本的情况下，理解任务需求并生成合理的预测结果。

- **核心目标**：开发并优化将图结构信息编码为 LLMs 能够有效且高效理解和处理的格式的方法。

- 两种预测方式的对比

  - **扁平化预测**：在有效性方面可能更具优势，但受 LLMs 输入长度限制，每个节点只能获取少数几跳内邻居的信息，难以捕捉长距离依赖关系；且由于不涉及 GNNs，**无法解决 GNNs 固有的异质性等问题。**
  - **GNN 辅助预测**：往往更高效，但需要额外训练 GNN 模块并将其插入 LLMs 进行联合训练，而深度 Transformer 早期层存在梯度消失问题，这使得训练难度较大。







### GNN-LLM对齐

理解 Figure-5 就行了。

首先我们需要明确一点的就是我们针对节点的文本特征使用 tokenizer 进行处理，得到的该节点的文本输入张量为 $X_{text} \in \mathbb{R}^{\text{seq} \times d}$ （注意是有一个 seq 的！）

其次我们要思考并理解 GNN-LLM 对齐的意义是什么？解决的问题是什么？

> **意义与目标：**
>
> “对齐”是让 GNN 与 LLM 对 **同一节点/实体** 形成的表示在**语义上相似/一致**。
>
> **解决的问题：**
>
> **多模态语义不一致问题：**
>
> - GNN 和 LLM 各自生成的 embedding 存在分布偏差，不利于下游任务统一建模（如节点分类、推荐、检索）。
> - 对齐有助于**统一语义空间**，让图结构和文本之间“说同一种语言”。
>
> **低资源图/冷启动问题：**
>
> - GNN 很依赖图的结构（边/邻居），但实际很多节点是“冷启动”（边少或无），这时文本成为重要补充。
> - 对齐后，可以**用 LLM 表示补全图信息**（如：zero-hop nodes）。
>
> **迁移与泛化能力弱：**
>
> - GNN 通常是 task-specific 的，缺乏像 LLM 那样的强泛化与预训练能力。
> - 对齐能让 GNN 学到部分 LLM 的泛化能力，提升表示的**可迁移性和稳健性**。
>
> **图神经网络的鲁棒性与可解释性弱：**
>
> - GNN 对 adversarial 攻击较敏感，文本模型（LLM）在这方面相对稳健。
> - Align 后可以提升**鲁棒性**，并借助语言生成提升**可解释性**。

有以下三种对齐做法

1. Contrastive（对比研究）对称对齐，将连接或对比学习应用于图嵌入和文本嵌入

2. Iterative（迭代做法）属于对称对齐，旨在实现两种模态嵌入的迭代交互：互相为对方生成伪标签

   > GNN 在结构上强 → 给文本做伪监督；LLM 在语义上强 → 给图结构提供伪监督；二者交替训练，迭代优化，共同提升性能。
   >
   > ✅ Step 1：使用 LLM 给图生成伪标签（文本监督 → 图）
   >
   > 1. 用 BERT 对每个节点的文本进行编码：
   >
   >    zitext=BERT(xitext)z^{\text{text}}_i = \text{BERT}(x^{\text{text}}_i)zitext=BERT(xitext)
   >
   > 2. 对 unlabeled 节点（比如论文 A），BERT 输出分类结果（如 softmax 概率）：
   >
   >    ```python
   >    P_text[A] = [0.2, 0.7, 0.1]  # 类别分布（AI, ML, Theory）
   >    ```
   >
   > 3. 将该概率分布作为 **伪标签** 给到 GNN 训练用：
   >
   >    - 即，**GNN 学习去拟合 LLM 对文本的判断结果**；
   >
   >    - loss:
   >      $$
   >      L_{\mathrm{GNN}} = \mathrm{KL}\left(P_{\mathrm{GNN}}(A) \,\|\, P_{\mathrm{BERT}}(A)\right)
   >      $$
   >
   > ✅ Step 2：使用 GNN 给文本生成伪标签（图监督 → 文本）
   >
   > 1. GNN 处理节点 A 和邻居节点（B, C）：
   >    $$
   >    z_A^{\mathrm{gnn}} = \mathrm{GCN}(A, \text{neighbors} = [B, C])
   >    $$
   >    
   >
   > 2. GNN 输出分类概率作为伪标签：
   >
   >    ```python
   >    P_gnn[A] = [0.05, 0.9, 0.05]
   >    ```
   >
   > 3. 用这个伪标签反过来监督 BERT：
   >
   >    - loss:
   >      $$
   >      L_{\mathrm{BERT}} = \mathrm{KL}\left(P_{\mathrm{BERT}}(A) \,\|\, P_{\mathrm{GNN}}(A)\right)
   >      $$
   >      
   >
   > 🔁 Step 3：交替优化
   >
   > - 每一轮训练中，固定 GNN，用它产生伪标签训练 BERT；
   > - 然后固定 BERT，用它产生伪标签训练 GNN；
   > - 迭代进行数轮，直到二者收敛或达到一致。

3. Graph-nested（图形嵌套）将GNN与变压器交织在一起的对称排列

   这里举一个具体的例子来进行说明（**重点需要理解如何在 transformers 的每一层中融合 GNN 的相关信息！**）

   > **场景设置（完整定义）**
   >
   > 我们构建一个小型 citation graph：
   >
   > - **节点：**3 篇论文 A, B, C
   > - **边：**A 引用了 B 和 C
   > - **文本：**
   >   - `A`: "Graph Neural Networks model relational data."
   >   - `B`: "DeepWalk learns embeddings using random walks."
   >   - `C`: "GCN uses convolution over graphs."
   >
   > 🔧 Step 1：构造输入文本的 Token Embedding
   >
   > 用 BERT tokenizer 得到：
   >
   > A 的文本：
   >
   > ```text
   > [CLS] Graph Neural Networks model relational data [SEP]
   > ```
   >
   > → 假设 tokenize 后是 8 个 token
   >  → 每个 token embedding 是 `768` 维（标准 BERT 设置）
   >  → 得到文本输入张量 `X_text ∈ ℝ^{8×768}`
   >
   > 🧱 Step 2：用 GNN 获取结构表示
   >
   > 我们构建邻接关系：A 的邻居是 B 和 C。
   >
   > - 用图神经网络（如 GCN）对图结构进行一层聚合：
   >   $$
   >   h_A = \text{GCN}(A) = f(\mathbf{A}, \mathbf{X}) = \text{aggregate}(X_B, X_C, X_A)
   >   $$
   >
   > - 每个节点初始特征 `X_i` 是由文本平均池化得到的，或者用 LLM（如 Sentence-BERT）先获得。
   >
   > - 假设输出维度仍为 768，则：
   >
   > hA∈R768h_A ∈ ℝ^{768}
   >
   > 🔁 Step 3：**Graph-Nested 融合机制**
   >
   > 我们现在要把 GNN 输出 `h_A` 嵌入到 Transformer 的每一层。
   >
   > ✅ 方法一：修改 Transformer 的每层输入（加法融合）
   >
   > Transformer 第 l 层中：
   >
   > $H(l)=\text{TransformerLayer}^{(l)}(H^{(l-1)})$
   >
   > 其中：
   >
   > - `H^{(0)} = X_text` 是初始文本 token embedding；
   > - 现在我们对 `[CLS]` token 的表示进行融合：
   >
   > $H^{(l)}[0] \leftarrow H^{(l)}[0] + W_gh_A$
   >
   > - 这里 `W_g ∈ ℝ^{768×768}` 是一个可学习的线性映射；
   > - `[0]` 表示 `[CLS]` token 的位置；
   > - 其他位置不加图信息。
   >
   > 这一步是 Graph-Nested 的核心设计：**结构信息通过每层的 `[CLS]` token 注入 Transformer 的决策中**。
   >
   > ✅ 方法二（更通用）：结构向量作为附加 Token
   >
   > 我们把 GNN 输出 `h_A` 作为一个新 token 拼接进输入：
   > $$
   > X_t^{\prime \, \text{text}} = [h_A;\ \text{token}_1;\ \text{token}_2;\ \ldots;\ \text{token}_7] \in \mathbb{R}^{9 \times 768}
   > $$
   > 
   >
   > 然后进入 Transformer，每一层都正常计算，只是注意：
   >
   > - **`[0]` 是图结构 token**
   > - `[1]` ~ `[8]` 是原始文本 token
   >
   > 最后我们可以：
   >
   > - 用 `[CLS]` 或结构 token 来做分类；
   > - 或者输出所有 token 做生成。
   >
   > 🎯 Step 4：用于下游任务（节点分类 / 文本生成）
   >
   > 最终输出：
   >
   > - `Z = Transformer(X'_text) ∈ ℝ^{9×768}`
   > - 用 `Z[0]` 或 `Z[1]`（即结构 token 或 [CLS]）送入 MLP 分类器
   >
   > 例如：
   >
   > ```python
   > logits = MLP(Z[0])
   > ```
   >
   > 用于判断这篇论文 A 的类别。
   >
   > 🔍 举个最终完整例子（维度演示）
   >
   > | 模块        | 输入                 | 输出维度        | 内容                   |
   > | ----------- | -------------------- | --------------- | ---------------------- |
   > | Tokenizer   | A 的文本             | 8 tokens        | `X_text ∈ ℝ^{8×768}`   |
   > | GNN（GCN）  | A + 邻居节点（B, C） | `h_A ∈ ℝ^{768}` | 聚合得到 A 的结构表示  |
   > | 拼接输入    | `[h_A; X_text]`      | `9×768`         | 构成 LLM 的输入        |
   > | Transformer | 6 层（如 BERT-base） | `Z ∈ ℝ^{9×768}` | 每层将结构表示传递下去 |
   > | 下游 MLP    | `Z[0]` 或 `[CLS]`    | 类别数（如5）   | 输出节点分类预测       |
   >
   > ✅ 总结这整个过程：
   >
   > | 步骤               | 描述                                        |
   > | ------------------ | ------------------------------------------- |
   > | ① Tokenize 文本    | 将文本变成 token embedding 输入 Transformer |
   > | ② GNN 编码图结构   | 获取结构感知表示 `h_A`                      |
   > | ③ 嵌入结构向量     | 将 `h_A` 融合进 Transformer 层（加法/拼接） |
   > | ④ Transformer 编码 | 层层传递结构感知语义                        |
   > | ⑤ 下游任务处理     | 分类 / 推理 / 生成等                        |

4. Distillation（蒸馏）属于非对称对齐，它使用GNN作为教师来训练语言模型，使其具有图形感知能力。



#### 总结

为了对齐GNN和LLM，对称对齐同等对待每种模态，目的是同时增强GNN和LL。这使得编码器能够有效地处理涉及两种模态的任务，利用其各自的编码优势来改进模态特定的表示。此外，非对称方法通过在 transformer 中插入图编码器或直接使用GNN作为教师来增强LLM。然而，在处理数据稀缺问题时，对齐技术面临着挑战。特别是，只有少数图形数据集（即分子数据集）包含原生图形文本对，限制了这些方法的适用性。



### 未来发展方向

在本节中，我们讨论了利用LLM理解图形数据的能力的剩余局限性，并列出了后续研究中进一步探索的一些方向。

> ### 处理非文本属性图
>
> 利用 LLMs 辅助文本属性图的学习已展现出优异的性能。然而，图结构数据在现实场景中无处不在，其中**许多缺乏丰富的文本信息**。例如，在交通网络中（如 PeMS03 [Song et al., 2020]），每个节点代表一个运行中的传感器；在超像素图中（如 PascalVOC-SP [Dwivedi et al., 2022]），每个节点代表一个超像素。这些数据集的每个节点都没有附加的文本属性，且很难用人类可理解的语言描述每个节点的语义。尽管 OFA [Liu et al., 2023a] 提出使用人类可理解的文本描述所有节点和边，并通过 LLMs 将不同领域的文本嵌入到同一空间中，但它可能并非适用于所有领域（如超像素图），且在某些领域和数据集上的性能可能欠佳。**探索如何利用 LLMs 强大的泛化能力来帮助构建图基础模型是一个有价值的研究方向。**
>
> ### 处理数据泄露
>
> LLMs 中的**数据泄露问题**已成为讨论的焦点 [Aiyappa et al., 2023]。由于 LLMs 在大规模文本语料库上进行预训练，它们很可能见过并记住了常见基准数据集的至少部分测试数据，尤其是引文网络。这削弱了当前依赖早期基准数据集的研究的可靠性。此外，Chen 等人 [Chen et al., 2023a] 证明，特定的提示可能会增强 LLMs 对相应记忆的 “激活”，从而影响评估结果。Huang 等人 [Huang et al., 2023a] 和 He 等人 [He et al., 2023] 都**试图通过收集新的引文数据集来避免数据泄露问题**，确保测试论文来自 ChatGPT 数据截止日期之后的时间段。然而，他们的研究仍然局限于引文领域，且其数据中图结构的影响并不显著。因此，重新思考准确评估 LLMs 在图相关任务上性能的方法至关重要。我们还需要一个公平、系统且全面的基准。
>
> ### 提高可迁移性
>
> 就是我在一张图上训练得到的模型，能否在另一张图上的表现也具备**很不错的泛化性**！
>
> 【~~一眼感觉不行~~】
>
> 可迁移性一直是图领域的一个挑战 [Jiang et al., 2022]。由于**各个图具有独特的特征和结构**，从一个数据集到另一个数据集，或从一个领域到另一个领域的知识迁移并非易事。图在大小、连通性、节点类型、边类型和整体拓扑结构等方面可能存在显著差异，这使得它们之间的知识直接迁移变得困难。尽管 LLMs 由于在海量语料库上的广泛预训练，在语言任务中展现出良好的零样本 / 少样本能力，**但利用 LLMs 中嵌入的知识来增强图相关任务的可迁移性的探索相对有限。**OFA [Liu et al., 2023a] 尝试通过将所有节点和边描述为人类可理解的文本，并使用单个 LLMs 将不同领域的文本嵌入到同一嵌入空间中，从而以一种统一的方式在图上执行跨领域任务。提高可迁移性仍然是一个值得研究的课题。
>
> ### 提高可解释性
>
> 可解释性，也称为可理解性，是指以人类可理解的方式解释或呈现模型行为的能力 [Zhao et al., 2023b]。在处理图相关任务时，LLMs 比 GNNs 表现出更好的可解释性，这主要是因为 LLMs 的推理和解释能力可以为图推理生成用户友好的解释，包括生成第 3 节中讨论的作为增强器的额外解释，以及提供第 4 节中讨论的作为预测器的推理过程。一些研究已经探索了提示范式中的解释技术，如上下文学习 [Radford et al., 2021] 和思维链 [Wei et al., 2022b]，这些技术通过向 LLMs 输入一系列演示和提示，引导其生成特定方向的内容并解释其推理过程。还需要进一步探索以增强可解释性。
>
> ### 提高效率
>
> 尽管 LLMs 在图学习方面表现出有效性，但它们在**时间和空间上**可能**存在效率问题**，特别是与专门的图学习模型（如 GNNs）相比，后者本质上是处理图结构的。当 LLMs 依赖于第 4 节中讨论的序列图描述进行预测时（图作为预测器），这种情况尤为明显。例如，通过 API（如 ChatGPT 和 GPT-4）访问 LLMs 时，处理大规模图的计费模式会产生高昂成本。此外，本地部署的开源 LLMs 的训练和推理都需要大量时间和大量硬件资源。现有研究 [Duan et al., 2023; Liu et al., 2023c; Ye et al., 2023; Chai et al., 2023; Liu et al., 2023d; Tang et al., 2023] 尝试通过采用参数高效微调策略，如 LoRA [Hu et al., 2021] 和前缀微调 [Li and Liang, 2021]，来实现 LLMs 的高效适配。我们认为，更高效的方法可能会在有限的计算资源下释放 LLMs 在图相关任务中的更多潜力。
>
> ### 分析和提高表达能力（重点看）
>
> 尽管 LLMs 近年来在图相关任务中取得了成就，但其理论表达能力在很大程度上仍未被探索。众所周知，标准的消息传递神经网络的表达能力与 1-Weisfeiler-Lehman（WL）（**这是图理论中一种图同构测试算法**）测试相当，这意味着它们无法区分 1 跳聚合下的非同构图 [Xu et al., 2018]。因此，出现了两个基本问题：**LLMs 对图结构的理解效果如何？它们的表达能力能否超过 GNNs 或 WL 测试？**此外，置换等变性是典型 GNNs 的一个有趣特性，这在几何图学习中具有重要意义 [Han et al., 2022]。探索如何赋予 LLMs 这种特性也是一个有趣的方向。
>
> > **现状问题**：LLM 在图任务中效果虽好，但**表达力和结构理解机制仍缺乏理论支撑**；
> >
> > **研究方向**：
> >
> > - 探索 LLM 对图结构的真正理解能力；
> > - 看 LLM 是否有潜力超越 GNN 表达能力上限；
> > - 如何让 LLM 拥有 GNN 的“置换等变性”，以提升泛化和结构感知能力。
>
> ### 作为智能体的 LLMs
>
> 在当前图与 LLMs 的融合中，LLMs 通常扮演增强器、预测器和对齐组件的角色。然而，在更复杂的场景中，这些应用可能无法充分释放 LLMs 的潜力。最近的研究探索了 LLMs 作为智能体的新角色，如生成式智能体 [Park et al., 2023] 和特定领域智能体 [Bran et al., 2023]。在 LLMs 驱动的智能体系统中，LLMs 充当智能体的 “大脑”，并得到规划、记忆和工具使用等基本组件的支持 [Weng, 2023]。在复杂的图相关场景中，如推荐系统和知识发现，将 LLMs 视为智能体，首先将任务分解为多个子任务，然后为每个子任务识别最合适的工具（如 GNNs），可能会产生更好的性能。此外，将 LLMs 用作智能体有望构建一个强大且高度通用的图相关任务求解器。





## A Fine-Tuning Approach for T5 Using Knowledge  Graphs to Address Complex Tasks

这篇文章好像是和问答有关的

通过微调知识图和T5模型，可以有效地提高模型在**特定领域任务**中的性能。（这篇文章特别关注**特定领域**）

本研究旨在探索基于知识图的T5模型的**微调优化方法**。具体来说，我们将==**研究如何将知识图中的实体信息和关系信息整合到T5模型的训练过程中**==，以提高其在特定领域任务中的性能。

1. 我们将分析知识图和语言模型的**结合**，并提出一种有效的**微调策略**，将知识图作为T5模型的辅助信息进行训练，使模型能够更好地在任务中利用结构化知识。
2. 我们将通过实验验证该方法在不同领域的任务中的效果，并评估该模型在知识推理和任务理解方面的改进。
3. 我们希望通过本研究为大型语言模型在专业领域的应用提供新的思路和方法。



### 补充知识

* 如何理解 LLM seq2seq 模型中 推理准确性、上下文理解能力与处理复杂问题的能力

  > **1. 推理准确性**
  >
  > 指模型从文本中准确推导出问题答案的能力，核心是 “答案的正确性”。
  > **例子**：
  > 若输入段落为 “《时间简史》由斯蒂芬・霍金于 1988 年出版，该书探讨了宇宙起源、黑洞等前沿物理问题”，问题为 “《时间简史》的作者是谁？”。
  >
  > - 若模型输出 “斯蒂芬・霍金”，则推理准确；若输出 “爱因斯坦”，则推理错误。
  >   论文中，基于知识图谱的 T5 模型（Ours）推理准确性达 85.2%，显著高于 GPT-2（72.5%）、BERT（78.3%）等基线模型，说明其在正确提取答案方面更优。
  >
  > **2. 上下文理解能力**
  >
  > 指模型对文本中隐含逻辑、指代关系、上下文关联的把握能力，核心是 “对文本语境的深层理解”。
  > **例子**：
  > 若输入段落为 “居里夫人发现了镭和钋两种元素，她因此成为首位两获诺贝尔奖的人。这种放射性元素的发现推动了核物理的发展”，问题为 “文中‘这种放射性元素’指的是什么？”。
  >
  > - 模型需要理解 “这种” 指代前文的 “镭和钋”，才能正确回答。
  >   论文中，该指标得分最高的模型（Ours）为 82 分，远高于 GPT-2 的 68 分，说明其能更好地捕捉文本中的上下文关联。
  >
  > **3. 处理复杂问题的能力**
  >
  > 指模型解决需要多步推理、依赖背景知识或跨句关联问题的能力，核心是 “多环节推理的连贯性”。
  > **例子**：
  > 若输入段落为 “特斯拉由马斯克创立，总部位于美国得克萨斯州。该公司以电动汽车和太阳能产品闻名，其 Model 3 是全球畅销的电动车之一”，问题为 “马斯克创立的公司总部在哪里？”。
  >
  > - 问题需要两步推理：①确定 “马斯克创立的公司” 是特斯拉；②找到特斯拉的总部是得克萨斯州。
  >   论文中，该指标的最优模型（Ours）得分为 83 分，高于 T5 基线模型的 77 分，体现其在多步推理任务中的优势。
  >
  > 
  >
  > 这三个指标从 “正确性”“语境把握”“复杂推理” 三个维度，全面评估了模型在知识密集型问答任务中的表现，而实验数据表明，引入知识图谱的 T5 模型在这三方面均显著优于传统模型。
  >
  > 指模型从文本中准确推导出问题答案的能力，核心是 “答案的正确性”。
  > **例子**：
  > 若输入段落为 “《时间简史》由斯蒂芬・霍金于 1988 年出版，该书探讨了宇宙起源、黑洞等前沿物理问题”，问题为 “《时间简史》的作者是谁？”。
  >
  > - 若模型输出 “斯蒂芬・霍金”，则推理准确；若输出 “爱因斯坦”，则推理错误。
  >   论文中，基于知识图谱的 T5 模型（Ours）推理准确性达 85.2%，显著高于 GPT-2（72.5%）、BERT（78.3%）等基线模型，说明其在正确提取答案方面更优。



### 相关工作

近年来，BERT、GPT和T5等大型语言模型在自然语言处理任务中取得了显著成功。然而，这些模型在推理和领域**特定适应性方面**仍然面临挑战。微调策略已被广泛探索以提高模型性能，最近的研究**侧重于提高效率和整合结构化知识**。

1. Yang等人[11]介绍了一种高效的微调框架，该框架在**优化模型自适应的同时降低了计算成本，为提高大规模模型性能提供了见解**。
2. 同样，李[12]提出了一种增强的Transformer架构，该架构在知识提取过程中**改进了特征对齐**，这与**将外部知识集成**到预训练模型中高度相关。
3. 此外，还有研究探索模型架构对处理复杂信息的影响，如 Wang 对基于 Transformer 的结构化数据处理方法的研究，Du 引入的优化深度学习技术，以及 Wang 对整合多种数据源方法的研究



虽然现有的研究已经探索了**模型优化**、**结构化推理**和**特征对齐**，但将知识图有效地整合到T5微调中的挑战仍未得到充分探索。本研究在这些进步的基础上，将知识图嵌入系统地整合到T5微调过程中，展示了它们对推理准确性和上下文理解的影响。这些发现有助于提高大型语言模型处理复杂知识的能力，为提高其可解释性和解决问题的能力提供了新的方向。

* 模型优化包括

  **高效微调框架优化**

  **架构优化以增强知识整合**

  **引入结构化知识优化推理**

* 结构化推理就是

  例如在问答任务中，模型可借助知识图谱中实体间的明确关系（如 “作者 - 作品”“国籍 - 人物”），更高效地完成多步推理，属于通过整合外部结构化信息实现的模型优化

* 特征对齐

  具体来说，在自然语言处理任务中，当需要将知识图谱等外部结构化知识整合到预训练语言模型（如 T5）时，特征对齐能帮助模型更好地匹配文本数据与结构化知识的特征表示，确保两者在语义空间中具有一致性，从而提升模型对知识的利用效率和任务性能。例如，在知识提取任务中，通过优化特征对齐，模型可以更准确地识别文本中的实体与知识图谱中实体的对应关系，以及文本中隐含的关系与知识图谱中关系的匹配，为后续的推理等任务奠定基础。



### 方法

T5模型是一个 `seq2seq` 模型。T5模型的预训练任务采用填空任务，即通过自监督学习方法使用大量未标记的文本数据来学习模型的参数。

其目标是将这种==**结构化的知识信息嵌入T5的输入中**==，以帮助模型理解并生成更符合领域知识的输出。

所以我们需要考虑的是如何嵌入输入到 T5 模型中

1. 我们通过一些 `pre-trained` 知识图谱嵌入模型得到关于实体（节点）和关系（边）的低维嵌入，比如说 $v_i \in R^d,e_j \in R^d$

2. 我们将上述的向量作为辅助信息，其将于文本输入一起输入 T5 模型进行训练。具体来说，假设我们需要处理一个特定的文本任务，例如问答任务，其中问题q的输入序列为q=[w1，w2，…，wn]，其中wi表示问题中的第i个单词。在传统的T5模型中，我们直接将问题序列作为输入，而在我们的模型中，将知识图实体的嵌入向量vi和e j以及与问题相关的关系作为额外的输入信息，形成新的输入序列x=[q，vi，e j]。这些嵌入向量与文本序列一起处理到T5的**编码器**中，以增强模型的领域知识和推理能力。

3. 损失函数的设计

   在传统文本预测损失 $L(y,y')$ 的基础上，引入了知识图谱中实体与关系嵌入的相似度项，通过权重系数 $\lambda$ 构建新的损失函数：
   $$
   L’(y,y',v_i,e_j) = L(y,y') + \lambda \cdot Sim(v_i,e_j)
   $$
   这个 $Sim$ 函数其实就是余弦相似度。==它在度量什么东西啊？==

   > 在这篇论文中，对实体（节点）嵌入\(v_i\)和关系（边）嵌入\(e_j\)计算相似度\(Sim(v_i, e_j)\)，并将其纳入损失函数，核心目的是==**引导模型在微调过程中充分利用知识图谱中实体与关系的语义关联，确保结构化知识能有效融入模型推理过程**。==
   >
   > ==例如 “牛顿”（实体）与 “作者是”（关系）、“《自然哲学的数学原理》”（实体）的嵌入应具有较高相似度，从而帮助模型在推理时正确调用这些关联知识。==

   这种相似性度量有助于确保模型在微调过程中充**分利用与任务相关的知识图信息**。通过优化损失函数L'，T5模型能够更好地**将结构化知识**整合到知识图中进行推理，同时保持其语言能力。



### 实验

这一趋势表明，知识图的规模直接影响T5模型的性能，更大规模的知识图可以提供更多的背景信息，帮助模型更好地理解和推理。特别是在处理复杂问题时，知识图的扩展使模型能够掌握更多相关信息，从而提高其推理和解决问题的能力。这进一步证明了知识图在提高大型语言模型推理性能方面的重要作用。



### 结论

本研究探索了基于知识图的T5模型微调方法，并证明知识图可以显著提高**推理准确性、上下文理解能力和处理大型语言模型复杂问题的能力**。通过与其他基线模型的比较实验，我们发现添加了知识图的T5模型不仅在推理任务中表现良好，而且有效地提高了模型对复杂问题的理解和推理能力。特别是，随着知识图规模的增加，模型的性能逐渐提高，表明知识图在提高模型的推理和理解能力方面发挥着重要作用。消融实验进一步验证了知识图中实体和关系嵌入的关键性，并证明了完整知识图在模型微调中的必要性。综上所述，基于知识图的T5微调方法为自然语言处理任务中的推理和复杂问题处理提供了新的思路和方法。



## GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily

[原文链接](https://arxiv.org/pdf/2105.13795)

[知乎链接](https://zhuanlan.zhihu.com/p/380639504)





## Pretraining Language Models with Text-Attributed Heterogeneous Graphs

最大的收获就是学习到了==拓扑感知思想== 以及一项==将节点文本特征与结构特征有效融合的技术（公式（3））==

[知乎链接](https://zhuanlan.zhihu.com/p/26098683075)

提出了一种新的框架，将文本属性或异构图中的**拓扑**和**异质信息**整合到语言模型中。

作者洞察到的点是：

1. 以前的 HGNN 不会去注意到图的文本信息（使用 LM 生成 textual representations）
1. 现有语言模型忽视图的高阶拓扑信息（引入了**拓扑感知**）
1. 节点文本信息不平衡（富文本节点 vs 贫文本节点）（使用了文本增强手段）

==论文中的公式（3）我还不是特别能接受（现在稍微能接受一点了）==，这种做法无法完全说服我。事实上，这篇文章的创新点与贡献就是

1. 作者引入了一个==**拓扑感知**==的预训练任务去预测文本图的节点是否在文本图中。这可以使得LM可以**利用高阶**的信号。具体体现在通过 “上下文图预测（CGP）” 任务，将目标节点的**K 阶邻居（高阶拓扑）** 纳入学习范围。映射矩阵在此过程中帮助融合 “文本语义” 与 “多阶结构关联”，使语言模型同时学到文本信息和图的拓扑关联。
2. 将图结构信息与图文本信息进行了**融合**，其桥梁就是邻居节点节点类型的可学习映射矩阵（用这个矩阵把这个融合的过程给“玄学”了进去）实际上可以这样想，不同类型节点的结构表示与文本表示的分布可能差异很大，$W$ 矩阵的作用是**将特定类型节点的结构表示转换到与文本表示兼容的向量空间**。它以节点类型为基，设定转换矩阵感觉是合理的。假设在节点类型相同的情况下（A 与 B），A 与 B 的结构嵌入不同，可能预示着它们存在一些不同，即便节点类型不一样，而这一不同通过 $W$ 矩阵映射到语义空间，这实际上是一个隐空间，但随着学习，它会逐渐与我们原本的语义空间对齐。

### 方法论

#### 拓扑感知预训练任务

当下的 PLM 无法用文本描述捕获节点之间的连接，即无法解决一些关于图的拓扑结构。有一些工作针对此问题进行了一些研究，但它们只关注节点之间一阶连接的建模，而忽略了高阶信号（只解决了一阶邻域的问题而忽略了高阶邻域的问题），这在网络分析等领域被证明是必不可少的。

为此，我们提出了一种拓扑感知预训练任务（即上下文图预测），**用于帮助LM捕获不同节点之间的多阶连接**。

##### 上下文图提取

理解定义即可，很好理解，这是符合直觉的。

##### 上下文图预测

其目标是将图神经网络的图学习能力注入语言模型（如何理解呢？）

TAGG不仅包含多种类型的节点和关系，还涉及节点的文本描述。与大多数PLM对单个文本进行预训练不同，我们提出了**上下文图预测** Context Graph Prediction（CGP），用于在TAHG上对LM进行**预训练**，**以捕获丰富的信息**。由于LMs已被证明在文本建模方面非常强大（Devlin等人，2019；Brown等人，2020），**CGP的目标是将图神经网络的图学习能力（Bing等人，2022）注入LMs。**

充分理解公式（1）（2）（3）！！！

> **Context Graph Prediction（上下文图预测）的核心是让模型学习判断节点间是否存在多阶拓扑关联（而非单纯的 “连接”）**，具体来说：
>
> 
>
> 1. 它通过语言模型编码的文本语义表示（公式 2）与图神经网络编码的结构表示（公式 1）交互，预测节点是否属于目标节点的多阶邻域（上下文图）。
> 2. 这里的 “关联” 不仅包括直接连接（一阶），还包括通过其他节点传递的间接连接（高阶），即论文中定义的 “k 阶路径可达关系”。
> 3. 例如，若节点 u 的上下文图包含节点 v（通过 k 阶路径关联），则模型需预测 $\hat{y}_{u,v} \approx 1$，反之则 $\hat{y}_{u,v} \approx 0$（公式 3）。
>
> 
>
> 简言之，上下文图预测通过联合优化文本语义与图拓扑结构，让模型学会 “推断节点间是否通过多阶路径形成拓扑关联”，为后续下游任务（如链接预测）提供支撑。

* 如何理解 “**为了捕捉节点 $u$ 的异质性**，我们在预训练语言模型的最后一层引入了一个投影头。$X_u$ 表示节点u的文本描述。”为什么这样的做法**能捕捉节点 $u$ 的异质性**？

  ==其实就是在捕捉节点的类型特征！！！这对我们的工作具有比较强的启发性==

  > 在预训练语言模型（LM）的最后一层引入投影头（projection header）以捕捉节点$u$的异质性，核心原因在于**通过类型感知的特征转换，让语言模型的语义表示适配节点的类型差异，从而区分不同类型节点的固有特性**。具体可从以下角度理解：
  >
  > 1. **节点异质性的本质**
  >
  > 文本属性异构图（TAHGs）中的 “异质性” 主要指节点类型的多样性（如学术网络中的 “论文”“作者”“关键词”），以及不同类型节点在语义、功能和拓扑角色上的差异：
  >
  > - 语义上：论文节点有长文本（标题 + 摘要），作者节点只有名字（短文本），关键词节点是术语（极短文本），文本的长度和语义密度差异显著。
  > - 拓扑角色上：论文节点与作者节点通过 “合作” 关系连接，与关键词节点通过 “标注” 关系连接，不同类型节点在图中的连接模式（关系类型、邻接节点类型）完全不同。
  >
  > 2. **语言模型的局限性**
  >
  > 现有语言模型（如 BERT）的核心是学习文本的通用语义表示，但**无法直接区分节点的类型差异**：
  >
  > - 即使输入文本属于不同类型的节点（如 “论文标题” 和 “作者名字”），语言模型输出的表示可能仅反映文本字面语义，而忽略 “这是论文” 还是 “这是作者” 的类型信息。
  > - 对于文本极少的节点（如关键词、作者），语言模型的输出可能无法有效编码其类型特性（例如，“关键词” 的语义应与它标注的论文强相关，而 “作者” 的语义应与合作的论文强相关）。
  >
  > 3. **投影头的作用：注入类型感知**
  >
  > 投影头本质是一个**与节点类型相关的可训练转换模块**，其核心功能是将语言模型输出的通用语义表示，映射到更贴合节点类型特性的特征空间，从而捕捉异质性：
  >
  > - 对于不同类型的节点（如 “论文”“作者”“关键词”），投影头会使用不同的转换参数（例如，论文节点用$W_{\text{论文}}$，作者节点用$W_{\text{作者}}$），强制模型为不同类型的节点学习差异化的表示。
  > - 这种转换能让语言模型的语义表示不仅包含文本信息，还隐含节点的类型属性，使其在后续的图任务（如上下文图预测）中，能更好地与异构图神经网络学习的结构表示（依赖节点类型和关系）对齐。
  >
  > **举例说明**
  >
  > 假设两个节点：
  >
  > - 节点$u_1$（类型：论文），文本为 “Graph Neural Networks in NLP”；
  > - 节点$u_2$（类型：关键词），文本为 “Graph Neural Networks”。
  >
  > 若仅用语言模型编码，两者的文本有重叠，输出表示可能相似；但通过类型专属的投影头后：
  >
  > - $u_1$的表示会被调整为更贴合 “论文” 的语义空间（强调完整主题、研究内容）；
  > - $u_2$的表示会被调整为更贴合 “关键词” 的语义空间（强调核心概念、分类功能）。
  >
  > 这种差异正是对节点异质性的捕捉 —— 即使文本相似，不同类型节点的表示也能反映其在图中的独特角色。
  >
  > 综上，投影头通过引入类型相关的特征转换，弥补了语言模型对节点类型不敏感的缺陷，使输出的语义表示既能编码文本信息，又能体现节点的类型特性，从而有效捕捉异质性。

##### 预训练过程

充分理解为什么不直接全图计算而是使用负采样技术？

把论文中**对应的损失公式**理解即可！

在这个工作中，作者使用BERT以及 R-HGCN。

> 这个公式是论文中预训练过程的总损失函数，用于联合优化语言模型和图神经网络。我们可以分部分拆解理解：
>
> **公式整体结构**  
>
> 总损失 $\mathcal{L}_u$ 由两部分组成：  
> $$
> \mathcal{L}_u = \mathcal{L}_u^{MLM} + \mathcal{L}_u^{CGP}其中：
> $$
>
> - $\mathcal{L}_u^{MLM}$：掩码语言建模（Masked Language Modeling）损失，用于优化语言模型对文本的理解能力。  
> - $\mathcal{L}_u^{CGP}$：上下文图预测（Context Graph Prediction）损失，用于优化模型对图拓扑结构的捕捉能力。  
>
> **1. 掩码语言建模损失 $\mathcal{L}_u^{MLM}$**  
> $$
> \mathcal{L}_u^{MLM} = -\log P\left(\tilde{X}_u \mid X_{u \setminus \tilde{X}_u}\right)
> $$
>
> - **含义**：这是BERT等预训练语言模型的经典损失，目标是让模型根据未被掩码的文本，预测被掩码的部分。  
> - 变量解释：  
>   - $X_u$：节点$u$的原始文本描述（如论文的标题+摘要、作者的名字等）。  
>   - $\tilde{X}_u$：对$X_u$进行掩码处理后的"损坏文本"（例如随机掩盖40%的token）。  
>   - $X_{u \setminus \tilde{X}_u}$：$X_u$中未被掩码的部分。  
>   - $P\left(\tilde{X}_u \mid X_{u \setminus \tilde{X}_u}\right)$：模型根据未掩码文本预测被掩码token的概率。  
> - **作用**：确保语言模型能正确理解节点文本的语义（如语法、上下文关联），为后续的图任务提供可靠的文本表示。  
>
> **2. 上下文图预测损失 $\mathcal{L}_u^{CGP}$**  
> $$
> \mathcal{L}_u^{MLM} = -\log P\left(\tilde{X}_u \mid X_{u \setminus \tilde{X}_u}\right)
> $$
> 这部分是针对图拓扑结构的损失，通过正样本和负样本的对比优化模型：  
>
> - **第一部分（正样本损失）**：$-\sum_{v \in \mathcal{V}_u^K} \log \hat{y}_{u,v}$  
>   - $\mathcal{V}_u^K$：目标节点$u$的上下文图中的节点集合（即与$u$存在k阶路径关联的正样本节点）。  
>   - $\hat{y}_{u,v}$：模型预测节点$v$属于$u$的上下文图的概率（公式3计算得到）。  
>   - **作用**：通过最大化正样本的预测概率（让$\hat{y}_{u,v} \approx 1$），迫使模型学习"正样本节点与$u$的拓扑关联"。  
>
> - **第二部分（负样本损失）**：$-\sum_{i=1}^{5} \mathbb{E}_{v_i' \sim P_n(\mathcal{V} \setminus \mathcal{V}_u^K)} \log \left(1 - \hat{y}_{u,v_i'}\right)$  
>   - $\mathcal{V} \setminus \mathcal{V}_u^K$：图中不属于$u$的上下文图的节点集合（负样本池）。  
>   - $v_i' \sim P_n(\cdot)$：从负样本池中随机抽取5个负样本（遵循论文的1:5正负比例）。  
>   - $\hat{y}_{u,v_i'}$：模型预测负样本节点$v_i'$属于$u$的上下文图的概率。  
>   - **作用**：通过最小化负样本的预测概率（让$\hat{y}_{u,v_i'} \approx 0$），迫使模型区分"与$u$相关的节点"和"无关的节点"。  
>
> **总结**  
>
> 整个公式通过**多任务联合优化**实现两个核心目标：  
> 1. 让语言模型**学好文本语义**（通过$\mathcal{L}_u^{MLM}$）；  
> 2. 让模型同时**学好图中节点的多阶拓扑关联**（通过$\mathcal{L}_u^{CGP}$的正负样本对比）。  
>
> 最终，模型能输出同时包含文本语义和图结构信息的节点表示，为下游任务（如链接预测、节点分类）提供更全面的特征支持。





#### 文本增强策略

如第1节所述，TAHG中不同类型节点的文本描述因长度不同而不同，从而产生富文本节点和无文本节点。富文本节点的详尽描述可以很好地揭示它们的特征，而无文本节点的简要描述不足以反映它们的语义，仅对这些描述进行编码会导致次优性能。因此，我们设计了一种**文本增强策略**来解决不平衡问题，该策略首先通过根据TAHG中的连接组合其**邻居的文本描述来丰富无文本节点的语义，然后通过LM计算增强文本**。

* 针对富文本节点

  `[CLS] X_u [SEP]`

* 针对少文本节点，将其自身文本与邻居节点的文本进行拼接

  `[CLS] X_u [SEP] X_{v1} [SEP] X_{v2} [SEP] ... [SEP] X_{vk} [SEP]`

我们实证发现，文本增强策略可以带来显著的改进，而不会显著增加模型的复杂性。

> 1. **解决文本不平衡问题**：少文本节点（如作者 “Tao Zou”）的原始文本仅包含名称，通过拼接其发表的论文标题和摘要（富文本邻居），可生成包含研究方向、合作关系等信息的增强表示。
> 2. **保留拓扑关联**：增强文本中显式包含邻居节点的文本，使语言模型在编码时自然捕捉节点间的间接语义关联（如作者与论文的 “发表” 关系），与图的拓扑结构形成呼应。
> 3. **兼容性**：增强后的文本仍可直接输入现有语言模型（如 BERT），无需修改模型架构，仅通过输入格式调整即可实现性能提升。



#### 下游任务的微调

在预训练过程之后，我们丢弃了辅助异构图神经网络 $f_{HGNN}(\cdot)$，仅应用预训练的LM $f_{LM}(\cdot)$来基于方程（2）生成节点的语义表示。我们选择了两个与图相关的下游任务进行评估，包括链路预测和节点分类。我们在 $f_{LM}(\cdot)$ 的顶部使用各种标头进行详尽的比较，包括多层感知器（MLP）、RGCN（Schlichtkrull等人，2018）、HetSANN（Hong等人，2020）和R-HGNN（Yu等人，2022）。对于下游任务，为了提高效率，$f_{LM}(\cdot)$ 被冻结，只有 header 可以微调。



### 实验

#### 性能比较

1. **融入图的结构信息能显著提升下游链接预测任务**的性能。此外，RoBERTa 在节点分类任务中相较于其他基线模型表现突出，这说明**利用更优的语言表示能进一步提升整体性能**。
2. 其次，我们观察到，**仅捕捉网络嵌入**的 MetaPath 方法在所有评估方法中**性能最差**。然而，当 MetaPath **与语义信息结合后（即 MetaPath+BERT），其性能达到了与其他方法相当甚至更优的水平**。这**凸显了为每个节点同时融入结构信息和文本语义表示的重要性**。
3. 此外，OAG-BERT 在学术领域数据集（OAG-Venue）上表现出竞争力，这得益于其在学术知识上的预训练。但由于它主要聚焦于论文与元数据（如作者、机构）的关联，**忽略了高阶结构信息**，因此性能仍低于 THLM。
4. 最后，THLM 显著优于现有模型，原因如下：1）通过**捕捉多阶图拓扑邻近性，整合了更全面的图结构信息**；2）通过聚合邻居的文本描述，增强了少文本节点的语义表示，有效缓解了文本不平衡问题。



#### 个人感受（知乎博主的感受）

从表2可以看出，作者提出的GCP好像么有啥特别大的效果(数值上与w/ RGCN对比)。但是，作者的方法又比其他人的方法好一点点，那么，其原因可能是：作者提出的第二部分：文本增强起作用了。**这个文本增强的方法，也类似于24年唐杰组的虚拟节点**



## Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs

[知乎链接](https://zhuanlan.zhihu.com/p/23340672251)



个人感觉 TAG 这个方向还是很有潜力的（毕竟能和 大语言模型相结合？）

P2TAG 是 “Pre-training and Prompting for few-shot node classification on TAGs” 的缩写，是一种专为文本属性图（Text-Attributed Graph，TAG）上少样本节点分类设计的框架。

### 前置铺垫

* 理解 few-shot

  few-shot 是小样本学习的概念，就是说标注量并不十分充足



### 摘要

文本属性图（TAG）是一种重要的现实世界图形结构数据，每个节点都与原始文本相关联。对于TAG，传统的少样本节点分类方法直接对预处理的节点特征进行训练，不考虑原始文本。性能在很大程度上取决于特征预处理方法的选择。本文提出了P2TAG1，这是一个为TAG上的少镜头节点分类而设计的框架，具有**图形预训练和提示功能**。P2TAG首先在具有自监督损失的TAG上预训练语言模型（LM）和图神经网络（GNN）。**为了充分利用语言模型的能力，我们将掩码语言建模目标应用于我们的框架**。然后，将预训练的模型用于少数镜头节点分类，**采用混合提示方法，同时考虑文本和图形信息。**我们对六个真实世界的标签进行了实验，包括论文引用网络和产品共同购买网络。实验结果表明，我们提出的框架在这些数据集上优于现有的图 few-shot 学习方法，提高了+18.98%-+35.98%。



如何理解这里的动机？

> 对于 TAG，传统的 few-shot 节点分类方法直接对预处理后的节点特征进行训练，不考虑原始文本。性能在很大程度上取决于特征预处理方法的选择。



这样理解：

> **TAG 场景**
>  TAG（Text-Attributed Graph）指的是**文本属性图**，也就是图中的节点除了有结构信息（邻接关系），还带有文本描述，比如论文引用网络里的每篇论文节点带有摘要、社交网络用户节点带有个人简介等。
>
> **传统 few-shot 节点分类方法**
>
> - 在 **few-shot** 场景下，我们只有少量带标签的节点，目标是利用有限的标注信息去分类未标注节点。
> - 传统做法通常**不直接用原始文本**，而是**先把文本转成节点特征向量**（预处理阶段，比如用 TF-IDF、word2vec、BERT embedding）。
> - 这些方法会在这个“预处理好”的向量上直接训练模型（如 GNN、MLP）。
>
> **不考虑原始文本**
>
> - 由于这些方法用的是**已经处理好的静态特征**，训练时不会去动态地建模文本。
> - 这意味着它们无法利用原始文本里的细粒度信息、上下文语义，或者**针对任务调整文本表示**。
>
> **性能依赖于特征预处理**
>
> - 因为特征是在训练前固定好的，所以**特征质量直接决定了模型效果**。
> - 如果预处理方法（比如选的词向量、文本编码器）很强，性能就好；如果编码方法弱（比如只用 TF-IDF），即使后面的模型很强，最终效果也可能很差。
> - 这就是所谓的“性能在很大程度上取决于特征预处理方法的选择”。
>
> ------
>
> 🔍 **简单总结**
>  这句话的意思是：
>
> > 过去在 TAG 上做 few-shot 节点分类时，通常先把原始文本转成固定特征再训练模型，而不直接利用原始文本。这种做法的效果主要取决于特征预处理的好坏，而不是下游模型本身的能力。



### 引言

* 少样本节点分类任务

* 一些列图预训练方法应运而生，它们借助自监督学习，在**没有标签**的情况下**生成通用的节点表示**

  * 对比式

    对比式自监督学习方法利用数据增强生成数据的多个视图，以计算对比损失。

    例子：

    > 假设我们有一个**学术引用网络 TAG**：
    >
    > - 节点：论文（节点特征是论文标题 + 摘要文本）
    > - 边：引用关系
    >
    > 对比式方法（比如 **GraphCL** 或 **GRACE**）的流程可能是：
    >
    > 1. 取节点 *A*（论文 "Graph Neural Networks for NLP"），生成两个不同的视图：
    >    - **视图1**：随机删掉部分边（比如删掉一个引用关系）。
    >    - **视图2**：在文本特征上做数据增强（比如同义词替换 "NLP" → "Natural Language Processing"）。
    > 2. 用 GNN 分别编码视图1和视图2中 *A* 的节点表示。
    > 3. **对比损失**：
    >    - 让 *A* 的两个视图的表示 **相似度高**（正样本对）。
    >    - 让 *A* 与其他论文节点（比如 *B*）的表示 **相似度低**（负样本对）。
    >
    > 📌 **结果**：模型学到的表示会对不同视图保持稳定，也就是说，它学到的特征在结构扰动或文本扰动下都能保持一致。

  * 生成式

    旨在重建图中（被掩码的）属性

    例子：

    > 还是同样的学术引用网络：
    >
    > 1. 选取一个节点 *A*（论文 "Graph Neural Networks for NLP"）。
    > 2. 在它的文本特征中 **随机掩码**一部分（比如把 "NLP" 这个词替换成 `[MASK]`）。
    > 3. 用 GNN 聚合邻居信息（比如引用的其他论文标题/摘要），并结合剩余的词，预测被掩码的词是 "NLP"。
    > 4. **生成式损失**：
    >    - 计算预测结果和原始词的差异（交叉熵损失）。
    >
    > 📌 **结果**：模型学到的表示不仅能保持上下文一致性，还能捕捉到局部语义和邻居结构，从而具备“补全缺失信息”的能力。
    >
    > **具体的细节还是要自己做一下掩码任务才知道**

  但它们仅考虑了文本属性图（TAGs）上自监督学习的部分内容，**使得图神经网络（GNNs）的训练独立于语言模型（LMs）的文本编码**。（==对吧，我也是这么想的，我觉得分别对图结构与图语义建模就是不太合理的，虽然这两种数据是多模态数据，但是又有不同。应该对这两种嵌入有一个合理的融合或者说一方引导另一方总之就是要有联系==，之前 NIE 工作上对这两者的融合我觉得做得不是很好）

* 最近的一些研究尝试将语言模型（LMs）和图神经网络（GNNs）的训练进行整合。这些方法**直接处理**原始文本和图中的拓扑结构，在下游任务上取得了更好的性能。

  > 过去的做法是：先单独用 NLP 模型（如 BERT）把文本变成向量，再交给 GNN 处理图结构。
  >
  > **缺点**：文本编码和图传播是**分开的**，语言模型不会在训练过程中感受到图结构的影响。
  >
  > 所以最近有一些研究希望 **直接联合训练** LMs 和 GNNs，这样：
  >
  > * 文本表示会根据图的结构信息来调整
  >
  > * 图的结构信息也会根据文本特征优化

  **以往的 TAG 方法很多是分步的**（先编码文本，再跑 GNN），而**最近的趋势是 LM 和 GNN 联合训练**，直接处理**原始文本**和**图结构**，这样效果会更好。不同方法（GIANT、GraphFormer、GLEM）对 LM 与 GNN 的耦合程度不同，从松到紧不等。

* 在少样本节点分类中，提示学习通过提供特定的输入线索，帮助预训练模型从少量示例中进行泛化。理解各种类型的 prompt-type

* 当前工作：P2TAG

  在本文中，我们提出了用于文本属性图（TAG）少样本节点分类的预训练与提示学习（Pre-training and Prompting）方法。在预训练阶段，与以往工作不同，我们的框架通过自监督方式对语言模型（LM）和图神经网络（GNN）进行有效整合与联合训练。针对语言模型与图神经网络整合过程中的挑战，我们提出了若干解决方案。我们将图神经网络的编码器作为语言模型的补充，并保留了语言模型原有的自监督损失，即掩码语言建模目标，这是一项难度较高的 pretext 任务，有助于缓解过拟合问题。为了实现图神经网络与语言模型的联合训练，我们的框架首先通过基于随机游走的采样器对小批量子图进行采样。每个小批量子图会被输入到语言模型中进行文本编码，分类标记（即 [CLS] 标记）的输出将被视为节点（或文本序列）的表示。[CLS] 标记的输出嵌入会被输入到图神经网络中，通过图传播聚合来自邻域的信息。对于掩码语言建模，我们将邻域信息的聚合结果融入 [MASK] 标记的输出中。最后，计算输出结果与原始标记之间的交叉熵损失12。

  

  在提示学习阶段，为了弥合预训练任务与下游任务之间的差距，我们采用了一种结合语言模型和图神经网络的混合方法，这与表 1 中所示的以往工作有所不同。具体而言，我们将通过标签文本初始化的图提示与通过简单语言模型输出初始化的文本提示进行联合拼接，以模拟训练范式。如图 2 所示，通过利用标签文本嵌入，我们能够轻松地将节点与下游标签空间对齐，而无需依赖离散且直观的手工构建提示。第 4.3 节中的实验结果证实了我们所设计提示的卓越效果，与现有的软文本提示方法 G2P2 [36] 相比，性能提升了 11.1%~25.2%3。

* 主要贡献

  * 一个统一的框架：通过掩码语言建模目标联合训练语言模型和图神经网络。
  * 提出了一种新的具有文本和结构信息的图文混合提示学习方法，以缩小预训练和少量下游任务之间的差距。
  * 在实验上效果极佳

### 预备工作

#### Text-Attributed Graph

理解 TAG 上的自监督学习问题。

目标：给每个节点学到一个低维向量表示（embedding），**既能捕捉它的文本语义，又能反映它在图中的结构位置。**

举例：对论文 v1，我们想得到一个向量 z1，比如

```python
z1 = [0.12, -0.38, 0.55, ..., 0.09]  (维度 d=128)
```

这个向量能表示该论文的主题、与其他论文的关系等。 

##### 联合训练 LM 和 GNN

这段话的意思是——在 TAG 上，自监督学习要学到同时包含文本语义和图结构信息的节点表示，而本文的做法是让 **LM 和 GNN 联合训练**，LM 先编码文本，GNN 再在图上传播，这样两者能互相配合提升效果。

> 以前的方法：
>
> 先用 文本编码器（LM） 把 s1 编成一个向量（如用 BERT 得到 768 维）
>
> 再用 GNN 处理这些向量，结合图结构得到新的节点表示
>
> 这两个步骤是分开的，LM 和 GNN 没有一起优化
>
> 这篇论文的做法：
>
> 把 LM 和 GNN 联合训练
>
> LM 负责从原始文本 $S$ 生成初始特征 $X$
> $X=f_{\theta_2}^{LM}(S)$
>
> GNN 负责在图 $G$ 上传播和聚合这些特征，得到最终表示 $Z$
> $Z=f_{\theta_1}^{GNN}(G,X)$
>
> 预测头（Head）用来自监督任务（比如 MLM、邻居预测等）计算损失 $L_{SSL}$，共同更新 LM 和 GNN 的参数
>
> 📌 举例流程（以 MLM 任务为例）：
>
> 输入节点 v1 的文本 "Graph Neural Networks for [MASK]"
>
> LM 把它转成 token 表示（带 [MASK]）
>
> GNN 根据 v1 的邻居（比如 v2、v3）的文本表示，聚合信息补充给 v1
>
> 最后预测 [MASK] 应该是 "NLP"
>
> 计算预测和真实词的差异（损失），更新 LM 和 GNN 的参数



#### Few-shot Node Classification

N-way K-shot 表示本次任务涉及 N 个类别，每个类别在训练时只给 K 个标注节点。还有 Q 表示查询集（其实就是测试集）



> **1. 概念回顾**
>
> * **Few-shot 节点分类**：训练时，每类标签只给你很少几个节点（支持集 Support Set），模型要用它们去预测其他节点（查询集 Query Set）的类别。
> * **N-way K-shot**：
>
>   * **N-way**：本次任务涉及 N 个类别
>   * **K-shot**：每个类别在训练时只给 K 个标注节点
>
> **2. 例子：论文引用网络（TAG）**
>
> 假设我们有一个学术引用网络：
>
> * **节点**：每篇论文（带有标题+摘要作为文本属性）
> * **边**：引用关系
> * **标签集合 C**：论文所属研究领域，比如：
>
>   ```
>   C = {机器学习, 自然语言处理, 计算机视觉, 数据挖掘, 图神经网络}
>   ```
>
> **假设任务参数**
>
> * **N = 3**（3 个类别）
> * **K = 2**（每个类别 2 篇标注论文）
> * **Q = 2**（每个类别 2 篇需要预测的论文）
>
> **步骤 1：随机选择 N 个类别**
>
> 从 C 中随机选取 3 个标签，例如：
>
> ```
> C_t = {机器学习, 自然语言处理, 图神经网络}
> ```
>
> **步骤 2：构造支持集（Support Set）**
>
> * 每个类别选 K=2 篇论文作为训练样本：
>
> ```
> S_t = {
>   (v1, 机器学习), (v2, 机器学习),
>   (v3, 自然语言处理), (v4, 自然语言处理),
>   (v5, 图神经网络), (v6, 图神经网络)
> }
> ```
>
> 这里 $v_i$ 表示论文节点，标签是它所属的研究领域。
>
> **步骤 3：构造查询集（Query Set）**
>
> * 每个类别再选 Q=2 篇论文作为测试样本：
>
> ```
> Q_t = {
>   (v7, 机器学习), (v8, 机器学习),
>   (v9, 自然语言处理), (v10, 自然语言处理),
>   (v11, 图神经网络), (v12, 图神经网络)
> }
> ```
>
> **步骤 4：任务流程**
>
> 1. **训练阶段**：
>
>    * 模型只能看到 **支持集 S_t**（这 6 篇标注的论文）
>    * 学习如何根据节点的文本特征和图结构来区分类别
> 2. **测试阶段**：
>
>    * 给模型看 **查询集 Q_t** 的论文（不提供标签）
>    * 模型预测它们的类别
>    * 与真实标签比较，计算准确率
>
> **3. 对照公式**
>
> * $N = 3$，$K = 2$，$Q = 2$
> * 支持集：
>
>   $$
>   S_t = \{(v_1, c_1), (v_2, c_1), (v_3, c_2), (v_4, c_2), (v_5, c_3), (v_6, c_3)\}
>   $$
> * 查询集：
>
>   $$
>   Q_t = \{(v'_1, c'_1), (v'_2, c'_1), (v'_3, c'_2), (v'_4, c'_2), (v'_5, c'_3), (v'_6, c'_3)\}
>   $$
>
> ---
>
> ✅ **一句话总结**：
> 这段话的意思就是在 **Few-shot 节点分类** 中，每个任务选 N 个类别，每类用 K 个标注节点训练，Q 个节点测试。比如在一个论文网络中，你只看每个领域的少量样本，就要学会去预测其他论文属于哪个领域。



### 方法论

#### 预训练框架

我们采用**预训练**（应该就是指掩码学习）与提示学习的范式来解决少样本节点分类问题，具体如图 1 所示。为了更好地利用文本属性图（TAGs）的独特特征，我们提出了一种新的自监督预训练框架。

该部分主要介绍了 P2TAG 在文本属性图（TAG）上的预训练思路和设计。

1. **动机与改进点**
    以往 TAG 上的自监督方法分两步走：

   - 先用 Bag-of-Words、word2vec 或预训练语言模型（LM）将原始文本编码成节点特征；
   
   - 再将这些特征输入 GNN 做自监督学习。
      这种分离式流程容易造成==信息损失==（如何理解这里提到的信息损失）。
       
       > 1. **文本编码阶段的信息压缩与失真**
       >
       > 先不谈使用 Bag-of-Words、word2vec，编码文本嵌入，即便是使用预训练 LM，将其输出的 [CLS] token 作为节点表示时，也可能无法保留文本的**细微差异**。预训练语言模型虽然能生成较为丰富的词向量，但这些向量**通常是基于上下文的短期依赖**，而图的结构信息（如节点之间的关系）在特征编码阶段未被充分利用。
       >
       > 例子：
       >
       > > 两篇论文节点，使用 Bert 编码它们的标题和摘要。分别得到对应的向量 $C_a$ 与 $C_b$。这两篇论文虽然是 cs 中不同领域的内容，但都引用了某些关键技术论文，这一信息在以往的做法中并没有充分利用起来。但不排除会在后续传播时间接利用，可是这样的话效果可能会更弱。
       >
       > 但是这一点在我们的 NIE 工作中是不存在的。。。
       >
       > 2. ~~**图结构与文本信息的割裂**~~
       >
       > 传统方法中，文本编码与 GNN 训练是独立的：文本编码阶段不考虑节点间的拓扑关系（如图结构），而 GNN 阶段仅基于已编码的**静态节点**特征进行信息聚合，无法反向调整文本**编码过程**。这种割裂会导致 GNN 无法利用图结构优化文本特征的表达，例如，两个语义相似但拓扑位置不同的节点，其文本特征可能被编码为相似向量，而 GNN 难以通过结构信息修正这种偏差。
       >
       >  NIE 以往工作的做法也是先使用 word2vec 与 transformer-XL 来生成**静态的特征**，虽然在下游过程中文本特征其实是利用了图的拓扑结构的，也会从目标节点的邻域去聚合邻居节点的文本嵌入。但是在编码过程也即预处理过程中它并没有充分利用图的结构信息
       >
       > 3. **自监督目标与下游任务的错位**
       >
       > 分离式流程中，GNN 的自监督目标（如对比学习、属性重构）仅基于预处理后的节点特征设计，与文本的原始语义关联较弱。而 P2TAG 通过联合训练 LM 和 GNN，将掩码语言建模（MLM）作为自监督目标，**直接关联文本原始 token 与图结构信息**，减少了因目标错位导致的信息损耗
       >
       > **简而言之就是将自监督的目标提到预训练阶段来处理**
       
       **P2TAG** 改为 **端到端联合训练 LM 与 GNN**，直接在原始 TAG 上学习表示。
       
    - ==总结一下这个动机与改进点对于当下 NIE 任务的启发==
    
       > 1. 最核心的一点就是此工作直接在**文本编码**阶段也即所谓的预处理阶段直接利用了图结构信息。以往的工作包括最前沿的 NIE 的做法也是在文本编码时只考虑了**当前节点文本特征**。这种做法仅是基于上下文的短期依赖，它不会利用周遭节点的特征，缺乏图的结构信息。
       >
       >    当然，在下游过程的编码-解码处理过程中还是会充分利用图结构信息的。
       >
       > 2. 关于 struct_feat。由于它是使用 node2vec 编码，因此在编码过程（预处理过程）就已经充分利用了图的结构信息。但事实上我们完全可以考虑在节点结构编码时利用一些异构信息！！！
       >
       > 3. 所以这个工作对我们的启发就是利用 LM 与 GNN 在编码的预处理阶段入手！提高静态的输入特征（嵌入的质量）。但是我们还得证明在 NIE 这个问题上，花精力提升上游给下游的输入质量确实比提高下游的输出质量有效！
       >
       >    P2TAG 的想法是：
       >
       >    1. 在 LM 阶段就让 token 表示吸收邻域结构信息，避免 token-level 信息在节点聚合前就被压缩/丢失。
       >    2. 这样一来，进入 GNN 阶段的节点表示已经是“结构增强版”，传播时更容易学习全局模式。
       >
       >    这种方法在少样本（few-shot）和稀疏图上优势尤其明显，因为下游的学习机会少，初始特征质量的提升能直接带来泛化提升。
    
2. **整体结构**

   - **LM 模块**：用于从原始节点文本生成表示（默认使用 DeBERTa-base，也可替换其它 LM）。
   - **GNN 编码器**：利用图结构传播并增强节点表示。
   - 二者联合训练，使 LM 和 GNN 协同优化。

3. **训练目标**

   - 采用 **Masked Language Modeling (MLM)** 作为自监督任务：随机遮盖文本中的部分 token（比例 p），要求模型预测原词。

   - **节点文本经过 LM 编码后取 `[CLS]` 作为节点向量，送入 GNN 聚合邻居信息，再将节点向量与每个 token 向量拼接，用 MLP 预测被 mask 的 token。**

     $z_t = MLP([h_i^{\top}, o_t^{\top}])$ 这里的 $h_i^{\top}$ 是目标节点 $i$ 聚合其周围邻居的 $s_j$ 的第一个（即 CLS） 的隐藏向量 $x_j$ （$j \in \mathcal{N}(i)$）的信息得到的一个隐层嵌入。

     使用这个嵌入 $h_i^{\top}$ 分别与该结点的其他 token 嵌入进行拼接并送入分类头 MLP （通常是 MLP + softmax）中得到对应在 $t$ 位置上的概率 $z_t$。用这个 $z_t$ 来预测掩码概率。

     - **预测概率的对数**：$\log \Pr(T_{t} \mid z_{t})$表示在模型输出 $z_t$（由节点的图神经网络表示 $h_i$ 与文本标记的语言模型表示 $o_t$ 拼接后经 MLP 处理得到）的条件下，预测出原始标记 $T_t$ 的概率的对数。由于对数函数的单调性，**最大化该值**等价于最大化预测正确的概率。
     - ==**整体损失含义**==：整个公式通过对所有被掩码标记的预测概率对数求和，**得到单个节点文本序列的损失** $\mathcal{L}_i$。模型训练的目标是最小化该损失，从而提升对被掩码标记的预测能力，**促使语言模型和图神经网络协同学习文本语义与图结构信息，最终获得更优的节点表示**。==这个做法不仅综合了目标节点的文本语义，还通过 GNN 聚合了其邻居节点的文本语义。（很直观的 idea，被唐杰组给抢了）==
     - 问题
       1. 这里的学习文本语义体现在哪？就是针对每个节点的文本描述进行掩码自监督学习（当然这会更加复杂，而这一复杂是有意为之的，为了防止模型的过拟合）
       2. 这里的学习图结构信息体现在哪？GNN聚合信息。

4. **训练过程**

   - 使用随机游走的 **GraphSAINT** 子图采样器生成 mini-batch 子图，降低大 LM 的显存与计算压力。
   - 每个 mini-batch 的节点文本先送入 LM 得到表示，然后 `[CLS]` 输出送入 GNN 做邻居信息聚合。
   - 对被 mask 的 token 预测计算交叉熵损失，联合更新 LM 与 GNN 参数。

5. **优点**

   - 避免了分步处理导致的特征损失；
   - MLM 作为较难的自监督任务，有助于防止过拟合；
   - mini-batch 子图采样平衡了计算效率和全图信息保留。

6. **LM + GNN 联合预训练会更难**

   与传统的图自监督学习不同，为文本属性图（TAGs）设计自监督目标更具挑战性。我们的模型架构包含两个不同的模块，且模型参数规模不同（**大型语言模型**与**小型图神经网络**），这使得训练难度大大增加。简单的自监督目标很可能会导致模型过拟合。

   > **1）选择合适的自监督训练目标以避免过拟合**
   >
   > 含义
   >
   > - 如果只是直接把 **LM + GNN** 联合起来训练，容易在少样本任务中过拟合，因为 LM 参数量大，很快就能记住训练数据，而不是学到可泛化的特征。
   > - 所以需要选一个**足够“难”**、又能引导模型学习通用表示的自监督任务，比如 **Masked Language Modeling (MLM)**。
   > - MLM 的难度在于：不仅要预测被 mask 的 token，还要利用邻居节点的信息，这样 LM 和 GNN 都必须参与，避免只靠 LM 就能完成任务。
   >
   > 📌 **举例（TAG：论文引用网络）**
   >
   > - 如果任务只是“预测邻居节点的标签”，LM 可能完全忽略图结构，直接凭论文标题预测出标签。
   > - 换成 MLM：
   >   - 节点文本 `"Graph Neural Networks for [MASK]"`
   >   - LM 需要理解上下文，但有时光靠上下文不够，需要 GNN 从邻居论文中补充语义信息（比如邻居论文都提到 “NLP”），这样 LM 和 GNN 才会协同工作。
   > - 因为任务更难，模型不容易在少数样本上死记硬背，从而降低过拟合风险。
   >
   > **2）对小型 mini-batch 进行采样以降低计算和显存成本**
   >
   > 含义
   >
   > - LM（尤其是 DeBERTa、BERT 这种 transformer）计算成本高，如果对**全图**每个节点文本都编码，显存和计算量会爆炸。
   > - 所以需要采用 **mini-batch 子图采样**（比如 GraphSAINT、随机游走采样），一次只处理一小部分节点及它们的邻居。
   >
   > 📌 **举例（TAG：论文引用网络）**
   >
   > - 假设图有 100 万篇论文，每篇标题+摘要 200 token，LM 一次全处理会需要几十 GB 显存（甚至上百 GB）。
   > - 解决办法：
   >   - 用 **GraphSAINT 随机游走采样**得到一个 200 节点的小子图；
   >   - 只对这个子图的节点文本做 LM 编码，再送入 GNN 聚合；
   >   - 这样每次 batch 显存占用可能从 80GB 降到 12GB，可以在单机 GPU 上训练。
   >
   > ------
   >
   > ✅ **一句话总结**：
   >
   > - 第一点是 **设计足够难的自监督任务**（如 MLM）让 LM 和 GNN 必须合作，从而避免过拟合。
   > - 第二点是 **用子图采样控制计算规模**，否则 LM 处理全图文本会让显存和时间消耗不可承受。



#### Mini-batch 训练

由于语言模型很重，即使我们在小图数据集上训练模型，我们也需要采用小批量训练策略。

GNN有几种类型的小批量训练技术。为了权衡效率和灵活性，我们选择了一种基于子图的方法GraphSAINT[48]作为我们的训练策略。在每个训练步骤中，GraphSAINT通过从原始图中采样子图来构建一个小批量，并根据采样的子图生成节点表示。在这项工作中，我们采用随机游走采样器来保持整个图的连通性。采样过程首先从整个节点集V中随机均匀地选择r个根节点。从每个根节点开始，从原始图结构中采样长度为l的随机游走。然后，通过将随机游走中出现的所有节点相加，我们得到了一个采样节点集Vs，由Vs诱导的子图Gs在GNN编码器中用于生成小批量中的节点表示。

#### 图文混合提示学习（Graph-Text Mixed Prompt Learning）



##### Graph Prompt 设计

这是从 **图结构侧** 提供标签/语义提示（**通过==虚拟节点==+连边**）



**Prompt Graph 节点** = “类别原型” + “语义锚点”

**内部连边** = 让语义相近的标签原型互相交流

**跨图连边** = 把类别语义直接插到目标节点的本地子图里，让少量训练样本的监督信号能快速扩散到相关节点



##### Text Prompt 设计

这是从 **文本侧** 对 LM 的输入做一个**可训练的语义引导**。

**这里只是先有个印象，最终的流程还是以后文的形式为主！！**

下游为了与上游的文本与图结构结合的做法保持一致（公式3）

拼接方式如下：

对一个节点 $v$：

- 它有原始文本（或标签文本） → 用 LM tokenizer 切成 token embedding。

- 在这些 token embedding **前面**加上 $N$ 个可训练的 prompt token embedding $[P_1],[P_2], \cdots,[P_N]$。

- 如果节点是 Prompt Graph 节点，还会有它的 label embedding（标签文本 LM 编码得到的向量）。

- **最终输入 LM 的 embedding 序列**类似：
  $$
  [P_1],[P_2], \cdots, [P_n], \text{label}/ \text{text tokens}
  $$
  



##### 最终提示学习转发功能

$$
\mathbf{z}_v = \text{MLP}\big(\text{READOUT}(f_{\text{GNN}}(G_v; G_p)), \mathbf{w}_t \big)
$$

意思是 **在 Prompt Learning 阶段**，给定一个目标节点 $v$ 的 **ego graph** $G_v$ 以及 **prompt graph** $G_p$，模型的前向过程是：

1. $f_{\text{GNN}}(G_v; G_p)$
   - 把目标节点的 ego graph 和 prompt graph 拼在一起，送入 GNN 编码器（**参数冻结**，来自预训练阶段）。
   - 这样得到每个节点（包括目标节点和 prompt 节点）的结构化表示。
2. $\text{READOUT}$
   - 从 GNN 输出中提取目标节点的表示向量（或者对 ego graph 聚合得到目标节点的最终 embedding）。
3. **与 text prompt $\mathbf{w}_t$ 结合**
   - $\mathbf{w}_t$ 是一个**可训练的文本提示向量**（Text Prompt）。
   - 在这里它和结构表示一起输入到 MLP 分类器，相当于给分类器提供一个额外的**任务引导==偏置==**。
4. **MLP 分类头**
   - 只训练 prompt graph $G_p$、text prompt $\mathbf{w}_t$ 和 MLP 分类头，LM 和 GNN 主体参数保持冻结。

------

🔹 **直观理解**

- $f_{\text{GNN}}(G_v; G_p)$ → 提取结构表示（包含 prompt graph 的标签语义注入）。
- $\mathbf{w}_t$ → 从文本侧提供可学习的任务适配信号。
- MLP → 融合两部分信息做分类。

这样结构 prompt（graph prompt）和文本 prompt（text prompt）在分类阶段（即下游任务）融合到了一起。



#### 模型推理

我们在模型推理阶段整合了预训练模型 $f^{\text{LM}}$、$f^{\text{GNN}}$ 和提示图 $G_p$。我们提出了三种推理方法：

1. **P2TAG（LM）**：
   - 仅使用训练好的语言模型 $f^{\text{LM}}$
   - 通过小批量方式计算图中每个节点的语言模型 [CLS] 令牌输出
   - 公式表示为：$\mathbf{h}_v = f^{\text{LM}}(\text{[CLS]}_v)$

2. **P2TAG（GNN）**：
   - 将 P2TAG（LM）输出的节点 [CLS] 令牌进一步输入图神经网络编码器 $f^{\text{GNN}}$
   - 在测试阶段采用全邻域采样策略以避免训练时的随机性
   - 第 $\ell$ 层的节点表示计算：
   $$
   \mathbf{h}_v^{(\ell)} = \text{AGGREGATE}^{(\ell)}\left(\{\mathbf{h}_u^{(\ell-1)} | u \in \mathcal{N}(v)\}\right)
   $$
   其中 $\mathcal{N}(v)$ 表示节点 $v$ 的全邻域

3. **少样本节点分类**：
   - 通过 3.3 节描述的提示学习进行
   - 整合结构提示（$G_p$）和文本提示（$\mathbf{w}_t$）
   - 最终分类表示为：
   
   $$
   \mathbf{z}_v = \text{MLP}\left(\text{READOUT}(f_{\text{GNN}}(G_v; G_p)), \mathbf{w}_t \right)
   $$

我们的框架的 P2TAG 推理流程如算法 2 所示。



### 其他

大概把流程弄懂了，学到了很多新的知识。比如 graph prompt，text prompt 的设计。对我自己实验的启发是可以从预处理阶段下手，然后用一下虚拟节点的技术？

实验大概率是做不了了，没代码没算力...
