#  论文复现日志（2）

这里记录的主要是一些本人思考 idea 时借鉴的文章。



## HHGT

[链接](https://dl.acm.org/doi/pdf/10.1145/3701551.3703511)

2024.7

### ==思考与感悟==

#### idea

这篇文章给我和我之前的一个想法有点相似：先局部 transformer 再全局 transformers



==我发现针对 HGT （异构图 transformer）的方法所遵循的原则和 dgl 的 block 机制（图采样 + 批处理手段）的底层逻辑几乎是一样的==。最大的区别是：HGT 强调 **异构类型**，DGL Block 是框架机制，但支持异构图处理。

那么针对我的 NIE 方向，我是不是可以通过**改进预处理**图的结构来使整个任务性能变得更好？

这篇文章指出当下 HGT 的手段与缺陷（1）不区分距离（2）不区分类型，所以我们需要关注的是：transformers 对异构信息的识别能力如何？直接喂给它一个异构图它能否准确地判断异构信息？若我们使用某种策略来人为地区分图上的异构信息（一种预处理手段），transformer的能力是否会增强。还是说我们直接再次设计一种能够天然识别针对图异构信息的 transformers 框架？（但是这项工作前人已经做过了）。

事实上，我们可以将目标放在大规模图上，那么进行训练的话就离不开 dgl 的 block 机制，这和之前HGT遵循的准则是一样的，这个操作顺便还能解决显存问题。所以我们可以考虑将重心放在预处理的框架上？？



transformers 能否自己学习到异构结构？

下面两个公式分别是 transformers 和 GAT 的异构图聚合和注意力计算公式（NIE 方向上）

下面这个公式是第 $h$ 个头的两点之间的注意力公式的计算

RGTN、SKES、EASING 均使用了下式这种聚合注意力计算公式（transformers）
$$
w^{(l),h}_{y \rightarrow x} = 
\frac{
\left( W^{(l),h}_Q H^{(l)}_x \cdot \left( W^{(l),h}_K H^{(l)}_y \right)^\top \right)
\cdot W^{(l),h}_E E^{(l)}_{y \rightarrow x}
}{
\sqrt{d}
}
$$

$$
\alpha_{y \to x}^{(l),h} = 
\sum_{e \in \mathcal{E}(y, x)} 
\frac{
    \exp\left( w_{y \xrightarrow{e} x}^{(l),h} \right)
}{
    \sum\limits_{y' \in \mathcal{N}(x)} \sum\limits_{e' \in \mathcal{E}(y', x)} \exp\left( w_{y' \xrightarrow{e'} x}^{(l),h} \right)
}
$$

___

GENI、MultiImport、EASING均使用了下面面这种聚合注意力计算公式（GAT）

> $\mathbf{h}_i$ 和 $\mathbf{h}_j$ 分别表示汇点和源点的隐向量， $\alpha$ 是一个可学习的注意力权重系数，学习并度量目标节点对该边的依赖强度

$$
\omega_l(i, j, m) = 
\frac{
    \exp\left( \mathrm{LeakyReLU}\left( \mathbf{a}_l^\top [ \mathbf{h}_{i}^{l-1} \| \pi(\rho_{ij}^{m}) \| \mathbf{h}_{j}^{l-1} \right]) \right)
}{
    \sum_{(k, n) \in \mathcal{N}(i)} 
    \exp\left( \mathrm{LeakyReLU}\left( \mathbf{a}_l^\top [ \mathbf{h}_{i}^{l-1} \| \pi(\rho_{ik}^{n}) \| \mathbf{h}_{k}^{l-1} \right]) \right)
}
$$

前者是**点积注意力**，用两个变换（Query/Key）产生向量，然后直接计算相似性，**能够自然捕捉高阶结构相似性、语义相似性、空间相似性**，并且**在理论上是通用函数逼近器**

后者是**加性注意力**，用一个向量 $\mathbf{a}$ 和拼接后的表示做一次投影 + 激活函数，属于浅层打分方式（Single-layer MLP 近似），**表达能力弱、结构简单**。

上面的就是 GPT 口胡的，两者的区别和联系的深刻理解 我具体也还不知道





transformers 的优越性有

**表达能力更强**（点积注意力能建模更复杂的语义关系）；

GAT 是**加性注意力**，用一个向量 $\mathbf{a}$ 和拼接后的表示做一次投影 + 激活函数，属于浅层打分方式（**Single-layer MLP 近似**），**表达能力弱、结构简单**。

Transformers 是**点积注意力**，用两个变换（Query/Key）产生向量，然后直接计算相似性（该相似性是基于这一跳子图中目标节点及其邻居入点的”全局“视角的），**能够自然捕捉高阶结构相似性、语义相似性、空间相似性**，并且**在理论上是通用函数逼近器**

尽管 Transformer-style 注意力机制在理论上具有较强的表达能力，但在图神经网络中，它并不具备 LLM 中 Transformer 的那种显著的计算效率和扩展性优势。在 dgl 框架下，其原因在于，图结构是天然稀疏的，注意力仅在目标节点与其邻居之间计算，而非全图 token 之间的全连接计算。即使采用 full-batch 训练，DGL 等稀疏图框架下的计算也采用基于边的消息传递机制，不会构造显式的 $N \times N$自注意力矩阵，因此无法像 LLM 中那样使用高效的矩阵乘法加速（如 FlashAttention）。这一结构限制使得图上的 Transformer 无法在并行性和计算效率上获得与 NLP 中 Transformer 相同的优势。但是这不重要。既然是这样，那我们为什么会爆显存？仔细分析 `EASING` 中的 `GTLayer` 代码模块可以知道，内存复杂度随节点和边数是**线性增长**的，所以即便是在全图视角下怎么会爆内存呢？首先需要明确一点的是：**数据中所有的节点都会用到！因为我们为所有的节点都创建了结构特征与语义特征，但是只有少部分节点有标签。**

不懂啊，为什么会爆显存啊。刚测试了一下，使用自己能在服务器上跑的参数与FB15k数据集去训练模型，发现一个模型第一次反向传播后会用到 2G，继续深究

边[607164, 10]和节点[14951, 64]特征加起来约为 24MB

q, k, v[14951, 4, 8] 加起来大约 10M

`graph.apply_edges(fn.v_dot_u('q', 'k', 't'))` 约 9M（4B存）

`attn_score = graph.edata.pop('t').sum(-1) / self.sqrt_dim  # [n_edge, n_head]` 约 9M（4B存）

```python
attn_score = self.attn_drop(edge_softmax(graph, attn_score.unsqueeze(-1), norm_by='dst'))

graph.edata.update({'t': attn_score})
```

  30MB

后续的 前馈神经网络 一共也就加了 20MB，不仔细细算了。

反正截至到计算 struct_h 的编码器的两层一共是223.01MB正常

content_h 则是 500.90MB



解码器部分一计算 x_m 和 x_v 直接变成 2361 MB，接下来细究

```python
s = torch.bmm(self.p_s, q_input) # [B, N, 2d]
u = torch.bmm(self.p_u, q_input) # [B, N, 2d]
```

花费了 70MB（4B），这里是 `[14951, 10, 64]`，我们将 N 设置为10， 我们的 num_hidden 设置为 8 ，heads 设置为 4，这里的 2d 就是 4 * 8 * 2 = 64



计算完下述这6个矩阵，增加了216MB

```python
qs = self.w_qs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
ks = self.w_ks(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
vs = self.w_vs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]

qu = self.w_qu(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
ku = self.w_ku(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
vu = self.w_vu(u).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
```

[14951, 4, 10, 16]



60MB

```
ua_s_attn = torch.matmul(qs / self.sqrt_d, ks.transpose(-2,-1)) # [B, heads_num, N, N]
```

[14951, 4, 10, 10]



后面的不细算了，反正一层前向 DJE 大概是700MB。我们还用了两个模型，每次采样还要再重复一整个前向过程。这么一累计确实大了。所以问题终于得到了解决，和 transformers 的自注意力机制无关（==**自注意力需要的显存其实只与 $E$ 呈线性关系而不是和 $n^2$ 正相关**==），就是单纯的模型参数多（尤其是解码部分） +  数据量大（整图 + 大图） + 常数大（模型多用）



上述这种transformers 的 attention 构造方式**能有效建模==单跳==邻接关系中的异构性**，适合做类型感知的局部信息聚合；但它的表达能力受限，**不能捕捉复杂、跨边/跨跳/多元异构路径下的语义交互**。（**我所见的论文中 NIE 方向相关的 transformer 建模在聚合层面确实只关注一层的信息，但不排除其他 HIG 方向的论文有跨边/跨跳/多元异构路径下的建模**）

```python
# 对异构图边注意力机制的处理
if self.edge_mode == 'DOT':
    rel_feat = self.e_linear(edge_feat).reshape(-1, self.n_heads, self.out_dim)
    graph.edata.update({'rel': rel_feat})
    graph.apply_edges(lambda edges: {'e': edges.src['q'] * edges.data['rel']})
    rel_attn = graph.edata.pop('e').sum(-1) / self.sqrt_dim
    attn_score = attn_score + rel_attn
elif self.edge_mode == 'MUL':
    edge_attn = self.e_linear(edge_feat)  # [n_edge, n_head]
    attn_score = attn_score * edge_attn
elif self.edge_mode == 'ADD':
    edge_attn = self.e_linear(edge_feat)  # [n_edge, n_head]
    attn_score = attn_score + edge_attn
elif self.edge_mode == 'GATE':
    edge_attn = self.e_linear(edge_feat)
    attn_score = attn_score * edge_attn
    attn_score = self.attn_drop(edge_softmax(graph, attn_score.unsqueeze(-1), norm_by='dst'))
```



理论上 transformer 有 capacity，但存在问题：

| 问题               | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| **异构类型弱建模** | 多数 HGT 架构中 type embedding 只是一个加性偏置？（反正我见到的都是乘一个系数即直接对边类型嵌入得到一个乘法系数加入到自注意力的计算当中），事实上，做法有很多具体见上面的公式和代码。是否能建模复杂关系呢（在 NIE 这个方向上的 Gtrans 确实是聚合时只关注周围一层，而没有捕捉复杂、跨边/跨跳/多元异构路径下的语义交互） |
| **距离信息缺失**   | 由于 attention 本身不感知 hop distance，需要额外引入 positional bias |
| **图结构信息稀疏** | **大图场景中，局部结构不完整，Transformer 表现力过剩，容易过拟合或失效** |

我们具体来理解一下这个问题==大图场景中，局部结构不完整，Transformer 表现力过剩，容易过拟合或失效==

> 这句话出现在图神经网络（GNN）或图Transformer等相关研究中，传达了一个**图结构建模中的常见问题**。我们逐句解释：
>
> ✅ 原句拆解：
>
> > **图结构信息稀疏**
>
> 👉 指的是图中**边的连接较少**，或者**节点之间的关系不完整**。
>  例如，在一个社交网络中，如果你只能看到一个用户的极少朋友，你就很难推断他的行为或兴趣。
>
> ------
>
> > **大图场景中，局部结构不完整**
>
> 👉 在大规模图（如百万级节点的图）中，对每个节点你通常**只能观测其局部的邻居**，而且这些邻居可能本身也缺乏足够信息（例如标签或特征），导致这个节点的“局部结构”是不完整的。
>
> ------
>
> > **Transformer 表现力过剩**
>
> 👉 指的是 Transformer 模型的表达能力非常强（**因为它可以建模任意两个节点之间的关系**），但在图结构中如果数据本身很“稀疏”、信息很少，这种强模型就**没有足够的数据支撑其学习能力**，反而容易出问题。
>
> ------
>
> > **容易过拟合或失效**
>
> 👉 因为训练数据中的图结构太稀疏、局部信息不完整，导致模型学到的是**噪声而不是规律**，最终在测试时表现差。这叫做 **过拟合**。
>  更极端的情况是模型根本没学到东西 —— 这就叫 **失效**。
>
> 📌 举个通俗的例子：
>
> 你有一张社交图（图结构），每个人是一个节点，他们之间有“认识”这个边。
>
> - 如果你只能看到某个人的 1～2 个朋友（信息稀疏），
> - 而你用一个复杂的 Transformer 模型试图去推理他的兴趣（表现力过剩），
> - 最终可能模型会胡乱“猜”或者只记住训练集中的特例（过拟合），在新的人身上完全失败（失效）。（**这部分存疑，transformer 性能很强，这并不能说明它在稀疏图上表现很弱**）
>
> ✅ 总结：
>
> 这句话的意思是：
>
> > 在大规模图中，由于每个节点的邻居信息很有限（稀疏/不完整），直接使用高表达能力的模型（如 Transformer）反而可能适得其反，容易陷入过拟合或学习失败。
>
> ------
>
> 如果你是做图学习相关的任务（比如图推荐、节点分类、节点排序等），这句话提醒你要**注意模型复杂度与图数据结构之间的匹配**，必要时可能需要：
>
> - 图结构增强（如图补全）
> - 模型简化（降低参数量）
> - 增强监督信号（如伪标签、蒸馏）
> - 局部感受野限制（如采用 GNN 或局部 attention）



事实上，既然已经明确==**自注意力需要的显存其实只与 $E$ 呈线性关系而不是和 $n^2$ 正相关**==，那么很遗憾，我之前的 idea 计划的动机就已经被打翻了。我以为 g_trans 是针对全图的 g_trans 因而有了将图分成子图训练的想法，但实际上 g_trans 就是针对的一跳子图即**基于邻居进行聚合**，所以上述的做法其实并不需要了。这波很伤。

那么我们还可以往哪里思考突破呢？

1. NIE 方向上的 g_trans 相关论文确实可能没有充分考虑到图的异质性 $\rightarrow$ 如何充分挖掘图中的异质性（**从异构图上突破**）

   1. 新框架？（重新设计一个 HGT 还是做一些预处理之类的工作）

      当下的 HGT 存在表达能力[35]、过度平滑[5]和过度压缩[1]等挑战，如果选择往这个方向做那就是要换塞到了...

      或者说我们图补强一波，发挥一下 transformers 的优势？引入图补强 + 对比学习？

      HHGT 这篇文章的 Figure-4 也启发我们异质性区分的重要性

   2. 新理论？

      在 EASING 这个实验上，貌似在减少 hidden-dim 但是在启用批出批处理的情况下，跑出来的效果要更好？并且异构图感觉就是可以天然地去分图

2. 我们将目标瞄到异构图的大规模图的训练上，那么势必要用到图采样 + 批处理技术。之前的做法是随机采样目标节点，但是我们是在大规模的异构图上，所以直觉上感觉这种做法其实是比较莽撞的，原因如下（可能有误）：

   1. （训练稳定性角度）打乱节点顺序和在小批量中处理节点，可能会导致每次训练的梯度计算**基于不同的子图**。这样会使得训练过程中的梯度波动较大，因为每个 mini-batch 内的节点信息局部性较强，**无法全面反映全图结构**。因此，训练的收敛速度可能较慢，需要更多的训练轮次来达到与 full-batch 相同的效果。训练的效果也可能受到批次大小和打乱顺序的影响，我们可能需要一定的策略来保证每个 batch 能代表整体图的结构。
   2. （异构图复杂性角度）在 mini-batch 中，每个 batch 的节点和其邻居可能来自图的不同部分，并且包含不同类型的节点和边。因此，打乱节点顺序可能导致某些类型的节点信息不足，影响异构图中各类信息的均衡学习。为此，一些方法会使用采样策略（如基于节点类型或边类型的采样）来保持训练中各类节点的代表性。

   我们是否可以考虑在这一个方向优化，请仔细思考
   
3. 从伪标签上突破，EASING 那篇文章上，学长和老师都敏锐地更想要关注训练得到的伪标签的质量，我们是否可以从这里寻找创新点和突破？

   针对异构图构造分层图？在这些分层图上进行学习 batch，然后利用一种模型将这些 batch 上的东西进行融合和对其，学习到更吊的嵌入？（口胡的）





#### 思考

* 在输出层方面

**Ring-level 级别是做拼接 + 投影 + 归一化得到注意力系数；而 Type-level 级别是做 向量点积 + 归一化得到注意力系数。这两者有什么区别.**

首先尝试自己思考，思考不出来...（这个问题应该是比较深刻的）

> > **Type-level 注意力用的是点积 + softmax，而 Ring-level 用的是拼接 + 投影 + softmax，它们本质上有什么区别？**
>
> 我们来从**数学结构、表达能力、建模偏好**三个方面剖析它们的**注意力计算方法**本身的差异。
>
> 🔹1. 数学结构上的区别
>
> |          | 点积注意力（Type-level） | 拼接+投影注意力（Ring-level）  |
> | -------- | ------------------------ | ------------------------------ |
> | 形式     | 公式（4）                | 公式（6）                      |
> | 操作     | **内积**（不加权）       | **拼接再线性投影**（加权组合） |
> | 权重学习 | 无（只用输入特征）       | 有 learnable 参数 $W$          |
> | 特征维度 | 需对齐，必须维度一致     | 拼接后维度为 $2d$，更自由      |
>
> 🔸解释：
>
> - **点积注意力**是一种**无参数的相似性度量机制**：越相似（内积越大）→ 权重越高。
> - **拼接+投影注意力**是**可学习的匹配函数**：它通过参数 $W$ 来适配不同维度或关系的融合，表达能力更强，但更复杂。
>
> 🔹2. 表达能力差异
>
> |            | 点积（Type）                        | 拼接+投影（Ring）          |
> | ---------- | ----------------------------------- | -------------------------- |
> | 可表达函数 | 类似 cosine / L2 的距离度量（固定） | 可以学习非线性偏好（可变） |
> | 灵活性     | 较低                                | 高，可编码复杂交互关系     |
> | 稳定性     | 高，训练容易收敛                    | 低，依赖初始化与正则化     |
>
> > 📌 总结：拼接+投影机制能学到复杂的“偏好权重”，比如“靠近+远处组合权重最高”，而点积只能“相似度大就重要”。
>
> 🔹3. 建模偏好（适合处理的语义）
>
> | 任务                       | 点积注意力适合       | 拼接+投影注意力适合           |
> | -------------------------- | -------------------- | ----------------------------- |
> | 处理实体之间的相似性       | ✅ 是                 | ❌ 否                          |
> | 需要学习不同尺度的组合影响 | ❌ 否                 | ✅ 是                          |
> | 构造局部对比               | ✅ 是（每个 ring 内） | ❌ 否                          |
> | 融合跨尺度/层次结构        | ❌ 否                 | ✅ 是（例如 0-ring vs 2-ring） |
>
> ✅ 举个直观例子：
>
> 假设我们有中心节点 $p_1$，它的 1-ring 有 3 个类型：作者、主题、引用的论文：
>
> - **Type-level 的点积注意力**就像问：“这些类型谁跟我语义最像？” → 一个更**对称、静态的度量**；
> - **Ring-level 的拼接+投影注意力**就像问：“1-ring、2-ring加权后谁整体更重要？” → 是个更**复杂且可调的评估函数**。
>
> ✅ 总结一句话：
>
> > **点积注意力**是一种简单有效的“相似度选择机制”，适合**局部语义对比**；而**拼接+投影注意力**是一种带参数的“加权表达机制”，适合==**跨尺度融合与层次建模**。==
>
> 你可以根据你的任务需求来选择：
>
> - 追求简洁、稳定 → 点积；
> - 需要灵活建模非线性层次依赖 → 拼接 + 投影。



* 这篇文章貌似没有充分利用边的异构性



### 摘要

表现力限制和过度平滑促使研究人员探索图形变换器（GT）以增强 HIN **表示学习**。





### 引言

近年来，关于异构图的表示学习激增，这项技术可以**将节点嵌入到低维表示**中，同时**保留图结构和异构性**。

#### HGCN

基于HGNN的方法[11,19]通常利用**邻居聚合**策略，在HIN中的不同类型节点之间有效地捕获和传播信息。例如，R-GCN[23]通过结合关系特定的权重矩阵，在HIN中捕获各种关系，扩展了传统的图卷积网络（GCN）[17]。Fu等人[11]提出沿元路径合并中间节点，使用元路径内和元路径间信息进行高阶语义信息聚合。

尽管HGNN在模拟现实世界的HIN方面取得了成功，**但表达能力[35]、过度平滑[5]和过度压缩[1]等挑战的存在促使研究人员研究图形变换器（GT）[40]，以增强HIN表示学习**。例如，Hu等人[14]提出了一种**用于邻居聚合的异构Transformer式注意力架构**。Mao等人[21]利用**局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息**。一般来说，现有的基于GT的HIN表示学习方法，即图1（b）所示的基于HGT  （异构图 transformer） 的方法，遵循一个典型的原则：**给定一个目标节点，首先提取其k跳邻域（即距离目标节点≤k的可达距离内的节点）。然后，Transformer[29]将用于将信息从这些节点传播到目标节点。**（==其实就是做一个子图的处理==）这不就是一个 dgl 的 block 机制

比如 `figure 1` 中的 `b Existing HGT-based methods` 就揭露了当下的 HGT 的手段与缺陷！！！：

> 将 P1 的 2-hop 邻居（P2、A1、A3、S1、P3、P4）**全部统一处理**；
>
> 也就是说：
>
> - **不区分距离**：P2 是直接邻居（1-hop），P3 是间接邻居（2-hop），都混在一起；
> - **不区分类型**：论文、作者、主题的表示**放到一个 attention 序列中统一计算**；
>
> 缺点：语义容易混淆，难以理解不同节点“在结构中的不同语义角色”。



论文中也指出了了当下 HGT 的痛点、缺陷与不足（==这一块务必仔细阅读，好好理解==）

> 然而，现有的基于HGT的方法**倾向于混合不同类型的节点，并在邻居聚合过程中统一处理k-hop邻域内的所有节点，从而导致潜在的语义混淆。**特别是，（1）限制1：HIN中不同距离的目标节点的邻居具有不同的语义。以图1（a）为例，论文P1的直接邻居论文P2表示引用关系。相反，p1的间接邻居p3暗示了一种没有直接引用关系的主题联系，展示了不同的内涵。遗憾的是，现有的策略通过统一处理距离k内的每个邻居来忽略这些区别，即将P1、P2、P3**打包成一个序列**并统一聚合它们。这是不理想的，因为这些节点服务于不同的功能。（2）限制2：不同类型的目标节点的邻居也具有不同的语义。以图1（a）为例，论文P1的直接邻居包括论文P2、作者A1、A3和受试者S1。这里，P2表示引用关系，A1、A3表示作者关系，S1表示主题对齐关系。虽然现有的基于HGT的方法**考虑了节点类型**，**但它们通常将P2、A1、A3、S1打包在一起作为一个统一的序列**（**这是 HGT 所没有考虑到的**）。这种方法是不可取的，**因为它在邻居聚合过程中混合了不同类型的节点，模糊了论文、作者和受试者的不同功能。**

总结来说就是异构图中存在距离和邻居的异质性，而之前的 HGT 都没有考虑到



下面列举了一些**以前的**代表性工作

* R-GCN

  其实就是**最原始的**异构图的处理手段，对不同类型的边（关系）使用不同的变换矩阵，从而实现“关系感知”的聚合。

  > 我们以一个学术图（Academic Graph）为例，图中包含：
  >
  > - **三种节点类型**：Paper（P）、Author（A）、Venue（V）
  > - **几种关系类型**：
  >   - Paper ←write-by→ Author
  >   - Paper ←published-in→ Venue
  >   - Paper ←cite→ Paper
  >
  > 例如：
  >
  > - P1 是由 A1 和 A2 撰写的论文，发表于 V1；
  > - P2 是由 A2 撰写的论文，引用了 P1；
  > - P3 是由 A3 撰写的，和 P1 都发表于 V1。
  >
  > 图自己画一下就行了（很简单）
  >
  > 
  >
  > **方法核心：**
  >
  > ==R-GCN 对不同类型的边（关系）使用不同的**变换矩阵**，从而实现“关系感知”的聚合。==
  >
  > 
  >
  >  **举例：**
  >
  > 假设我们要更新节点 P1 的表示：
  >
  > - P1 的邻居包括：
  >   - A1, A2（write 关系）
  >   - V1（published-in）
  >   - P2（被 P2 引用）
  >
  > R-GCN 的聚合逻辑是：
  >
  > - 使用写作者的表示向量，通过参数矩阵 **W_write** 变换；
  > - 使用 Venue 的表示向量，通过 **W_publish** 变换；
  > - 使用被引用的论文的表示向量，通过 **W_cite** 变换；
  > - 聚合时不同关系分别处理，不会“混淆”。

  

* MAGNN [csdn阅读链接](https://blog.csdn.net/byn12345/article/details/105101492) （**暂时搁置**。。）反正这里这种方法是基于元路径的多种聚合手段

  基于元路径的手段，使用元路径合并中间节点，使用元路径内和元路径间信息**进行高阶语义信息聚合**。聚合整个路径的表示

  > 现有的基于元路径的嵌入学习方法有以下局限性：
  >
  > （1）忽略节点的内容特征（属性信息），不能很好地处理节点属性特征丰富的异质图。例如 metapath2vec, ESim, HIN2vec, HERec。
  >
  > （2）舍弃了元路径内部的节点信息，只考虑元路径的起始节点和末尾节点，造成信息损失。例如 HERec, HAN。
  >
  > （3）只依赖于单个元路径，因此需要人工选择元路径，丢失了来自其他元路径的部分信息，导致性能不佳。例如 metapath2vec。
  >
  > 为了解决上述问题，本文提出MAGNN（Metapath Aggregated Graph Neural Network ）。
  >
  > MAGNN由三个部分组成：
  >
  > （1）节点内容转换(node content transformation )，将异质的节点属性信息映射到同一个**隐层的向量空间**；
  >
  > （2）元路径**内部聚合**(intra-metapath aggregation )，使用注意力机制将元路径**内部的语义信息**纳入考虑；
  >
  > （3）元路径**间的聚合**(inter-metapath aggregation )，使用**注意力机制从多个元路径聚合信息**。
  > 
  >



* HGT （贡献很大的一篇文章）

  > 将标准 Transformer 改造为**异构图专用模型**，支持多种节点类型与边类型。
  >
  > 可以理解为：**注意力不仅考虑邻接关系，还区分每种异构边、异构节点对的交互模式。**
  >
  > **2. 时间编码（可选）：**
  >
  > 他们还为动态图（如引用网络）引入时间编码，可以处理时间序列。
  >
  > **3. 基于 Transformer 的堆叠式结构：**
  >
  > 和标准 GNN 一样，多层 HGT 会逐层融合多跳邻居信息，但内部机制是带类型感知的 Transformer。



* HINormer （2023年 80 引用）

  利用局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息。（貌似和我最开始的想法很像，可以找个时间去研读一下）

  进一步改进 HGT，缓解计算量大、信息干扰的问题，同时提升表达能力。

  > 1. **Local Structure Encoder**
  >
  > - 构造目标节点的局部结构视图，比如：
  >   - 局部子图中有哪些节点、类型、边；
  >   - 用轻量注意力机制捕捉结构模式。
  >
  > 2. **Heterogeneous Relation Encoder**
  >
  > - 更精细建模不同关系的作用（类似于 HGT 但更高效）；
  > - 使用了结构编码 + 边编码的组合策略；
  > - 学习关系类型与语义的交互表示。
  >
  > 3. **优势：性能更优，计算更快**
  >
  > - 他们主张结构编码和语义融合可以减少**不必要的全 attention**；
  > - 因此**更适合大图、低资源环境**；
  > - 实验中比 HGT 更快，效果也更强。

#### HHGT 的设计

##### 设计1

**为了区分不同距离的节点邻居**，我们引入了一种称为kring邻域的创新结构。这种结构具体指的是与目标节点的距离恰好为k的节点，将其与众所周知的k-hop邻域区分开。本质上，我们将k跳邻域划分为k+1个不重叠的k环邻域，其中每个k环邻域中的节点与目标节点的距离相同。如图1（c）所示，考虑k=2，对于纸张P1，其距离为2的邻居可以分解为三个不同的k环邻居：0环邻居{P1}、1环邻居{P2，A1，A3，S1}和2环邻居{P3，P4}。基于这种新结构，**我们为每个节点提取了不同的k环邻域，这可以自然地辨别不同的功能，从而防止语义混淆**（但事实上，个人认为即便是在同一环领域的节点，其语义信息也并不一致）。**然后，设计一个环级变换器，分别聚合不同的k环邻域，聚合基于每个k环邻域与目标节点的相关性和重要性。**（相当于在距离这一层上直接编码区分）

##### 设计2

**为了避免在每个k环结构中混合不同类型的节点（这里是考虑在同一环形邻域内的情况）**，我们进一步提出了一种新的（k，t）环结构，根据每个k环的类型将节点排列成不同的组。基于这种邻域划分，考虑到每种类型对目标节点的重要性，提出了一种类型级变换器，用于分别聚合每个k环结构内目标节点的不同类型的邻域。在图1（a）中，考虑节点P1的1-环邻域（即P2、A1、A3、S1），其中不同类型的节点共存。我们根据节点类型将这个1-环邻域分为三组，即论文P2、作者A1、A3和主题S1，**每组都携带唯一的函数**。然后，我们应用一个类型级Transformer来单独聚合每个组，而不是像现有的基于HGT的方法那样将它们视为一个统一的序列。这种方法使我们能够模拟不同类型节点的不同角色。



总结：

对于每个目标节点，我们从不同的k环邻域中提取其邻居，其中每个环内的节点根据其类型进一步分组，形成创新的（k，t）**环**    **邻域**    结构。基于这种结构，我们引入了一种新的分层异构图变换器（HHGT）模型。该模型无缝集成了一个类型级转换器，**用于分别聚合每个k环邻域内不同类型的节点**，然后是一个**环形转换器**，用于以**分层方式聚合不同的k环邻域**。



##### 贡献

* 我们首次为HIN表示学习设计了一种创新的（k，t）环邻域结构，强调了HIN中距离和类型的异质性。（这里的类型可以更加激进地理解为同一环形邻域内的不同节点类型）
* 据我们所知，我们是**第一个使用分层的图 transformers 模型，来进行异构图中的节点表征学习任务**，该模型无缝集成了类型级变换器，用于分别聚合每个k环结构中不同类型的节点，然后利用环形级变换器对不同的k环邻域进行分层聚合。
* 在两个真实世界的HIN基准数据集上的实验结果表明，我们的HHGT在两个典型的下游任务上明显优于14种基准方法。此外，消融研究验证了考虑HIN距离和类型异质性的优势和意义。



### 相关工作

#### HIN嵌入的浅层模型

近年来，许多图嵌入技术应运而生，旨在将节点或子结构映射到低维空间，同时保留图中的连接结构。由于现实世界中的网络通常包含多种类型的节点和关系，针对异构信息网络（HIN）嵌入的浅层模型研究引起了广泛关注。浅层模型主要分为基于随机游走的方法和基于一阶/二阶邻近性的 methods。这些方法利用**元路径**或**类型感知的网络邻近约束**来利用网络的异构性进行HIN嵌入。尽管这些方法有所贡献，但它们未能有效捕捉HIN中的复杂关系和语义，导致表示学习效果不理想。



#### HIN嵌入的深度模型

1. **HINs的研究背景**：
   - 深度学习模型在同质图中的成功激发了对异构信息网络（HINs）研究的关注。HINs涉及不同类型的节点和边，要求模型能够处理这种复杂的结构。
2. **两类HIN嵌入模型**：
   - **基于元路径的深度模型**：这些模型使用**元路径**来聚合特定类型邻居的信息，能够捕捉更高阶的语义信息。一个例子是**HAN**模型，它采用了层次注意力机制来学习节点和元路径的重要性。缺点是**需要专家知识来选择元路径**，这对模型性能有较大影响。
   - **无元路径的深度模型**：这些模型（如**Schlichtkrull**的关系感知图卷积层和**Hu**的基于Transformer的注意力机制）不依赖于元路径，**旨在通过==邻居聚合==来进行表示学习**。虽然无元路径的模型减少了人工选择元路径的需求，但存在两个问题：
     1. **节点类型混合**：在邻居聚合过程中，不同类型的节点被混合处理，导致无法充分捕捉不同类型节点之间的关联。
     2. **距离语义混淆**：忽视了HIN中不同距离邻居所携带的不同语义信息，导致邻居聚合时出现语义混淆，从而影响下游任务的性能。

HIN嵌入有两种主要方法，并指出了无元路径模型在处理节点关系和语义信息时的不足之处。

有时间去了解一下这两类处理方法的代表性方法



### 准备工作

* 问题定义
* transformers 编码器



### 方法论

该模型由两个重要模块组成：Ring2Token和TRGT。总体框架如图2（a）所示。给定一个HIN和一个整数K，对于每个目标节点，我们最初利用Ring2Token提取多个K环邻域（K∈[0，K]），从0环邻域到K环邻域，在每个K环结构中按类型划分组织良好的节点，形成（k，t）-环邻域结构。在邻域划分之后，**我们使用TRGT模块基于这些提取的（k，t）环邻域通过GT层学习节点表示**。这涉及一个类型级转换器来聚合每个k环邻域内不同类型的节点，然后是一个环级转换器来分层聚合不同的k环邻域。



#### Ring2Token

==如何有效地将**邻居的信息聚合到节点**中，对于设计强大的HIN表示学习模型至关重要==。

这个模块好理解，看 Figure 3 就能理解了

* k-ring Neighborhood

* (k, t)-ring Neighborhood

  具体来说，给定整数K和节点类型号T，对于节点u，它具有长度为K+1的K环邻域序列。在每个k环邻域内，它可以进一步划分为一系列长度为t的（k，t）环邻域。这些（k，t）环组将被送入TRGT模块进行模型训练。



#### TRGT Module

##### Type-level Transformer

**T 一致，不够补 0** （也就是说每一跳的 T 都是0，不足的使用 0 向量代替）

HIN中的节点有各种类型，每种类型都代表不同的概念。然而，现有的基于HGT的方法倾向于将不同类型的节点混合在一起，将所有节点类型打包成一个序列，并**在邻居聚合过程中统一关注它们**，未能像前面讨论的那样对各种类型的节点的不同角色进行建模。

$x_k = \{x_{K,1},x_{k,2}, \cdots, x_{k,T}\} \in \mathbb{R}^{T \times d}$ 在第 k 层上，针对这些嵌入/特征向量做 transformer，这样可以在每一跳之间区分节点类型（==这里还有一个比较吊的一点是，每个 k 层之间的节点之间是不存在边相连的，它在这一层做了 transformer 相当于直接建边了，增强了图的结构==）

不够补 0 向量

对于 0 环特征 $x_0$，我们使用多层感知器（MLP）将嵌入维数从 $\mathbb{R}^{T \times d}$ 转换为 $\mathbb{R} ^ {1 \times d}$

* Type-level Attention Mechanism.

  其实就是在解释公式（4）和（5），按照我的理解，这里其实可以看做 Type-level Transformer 的**输出层的处理**。（想通了之后完全合理）

  这个注意力的计算其实也是很自然能想到的，结合 Figure 2（b）那张图以及以往的经验

##### Ring-level Transformer

还没精读论文，感觉就是针对 Type-level Transformer 那一阶段得到的嵌入，作为输入在做一轮 Ring - level 级别的 transformer。

Figure 2（b）讲得很清楚

每个k环邻域都贡献了一层新的信息，它们的组合提供了不同的视角，这对于全面理解HIN至关重要。因此，从这些克林社区有效收集信息至关重要。为了应对这一挑战，我们开发了环级转换器，用于跨不同k环邻域进行全局聚合。对于每个节点，给定从类型级Transformer获得的k环令牌序列 $h_0,h_1, \cdots, h_k$，每个令牌都总结了属于特定k环结构的邻居的唯一信息，环形级Transformer首先利用Transformer编码器来学习节点表示。堆叠L个Transformer层后，我们得到每个节点的表示，即表示序列 $z_0, z_1, \cdots, z_k$，其中 $z_k \in \mathbb{R}^d$。

* Ring-level Attention Mechanism.

  同样的，这也可以看做是 Ring-level trainsformer 的**输出层的处理**

  ==Ring-level 级别是做拼接 + 投影 + 归一化得到注意力系数；而 Type-level 级别是做 向量点积 + 归一化得到注意力系数。这两者有什么区别.==





### 目标函数

做的是节点分类的任务，使用的损失函数是交叉熵损失。

那我们可以考虑将这个东西迁移到 NIE 上做一个回归任务



### 实验

我们进行实验来回答以下研究问题：

* RQ1:HHGT能否在下游任务中超越基线？

  HHGT实现了最佳的整体性能，这表明其具有卓越的有效性。观察到的改进的主要原因可能包括：

  （1）类型级转换器在邻居聚合过程中最佳地利用了节点类型信息，能够有效地捕获不同类型节点之间的适当相关性；

  （2）环级转换器考虑了不同距离的邻居之间的区别，促进了跨不同层次的强大邻居聚合。

  此外，在许多情况下，基于同构GT的方法比基于异构GT的方法表现更差，这验证了在HIN中考虑关系异构性的重要性。我们的模型也从这方面受益，通过设计（k，t）环邻域结构来强调HIN内距离和类型的内在异质性。

* RQ2：学习到的节点嵌入代表什么？他们能捕捉到HIN内部复杂的结构和异质性吗？

  为了进行更直观的比较，我们进行了嵌入可视化，以在低维空间中表示HIN。目标是使用HIN表示学习模型学习节点嵌入，并将其投影到二维空间中。我们采用t-SNE[9]技术进行可视化，特别关注两个数据集上的论文表示。根据[39]，这里的节点分别根据ACM和MAG的字段和已发布的场所进行颜色编码。

  结果见 Figure 4

  1. 基于浅层模型的方法总是在来自不同场所的论文之间显示出混合模式，缺乏明确的聚类边界。例如，HIN2Vec将所有论文混合在一起，限制了它在HIN中捕获复杂结构和语义关系的能力。
  2. 基于HGNN的模型，比如GTN和R-GCN提供了更合理的可视化结果，但它们的聚类更分散，不那么紧凑。
  3. 相比之下，基于异质GT的模型（包括HINormer、HGT和我们的HHGT）的可视化始终表现出高度的类内相似性

* RQ3：HHGT的不同模块如何有助于提高模型性能？

  为了了解拟议框架内不同组件对两项任务整体性能的影响，我们通过在两个数据集上删除或替换HHGT的关键建模模块来进行消融研究。具体来说，我们关注三个关键模块：（1）用于距离异构建模的k环邻域结构及其相应的环级变换器，（2）用于进一步类型异构建模的（k，t）-环邻域结构和其相应的类型级变换器。（3）基于两个变换器编码器的**基于注意力的读出功能**。通过移除或替换这些模块，我们可以获得HHGT的不同变体，如下所示：

* RQ4：超参数如何影响HHGT的性能？

  * 环数的影响：

    我们将K的范围从1到10来分析环数的影响，节点分类结果如图8所示。正如观察到的那样，该模型在不同数据集上使用不同的K达到了最佳效果，因为各种HIN显示了不同的邻域配置。此外，随着K的增加，所有数据集的性能逐渐提高，随后随着进一步的增加，性能略有下降。**虽然更大的K意味着节点考虑更广泛的邻域，但太大的K可能会覆盖整个网络，导致节点的邻域包含大量不相关的信息，甚至过度拟合。**（这对我们也有一定的启发）

    





## A Survey of Graph Meets Large Language Model: Progress and Future Directions（图论和LLM的综述）

### ==思考与感悟==

增强器、预测器和对齐组件

* **LLMs 作为预测器的优势**：在**处理图的文本属性时表现出优越性**，与传统图神经网络（GNNs）相比，尤其在**零样本场景**下能取得显著性能。



### 其他补充

* 如何理解图生成学习（Generative Learning）

  最终目标是：训练出一个能够**理解图结构和节点语义**的强大图编码器（Encoder），**即便是在缺失信息时，也能恢复信息。**（这和 EASING 提出的缺少人工标注信息对应起来了）

  ==**既然之前说到过半监督学习可能和伪标签的质量有关，那么我们能不能考虑通过生成学习的方法生成更高质量的伪标签（感觉很靠谱）**==

  妈的貌似有人做过类似的了：

  > ENG[Yu等人，2023]授权LLM作为**样本生成器生成带有标签的额外训练样本，为GNN提供足够的监督信号。**

* 调是会涉及到之前预训练好的大模型？那我们如果在预训练好的大模型上微调，那岂不是又要重新训练，花费时间和资源。

  你这个问题问得非常关键，很多人初学 LLM 微调时都会有这个疑惑：

  > ❓“微调不是又要重新训练大模型吗？那还怎么叫‘微’调？不是又巨贵？”

  其实，**“微调 ≠ 从头训练大模型”**，而是在已有的**预训练模型参数基础上**，**对其中的一部分或全部进行小规模的“继续训练”**。下面我来详细解释。

  预训练过程花了巨大的资源，是一次性的，比如 GPT-3、T5、LLaMA、GLM 这些模型已经训练好了，模型参数你可以**直接下载使用**，不需要重头来过。

  | 微调方式      | 改动的参数（具体）                | 参数量（相对） | 成本 | 精度适配 |
  | ------------- | --------------------------------- | -------------- | ---- | -------- |
  | 全参数微调    | 模型所有参数                      | 💥 高           | 💸 高 | ✅ 最好   |
  | LoRA          | 每层中的 `W_q`, `W_v` 插入模块    | ⭐ 低（<1%）    | ✅ 中 | ✅ 很好   |
  | Adapter       | 每层插入小 MLP 模块               | ⭐ 低           | ✅ 中 | ✅ 好     |
  | Prompt Tuning | 输入 prompt embedding（不改模型） | 🌱 极低         | ✅ 低 | ❌ 较差   |
  | 只调任务 MLP  | Transformer 固定，仅调 head       | 🌱 极低         | ✅ 低 | ❌ 差     |



### 摘要

**背景重要性**：

- 图结构（Graph）在许多现实任务中扮演关键角色，如引文网络、社交网络、生物信息网络等。
- 图神经网络（GNN）一直是解决这些问题的主流方法，利用消息传递机制建模图结构。

**新趋势：引入大语言模型（LLMs）**：

- LLMs（如 GPT、BERT）在自然语言处理领域取得巨大成功，最近也被引入到图任务中，取得了超越传统 GNN 的性能。
- LLM 与图的结合能更好地处理文本属性丰富的节点、增强节点表示能力。

**文章内容**：

- 首先，**提出一个新的分类体系**：根据 LLM 在图任务中的角色，分为增强器（Enhancer）、预测器（Predictor）、对齐模块（Alignment Component）三类。
- 然后，**系统综述**这三类下的代表性工作。
- 最后，**讨论现有方法的不足与未来研究方向**。

**附加资源**：

- 提供了一个持续更新的项目列表（GitHub 链接），整理了相关论文和资源。



### 引言

> 这篇文章的**引言（Introduction）**部分主要围绕以下四个核心展开：
>
> 📌 一、研究背景与动机
>
> ✅ 图在现实世界中的重要性：
>
> - 图是一种基本的数据结构，被广泛用于建模**复杂关系**，如：
>   - **引文网络**（Citation Networks）
>   - **社交网络**（Social Networks）
>   - **分子结构**（Molecular Graphs）
> - 图数据通过节点之间的连接关系展现结构特性，能揭示系统中实体之间的依赖。
>
> ✅ 图神经网络（GNN）作为主要处理工具：
>
> - GNNs 是处理图数据的主流方法，如 GCN、GAT。
> - 通过递归的消息传递机制，实现对节点、边、图级别的表示学习。
>
> 📌 二、大语言模型（LLMs）的崛起
>
> ✅ LLMs 在文本任务中大放异彩：
>
> - Transformer、BERT、GPT 等 LLM 模型在自然语言处理任务中取得巨大成功。
> - 拥有强大的上下文建模与迁移能力，能用于情感分析、翻译、分类等任务。
>
> ✅ 趋势：将 LLMs 引入图任务：
>
> - 越来越多研究探索将 LLM 与图结构数据融合，尤其是图中含有**文本属性**（如论文标题、用户简介等）时。
> - **互补优势**（==**我们从这里可以得到启发**==）：
>   - ==**GNN 善于建模结构信息，但对节点语义信息建模能力弱。**==
>   - ==**LLM 善于理解文本信息，但缺乏对结构的建模能力。**==
>   - ==**结合两者可以提升表示能力和推理能力。**==
>
> 📌 三、典型案例介绍（说明 LLM 的三种作用）
>
> 文章引出了三个结合方式的代表性案例：
>
> 1. ==**TAPE：利用 LLM 生成与节点相关的语义解释作为 GNN 的输入增强。（2023）**==
> 2. ==**InstructGLM：用指令提示让 LLM 直接替代 GNN 的预测器。（2023）**==
> 3. ==**MoleculeSTM：将图嵌入与文本嵌入对齐，增强分子结构的推理能力。（2022）**==
>
> 这些例子展现了 LLM 在图学习中可以扮演多种角色。
>
> 📌 四、文章贡献与结构框架
>
> 📊 提出新的研究分类体系：
>
> - 根据 LLM 的角色，划分为：
>   - ==**Enhancer**（增强器）==
>   - ==**Predictor**（预测器）==
>   - ==**Alignment Component**（对齐模块）==
> - 并细化这些类别，梳理已有方法。
>
> 📚 系统综述：
>
> - 全面回顾并总结现有工作，覆盖主流方法与代表模型。
>
> 🔮 指出未来研究方向：
>
> - 包括：非文本图数据、迁移能力、解释性、计算效率、agent 化等挑战与机遇。

| 模块类型         | 类比作用                                    | 本质目标                           |
| ---------------- | ------------------------------------------- | ---------------------------------- |
| 增强器 Enhancer  | **替代/补充输入 embedding**                 | 强化节点/边的语义表征              |
| 预测器 Predictor | **替代 GNN 输出层（readout + classifier）** | 直接由 LLM 进行预测                |
| 对齐器 Alignment | **连接/协调两种编码器的表示空间**           | 融合结构信息与语言信息，提升表现力 |



### 预备知识

#### GNN

大多数现有的GNN都遵循**消息传递范式**，其中包含**消息聚合**和**特征更新**，如GCN[Kipf和Welling，2016]和GAT[Velickovic等人，2018]。它们通过迭代地聚合邻居的信息并用非线性函数更新它们来生成节点表示。（这部分完全懂了）

前向过程的范式是
$$
h^{(l)}_i = U \left( h^{(l-1)}_i, M\left( \{ h^{(l-1)}_i, h^{(l-1)}_j | v_j \in \mathcal{N}_i \} \right) \right)
$$
很好理解

* **Graph pre-training and prompting**

  > **一、Graph Pre-training 图预训练**
  >
  > 📌 问题背景：
  >
  > - GNN 在新任务或新图上表现受限，且需要大量人工标注。
  > - 图预训练（Graph Pre-training）是图领域中的“迁移学习”方案，目标是让模型学到通用图表示。
  >
  > ✅ 两类主流方法：
  >
  > 1. **对比学习（Contrastive Learning）**：
  >    - 思路：构造两个图的不同视图，训练模型让它们的表示更接近。
  >    - 代表方法：
  >      - GraphCL（2020）
  >      - GCA（2021）：也扩展到了高阶结构（如超图）
  > 2. **生成学习（Generative Learning）**：
  >    - 思路：对图进行“遮盖”，然后训练模型恢复被遮盖的部分。
  >    - 代表方法：
  >      - GraphMAE（2022）
  >      - S2GAE（2023）
  >      - WGDN（2023）
  >
  > 🧠 技术假设：
  >
  > - 下游任务与预训练任务有共享的表示空间。
  > - 类似 NLP 中的“先 pretrain 再 fine-tune”。
  >
  > **二、Graph Prompting 图提示学习**
  >
  > 📌 新趋势：
  >
  > - 在 NLP 领域，越来越多工作从“预训练 + 微调”转向“预训练 + 提示 + 微调（prompt-tuning）”。
  > - Prompting = 利用“人工或自动构造的输入形式”，激发模型内在能力。
  >
  > ✅ 代表性方法：
  >
  > 1. **GPPT（Graph Pretraining and Prompt Tuning）**：
  >    - 第一阶段：用边遮盖任务进行预训练。
  >    - 第二阶段：将节点任务重新表述为“边预测任务”，即把节点转成一对 token，再用边分类方式解决。
  > 2. **All in One**（Prompt统一化框架）：
  >    - 把**所有图任务和语言任务**都统一成“提示 + 输入 + 输出”的格式。
  >    - 可以在多个任务上做统一训练，提高多任务泛化能力。

#### LLM

> 1️⃣ LLM 的定义（Definitions）
>
> > ❗注意：目前并没有一个公认统一的 LLM 定义。
>
> 📌 本文采用的定义标准如下：
>
> - 借鉴两个重要综述 [Zhao et al., 2023d] 和 [Yang et al., 2023] 的观点：
>   - **LLM（Large Language Model）** 通常指参数量达到**十亿级别（billion-level）**的模型，经过大规模语料预训练。
>   - **PLM（Pretrained Language Model）** 泛指早期的、参数量较小的模型（百万级别），可以被轻量微调到特定任务上。
>
> 🤝 在本论文中：
>
> > ==**将 LLM 和 PLM 一并统称为“LLM”**==
> >  因为在图学习中，实际使用的大多是如 BERT、T5 这样参数较小、可嵌入的语言模型，而非超大模型（如 GPT-4），所以论文放宽了 LLM 的定义范围，便于分类分析。

* **Evolution**

  LLM 可分为非自回归（如 BERT）和自回归模型（如 GPT-3），分别用于理解和生成任务。随着模型规模扩大，它们表现出强大的 zero/few-shot 能力和推理能力，是图学习任务中非常宝贵的语义增强工具。

#### Proposed Taxonomy

本节提出了一个围绕 LLM 在图任务中作用的三层分类体系：增强器、预测器、对齐器。该体系清晰地区分了 LLM 参与图任务的方式，有助于系统化理解研究趋势，也为未来方法设计提供了参考。对于不能直接归类的创新方法，也额外提出了“其他”类，以展示 LLM 更广泛的图任务潜力。

> | 类别                     | 含义                                 | 核心任务             | 示例                   |
> | ------------------------ | ------------------------------------ | -------------------- | ---------------------- |
> | **1. LLM as Enhancer**   | LLM 增强 GNN 的输入或中间表示        | 增强节点/边嵌入      | TAPE, GIANT            |
> | **2. LLM as Predictor**  | LLM 替代传统 GNN 的预测模块          | 分类、推理、生成输出 | GPT4Graph, InstructGLM |
> | **3. GNN–LLM Alignment** | 对齐两个模型的表征空间，实现协同训练 | 模态对齐、跨模态检索 | MoMu, GLEM             |

详见论文 Figure-2



### LLM 作为增强器

为什么需要 Enhancer？

许多图任务中的节点具有文本属性（如论文标题、简介等），但传统做法（如 TF-IDF、word2vec）无法充分表达语义，导致 GNN 的输入特征信息稀疏、语义弱。

> LLM-as-Enhancer的目标
>
> 让 LLM 提供更高质量、更语义丰富的节点嵌入，**用于增强 GNN 的输入**（或中间层），从而提升最终表现。

分为两大类增强方式：

1. **Explanation-based Enhancement**（解释式增强）
2. **Embedding-based Enhancement**（嵌入式增强）



#### 基于解释的增强

需要彻底理解 Figure 3 - (a) 这张图

* 理解 LLM 和 LM 在论文中的区别与联系

  | 维度                    | LLM                                  | LM                             |
  | ----------------------- | ------------------------------------ | ------------------------------ |
  | **是否可训练 / 可微调** | ❌ 不可训练，视为黑盒（如 GPT-4 API） | ✅ 可训练（如 BERT、T5）        |
  | **使用方式**            | 用 prompt + 输入生成“解释文本”       | 把文本编码为向量嵌入           |
  | **输出类型**            | 文本（解释 / 标签）                  | 向量（embedding）              |
  | **代表模型**            | ChatGPT, GPT-4, Claude（API）        | BERT, SciBERT, RoBERTa（开源） |
  | **调用开销**            | 高（逐节点生成）                     | 低（可批量编码）               |

* 其范式为

  ```bash
  原始文本 ti + prompt p
         │
       调用 LLM（生成 ei：解释、知识等）
         │
  xi = fLM(ei, ti)  ← 组合原文本 + LLM输出，转成向量
         │
    输入 GNN（X + A）进行图学习
  ```

* 代表方法

  **TAPE**（He et al., 2023）：

  - 给节点（论文）输入 prompt：“请解释这篇论文主要讲了什么？”
  - LLM 生成伪标签或解释文本，再训练轻量模型提取向量表示。

  **KEA**（Chen et al., 2023a）：

  - 让 LLM 为节点生成相关的知识实体+文本描述（如“这篇论文与图注意力网络相关”），再编码为 embedding。

  **LLM4Mol**（Qian et al., 2023）：

  - 对分子结构的 SMILES 表达式，生成“分子作用、用途”等语言描述。

  **LLMRec**（Wei et al., 2023）：

  - 用 LLM 为用户和物品生成额外文本特征（如兴趣/属性），**缓解推荐场景中的稀疏性问题**（还真是）。

* 优势

  适用于封闭源 LLM（如 ChatGPT）

  解释信息对**分类、推荐**等任务尤为有效

* 局限

  每个节点都要请求一次 LLM → **成本高、慢**

  依赖 prompt 工程，控制力较弱（就是说我们生成的补充文本、知识片段、伪标签全都强烈依赖这个 prompt 的设计）



#### 基于嵌入的增强

直接将 LLM 作为编码器，**把文本属性转为向量嵌入 xi**，作为 GNN 的输入节点特征。

这种方法需要使用嵌入可见或开源LLM，**因为它需要立即访问文本嵌入**，并使用结构信息微调LLM。

如何理解论文中提到的级联形式？

> 具体而言，在嵌入性增强中，研究人员直接将大语言模型（LLMs）输出的文本嵌入作为图神经网络（GNNs）的初始节点嵌入。为了让 LLMs 更好地捕捉图的结构特征，这类方法通常会将图的结构信息（如节点间的连接关系、邻域信息等）融入 LLMs 的预训练或微调阶段，形成 “结构信息辅助语言模型训练→语言模型生成嵌入→GNNs 利用嵌入进行图学习” 的级联流程。

* 其范式为

  ```bash
  原始文本 ti
       │
  xi = fLLM(ti)   ← 直接调用 LLM 的 encoder，得到嵌入
       │
  GNN(X, A)       ← 正常图神经网络训练
  ```

* 代表方法（一看一个不吱声~~，完全不了解细节）

  **GIANT**（Chien et al., 2021）：

  - 用 PLM（如 BERT）编码文本特征，通过多视图训练得到更鲁棒的表示。

  **SimTeG**（Duan et al., 2023）：

  - 用链接预测任务微调 BERT，使其能感知图结构信息。

  **TouchUp-G**（Zhu et al., 2023）：

  - 增强结构感知性，用负采样强化训练。

  **G-Prompt**（Huang et al., 2023b）：

  - 在 BERT 后添加一个图适配模块（adapter），并加入提示向量以支持多任务学习。

  **WalkLM**（Tan et al., 2023b）：

  - 把图转成“随机游走 + 描述句子”，再送入 LLM 训练，提取表示。

  **LEADING**（Xue et al., 2023）：

  - 将 LLM 迁移到图表示学习，通过轻量微调方式提升可迁移性。

* 优势

  更系统化、自动化、端到端训练可能性更高

  开源 LLM（如 BERT）可直接嵌入 GNN 管道

*  局限

  需要嵌入式访问 LLM（必须能拿到 embedding）

  GPT-4 这类闭源模型无法使用该方式



#### 总结

| 对比维度     | Explanation-based              | Embedding-based         |
| ------------ | ------------------------------ | ----------------------- |
| 使用闭源 LLM | ✅ 支持（如 ChatGPT）           | ❌ 需要开放模型结构      |
| 节点处理成本 | 高（每个节点都需调用一次生成） | 低（批量编码）          |
| 表达能力     | 强，含人类解释                 | 强，但依赖 LLM 训练阶段 |
| 灵活性       | 高，可与任意 GNN 组合          | 高，但需嵌入访问权      |
| 可扩展性     | 差，大图不适合                 | 好，可用于大规模图      |



### LLM 作为预测器（主要用于解决分类和推理任务）

这一类别背后的核心思想是利用LLM在统一的生成范式内对广泛的图形相关任务（如**分类和推理**）进行预测。然而，将LLM应用于图形模态带来了独特的挑战，**主要是因为图形数据通常缺乏直接转换为连续文本的方法**，**因为不同的图以不同的方式定义结构和特征**。在本节中，我们将模型大致分为基于扁平化和基于GNN的预测，**具体取决于它们是否使用GNN来提取LLM的结构特征**。

#### Flatten-based Prediction

Flatten-based Prediction 是 “LLM as Predictor” 类别下的一种核心方法，其核心思路是将图结构转化为文本序列，使大语言模型（LLMs）能够直接处理图数据并完成预测任务1。该方法主要包含两个关键步骤：图的扁平化转换和预测结果解析。

* **图的扁平化（Graph Flattening）**

  通过一个扁平化函数 $\text{Flat}(\cdot)$ 将图的节点、边、节点文本属性、边文本属性等要素转换为文本序列 $G_{seq}$
  $$
  G_{seq} = \text{Flat}(\mathcal{V}, \mathcal{E}, \mathcal{T}, \mathcal{J})
  $$

* **预测与解析（Prediction & Parsing）**

  将文本序列 $G_{seq}$ 与任务相关的指令提示 $p$ 输入 LLMs，再通过解析函数 $\text{Parse}(\cdot)$ 从 LLMs 的输出中提取预测标签 $\overline{Y}$ ，公式为
  $$
  \overline{Y} = \text{Parse}(f_{\text{LLM}}(G_{seq},p))
  $$

* 之前工作的解析策略

  鸽了，不列举了



* 两者区别

  与解析策略相比，**扁平化策略可以表现出显著的差异**。在下文中，我们根据LLM的参数是否更新来组织扁平化方法。

  1. **扁平化策略**：聚焦于将图结构（节点、边、属性等）转化为 LLMs 可处理的文本序列，是 “**输入转化**” 阶段的操作。其目的是解决图数据与 LLMs 输入格式不兼容的问题，让 LLMs 能够 “读懂” 图的结构和信息。
  2. **解析策略**：针对 LLMs 生成的输出结果，提取出最终预测标签，是 “**输出处理**” 阶段的操作。其目的是从 LLMs 可能包含推理过程的自然语言输出中，精准获取任务所需的预测结果（如分类标签、推理结论等）。
  3. **扁平化策略**：确保图数据能被 LLMs 有效 “理解”，重点在于保留图的结构和属性信息。
  4. **解析策略**：确保从 LLMs 的输出中准确 “提取” 预测结果，重点在于适配下游任务对输出格式的要求。

   简言之，扁平化策略是 “如何让 LLMs 看懂图”，解析策略是 “如何从 LLMs 的回答中得到答案”，二者分别对应图输入到预测输出的两个关键环节。



##### LLM 冻结

指在将图结构转化为文本序列输入大语言模型（LLMs）时，不更新 LLMs 的参数，即保持模型参数冻结状态。

该策略的核心是通过特定方式将图结构转化为 LLMs 可处理的文本序列，而不对 LLMs 本身进行微调。具体的扁平化方式多样，例如：

- GPT4Graph 利用 GML、GraphML 等图描述语言来表示 graphs，这些语言为图中的节点和边提供了标准化的语法和语义；
- GraphText 受**语言语法树**启发，利用图语法树将图结构转换为节点序列，进而输入 LLMs 进行无训练的图推理；
- ReLM 使用简化分子输入线输入系统（SMILES）字符串来提供分子图结构的一维线性化表示；
- 还有一些方法直接采用数值组织的节点和边列表，以纯文本形式描述图数据；
- 此外，也可通过**自然叙述**来表达图结构，如 Chen 等人和 Hu 等人的研究**将引用网络的结构信息整合到提示中**，**用 “cite” 一词明确表示边关系，并用论文索引或标题表示节点**；==**Huang 等人则通过枚举当前节点的随机选择的 k 跳邻居来描述关系，而不使用 “cite” 一词。**==

这些方式均在不调整 LLMs 参数的前提下，实现了图结构向文本序列的转化，使 LLMs 能够处理图数据并完成相关任务。



##### LLM 微调

这类策略通过特定方式将图结构信息融入 LLMs 的微调过程，使模型更适配图相关任务。例如：

- GIMLET 采用基于距离的位置嵌入，**将两个节点在图中的最短距离定义为相对位置**，以此**扩展 LLMs 感知图结构的能力**，这种方式在图 Transformer 相关研究中被广泛应用。
- InstructGLM 则基于**最大跳数**设计了一系列可扩展提示，==**通过自然语言描述连接关系，让中心节点能与任意跳数的邻居建立直接关联，从而帮助 LLMs 更好地理解图中节点间的多跳连接。**==

通过微调，LLMs 能够更有效地处理经扁平化后的图文本序列，提升在图相关任务中的预测性能，**且部分模型经微调后可直接输出==预测标签==，减少后续解析步骤的成本。**



#### GNN-based Prediction

在 “LLM as Predictor” 框架下，“GNN-based Prediction”（GNN 辅助预测）是与 “Flatten-based Prediction” 并列的另一类核心方法。**其核心思路是利用图神经网络（GNNs）擅长捕捉图结构信息的优势，先提取图的结构特征，再将这些特征输入大语言模型（LLMs）进行预测，使 LLMs 具备结构感知能力。**



##### 核心流程

* **图特征学习**：通过 GNNs 处理节点嵌入矩阵 （$X$） 和邻接矩阵（$A$），生成包含结构信息的嵌入（$H$）,公式为：
  $$
  H= f_{GNN}(X,A)
  $$

* **预测与解析**：将结构感知嵌入 $H$ 与任务提示（$p$）输入 LLMs，再通过解析函数从 LLMs 的输出中提取**预测结果**
  $$
  \tilde{Y} = \text{Parse}(f_{LLM}(H,p))
  $$

与扁平化预测不同，这类方法依赖 GNNs 提取结构特征，且由于需要将 GNN 表示融入 LLMs，通常需要进行微调，以便在训练中标准化 LLMs 的预测格式。



##### 特征融合策略

现有研究提出了多种融合 GNN 结构特征与 LLMs 文本信息的策略，例如：

* **跨模态投射器**：GIT-Mol 和 MolCA 采用 BLIP-2 的 QFormer 作为跨模态投射器，将 GNN 编码器的输出映射到 LLM 的输入文本空间，并通过多目标注意力掩码策略实现有效的图 - 文本交互。
* **前缀调优**：GraphLLM 在 prefix tuning 过程中，通过线性投影将图表示转化为图增强前缀，使 LLMs 能与图 Transformer 协同融合图推理所需的结构信息。
* **线性层对齐**：GraphGPT 和 InstructMol 使用简单的线性层作为轻量级对齐投射器，将编码后的图表示映射为 “图令牌”，使 LLMs 能将这些令牌与各类文本信息对齐。
* **分层注入**：DGTL 将解耦的图嵌入直接注入 LLMs 的每一层，突出图的拓扑和语义的不同方面。



#### 总结

这部分内容围绕将大语言模型（LLMs）直接作为预测器处理图相关任务的优势、目标及两种预测方式（扁平化预测和 GNN 辅助预测）的特点与局限展开，具体如下：

- **LLMs 作为预测器的优势**：在**处理图的文本属性时表现出优越性**，与传统图神经网络（GNNs）相比，尤其在**零样本场景**下能取得显著性能。

  > 具体到图相关任务中，当将大语言模型（LLMs）作为预测器时，其在零样本场景下表现出色，即不需要针对某类图任务（如特定类型的图分类、推理等）进行专门的训练，仅通过输入任务描述或相关提示，就能利用自身预训练过程中习得的知识完成任务，**这与传统图神经网络（GNNs）需要大量标注数据进行训练才能较好地处理特定任务形成对比**。
  >
  > 
  >
  > **这种能力得益于 LLMs 在大规模文本语料上预训练所获得的强大泛化能力和知识储备**，使其能够在未见过特定任务样本的情况下，理解任务需求并生成合理的预测结果。

- **核心目标**：开发并优化将图结构信息编码为 LLMs 能够有效且高效理解和处理的格式的方法。

- 两种预测方式的对比

  - **扁平化预测**：在有效性方面可能更具优势，但受 LLMs 输入长度限制，每个节点只能获取少数几跳内邻居的信息，难以捕捉长距离依赖关系；且由于不涉及 GNNs，**无法解决 GNNs 固有的异质性等问题。**
  - **GNN 辅助预测**：往往更高效，但需要额外训练 GNN 模块并将其插入 LLMs 进行联合训练，而深度 Transformer 早期层存在梯度消失问题，这使得训练难度较大。







