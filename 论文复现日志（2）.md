#  论文复现日志（2）

这里记录的主要是一些本人思考 idea 时借鉴的文章。



## HHGT

[链接](https://dl.acm.org/doi/pdf/10.1145/3701551.3703511)

### 摘要

表现力限制和过度平滑促使研究人员探索图形变换器（GT）以增强HIN**表示学习**。



### 引言

近年来，关于异构图的表示学习激增，这项技术可以**将节点嵌入到低维表示**中，同时**保留图结构和异构性**。

#### HGCN

基于HGNN的方法[11,19]通常利用**邻居聚合**策略，在HIN中的不同类型节点之间有效地捕获和传播信息。例如，R-GCN[23]通过结合关系特定的权重矩阵，在HIN中捕获各种关系，扩展了传统的图卷积网络（GCN）[17]。Fu等人[11]提出沿元路径合并中间节点，使用元路径内和元路径间信息进行高阶语义信息聚合。

尽管HGNN在模拟现实世界的HIN方面取得了成功，**但表达能力[35]、过度平滑[5]和过度压缩[1]等挑战的存在促使研究人员研究图形变换器（GT）[40]，以增强HIN表示学习**。例如，Hu等人[14]提出了一种**用于邻居聚合的异构Transformer式注意力架构**。Mao等人[21]利用**局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息**。一般来说，现有的基于GT的HIN表示学习方法，即图1（b）所示的基于HGT  （异构图 transformer） 的方法，遵循一个典型的原则：**给定一个目标节点，首先提取其k跳邻域（即距离目标节点≤k的可达距离内的节点）。然后，Transformer[29]将用于将信息从这些节点传播到目标节点。**（==其实就是做一个子图的处理==）

比如 `figure 1` 中的 `b Existing HGT-based methods` 就揭露了当下的 HGT 的手段与缺陷！！！：

> 将 P1 的 2-hop 邻居（P2、A1、A3、S1、P3、P4）**全部统一处理**；
>
> 也就是说：
>
> - **不区分距离**：P2 是直接邻居（1-hop），P3 是间接邻居（2-hop），都混在一起；
> - **不区分类型**：论文、作者、主题的表示放到一个 attention 序列中统一计算；
>
> 缺点：语义容易混淆，难以理解不同节点“在结构中的不同语义角色”。



论文中也指出了了当下 HGT 的痛点、缺陷与不足（==这一块务必仔细阅读，好好理解==）

> 然而，现有的基于HGT的方法倾向于混合不同类型的节点，并在邻居聚合过程中统一处理k-hop邻域内的所有节点，从而导致潜在的语义混淆。特别是，（1）限制1：HIN中不同距离的目标节点的邻居具有不同的语义。以图1（a）为例，论文P1的直接邻居论文P2表示引用关系。相反，p1的间接邻居p3暗示了一种没有直接引用关系的主题联系，展示了不同的内涵。遗憾的是，现有的策略通过统一处理距离k内的每个邻居来忽略这些区别，即将P1、P2、P3打包成一个序列并统一聚合它们。这是不理想的，因为这些节点服务于不同的功能。（2）限制2：不同类型的目标节点的邻居也具有不同的语义。以图1（a）为例，论文P1的直接邻居包括论文P2、作者A1、A3和受试者S1。这里，P2表示引用关系，A1、A3表示作者关系，S1表示主题对齐关系。虽然现有的基于HGT的方法**考虑了节点类型**，**但它们通常将P2、A1、A3、S1打包在一起作为一个统一的序列**（**这是 HGT 所没有考虑到的**）。这种方法是不可取的，**因为它在邻居聚合过程中混合了不同类型的节点，模糊了论文、作者和受试者的不同功能。**



下面列举了一些**以前的**代表性工作

* R-GCN

  其实就是**最原始的**异构图的处理手段，对不同类型的边（关系）使用不同的变换矩阵，从而实现“关系感知”的聚合。

  > 我们以一个学术图（Academic Graph）为例，图中包含：
  >
  > - **三种节点类型**：Paper（P）、Author（A）、Venue（V）
  > - **几种关系类型**：
  >   - Paper ←write-by→ Author
  >   - Paper ←published-in→ Venue
  >   - Paper ←cite→ Paper
  >
  > 例如：
  >
  > - P1 是由 A1 和 A2 撰写的论文，发表于 V1；
  > - P2 是由 A2 撰写的论文，引用了 P1；
  > - P3 是由 A3 撰写的，和 P1 都发表于 V1。
  >
  > 图自己画一下就行了（很简单）
  >
  > 
  >
  > **方法核心：**
  >
  > ==R-GCN 对不同类型的边（关系）使用不同的**变换矩阵**，从而实现“关系感知”的聚合。==
  >
  > 
  >
  >  **举例：**
  >
  > 假设我们要更新节点 P1 的表示：
  >
  > - P1 的邻居包括：
  >   - A1, A2（write 关系）
  >   - V1（published-in）
  >   - P2（被 P2 引用）
  >
  > R-GCN 的聚合逻辑是：
  >
  > - 使用写作者的表示向量，通过参数矩阵 **W_write** 变换；
  > - 使用 Venue 的表示向量，通过 **W_publish** 变换；
  > - 使用被引用的论文的表示向量，通过 **W_cite** 变换；
  > - 聚合时不同关系分别处理，不会“混淆”。

  

* MAGNN [csdn阅读链接](https://blog.csdn.net/byn12345/article/details/105101492) （**暂时搁置**。。）反正这里这种方法是基于元路径的多种聚合手段

  基于元路径的手段，使用元路径合并中间节点，使用元路径内和元路径间信息**进行高阶语义信息聚合**。聚合整个路径的表示

  > 现有的基于元路径的嵌入学习方法有以下局限性：
  >
  > （1）忽略节点的内容特征（属性信息），不能很好地处理节点属性特征丰富的异质图。例如 metapath2vec, ESim, HIN2vec, HERec。
  >
  > （2）舍弃了元路径内部的节点信息，只考虑元路径的起始节点和末尾节点，造成信息损失。例如 HERec, HAN。
  >
  > （3）只依赖于单个元路径，因此需要人工选择元路径，丢失了来自其他元路径的部分信息，导致性能不佳。例如 metapath2vec。
  >
  > 为了解决上述问题，本文提出MAGNN（Metapath Aggregated Graph Neural Network ）。
  >
  > MAGNN由三个部分组成：
  >
  > （1）节点内容转换(node content transformation )，将异质的节点属性信息映射到同一个**隐层的向量空间**；
  >
  > （2）元路径**内部聚合**(intra-metapath aggregation )，使用注意力机制将元路径**内部的语义信息**纳入考虑；
  >
  > （3）元路径**间的聚合**(inter-metapath aggregation )，使用**注意力机制从多个元路径聚合信息**。
  > 
  >



* HGT （贡献很大的一篇文章）

> 将标准 Transformer 改造为**异构图专用模型**，支持多种节点类型与边类型。
>
> 可以理解为：**注意力不仅考虑邻接关系，还区分每种异构边、异构节点对的交互模式。**
>
> **2. 时间编码（可选）：**
>
> 他们还为动态图（如引用网络）引入时间编码，可以处理时间序列。
>
>  **3. 基于 Transformer 的堆叠式结构：**
>
> 和标准 GNN 一样，多层 HGT 会逐层融合多跳邻居信息，但内部机制是带类型感知的 Transformer。



*  HINormer 

利用局部结构编码器和异构关系编码器来捕获HIN中的结构和异构信息。（貌似和我最开始的想法很像）

进一步改进 HGT，缓解计算量大、信息干扰的问题，同时提升表达能力。

> 1. **Local Structure Encoder**
>
> - 构造目标节点的局部结构视图，比如：
>   - 局部子图中有哪些节点、类型、边；
>   - 用轻量注意力机制捕捉结构模式。
>
> 2. **Heterogeneous Relation Encoder**
>
> - 更精细建模不同关系的作用（类似于 HGT 但更高效）；
> - 使用了结构编码 + 边编码的组合策略；
> - 学习关系类型与语义的交互表示。
>
> 3. **优势：性能更优，计算更快**
>
> - 他们主张结构编码和语义融合可以减少**不必要的全 attention**；
> - 因此**更适合大图、低资源环境**；
> - 实验中比 HGT 更快，效果也更强。