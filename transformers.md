# transfroms

## å®‰è£…é—®é¢˜

æˆ‘ä»¬éœ€è¦ä½¿ç”¨ hugging faceï¼Œä½†æ˜¯è¢«å¢™ï¼Œå³ä¾¿ç§‘å­¦ä¸Šç½‘ä¹Ÿæ— æ³•å®‰è£…æ¨¡å‹ã€‚å…·ä½“è§£å†³åŠæ³•å¯ä»¥æŸ¥çœ‹[é•œåƒç½‘ç«™](https://hf-mirror.com/)çš„æ•™ç¨‹ã€‚æˆ‘å·²ç»è®¾ç½®ä¸ºå…¨å±€é•œåƒäº†ï¼Œä½†å¥½åƒæ²¡æœ‰ã€‚

~~æ„Ÿè§‰åªæœ‰æ‰‹åŠ¨ä¸‹è½½äº†~~

ç»ˆäºè¡Œäº†ï¼

è§£å†³æ–¹æ³•æ˜¯åœ¨å¯¼å…¥ç›¸å…³çš„ `transformers` **ä¹‹å‰**åŠ ä¸Šè¿™ä¸¤å¥ä»£ç å³å¯è§£å†³é—®é¢˜

```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

ä»¥ä¸‹æ˜¯åŸå› è§£é‡Šï¼š

> è¿™æ˜¯å› ä¸º Hugging Face çš„ `transformers` åº“åœ¨**é¦–æ¬¡å¯¼å…¥æ—¶**ï¼Œå°±ä¼šè¯»å–ç¯å¢ƒå˜é‡æ¥ç¡®å®šä½ ä½¿ç”¨çš„é•œåƒæˆ–è®¿é—®åœ°å€ã€‚
>
> ğŸ” è§£é‡ŠåŸå› ï¼š
>
> å½“ä½ æ‰§è¡Œï¼š
>
> ```python
> from transformers import AutoTokenizer, AutoModel
> ```
>
> `transformers` åº“ä¼šç«‹åˆ»ï¼š
>
> - æ£€æŸ¥ `HF_ENDPOINT` ç¯å¢ƒå˜é‡ã€‚
> - å¦‚æœä½ é…ç½®äº†ï¼Œå®ƒå°±ä¼šä»æŒ‡å®šçš„é•œåƒåœ°å€ï¼ˆå¦‚ `https://hf-mirror.com`ï¼‰ä¸‹è½½æ¨¡å‹æˆ–æ•°æ®ã€‚
> - å¦‚æœæ²¡æœ‰é…ç½®ï¼Œå®ƒé»˜è®¤ä½¿ç”¨å®˜æ–¹åœ°å€ï¼ˆ`https://huggingface.co`ï¼‰ï¼Œä¸­å›½å¤§é™†ç”¨æˆ·å¾€å¾€è®¿é—®å›°éš¾æˆ–è¶…æ—¶ã€‚
>
> âœ… æ‰€ä»¥æ­£ç¡®çš„é¡ºåºæ˜¯ï¼š
>
> å¿…é¡»åœ¨å¯¼å…¥ `transformers` **ä¹‹å‰**è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæ¯”å¦‚è¿™æ ·ï¼š
>
> ```
> pythonå¤åˆ¶ç¼–è¾‘import os
> os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
> 
> from transformers import AutoTokenizer, AutoModel  # å¿…é¡»æ”¾åœ¨åé¢
> ```
>
> å¦åˆ™ï¼Œ`transformers` æ¨¡å—åŠ è½½æ—¶å·²ç»é”™è¿‡äº†è¯»å–ç¯å¢ƒå˜é‡çš„æœºä¼šï¼Œé…ç½®å°±ä¸ä¼šç”Ÿæ•ˆäº†ã€‚



## ç®€å•å…¥é—¨

### ä¸‹è½½å¥½çš„æ¨¡å‹æ–‡ä»¶æœ‰å“ªäº›

æˆ‘ä»¬ä»¥ `uer/gpt2-chinese-cluecorpussmall` è¿™ä¸ªæ¨¡å‹ä¸ºä¾‹ï¼Œè§‚å¯Ÿä»¥ä¸‹å…¶ä¸‹è½½å®Œæˆä¹‹åéƒ½æœ‰å“ªäº›ä¸œè¥¿ã€‚

åœ¨å¿«ç…§ `snapshots` æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€äº›æ–‡ä»¶

1. `config.json`

   ```json
   {
     "activation_function": "gelu_new",
     "architectures": [
       "GPT2LMHeadModel"
     ],
     "attn_pdrop": 0.1,
     "embd_pdrop": 0.1,
     "gradient_checkpointing": false,
     "initializer_range": 0.02,
     "layer_norm_epsilon": 1e-05,
     "model_type": "gpt2",
     "n_ctx": 1024,
     "n_embd": 768,
     "n_head": 12,
     "n_inner": null,
     "n_layer": 12,
     "n_positions": 1024,
     "output_past": true,
     "resid_pdrop": 0.1,
     "task_specific_params": {
       "text-generation": {
         "do_sample": true,
         "max_length": 320
       }
     },
     "tokenizer_class": "BertTokenizer",
     "vocab_size": 21128
   }
   ```

   è¿™æ˜¯ä¸€ä¸ª**è‡ªå®šä¹‰ GPT2 æ¨¡å‹é…ç½®**ï¼Œç‰¹ç‚¹å¦‚ä¸‹ï¼š

   - æ¨¡å‹ç»“æ„å’Œå±‚æ•°åŸºæœ¬å’Œ GPT2-base ä¿æŒä¸€è‡´ï¼ˆ12å±‚ï¼Œ768ç»´ï¼Œ12å¤´ï¼‰ï¼›
   - ä½¿ç”¨äº† **BertTokenizer** å’Œ **ä¸­æ–‡è¯è¡¨å¤§å°ï¼ˆ21128ï¼‰** â†’ è¯´æ˜å¯èƒ½æ˜¯ **ç”¨äºä¸­æ–‡çš„ GPT2**ï¼›
   - å¯ç”¨äº†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„å…¸å‹è®¾ç½®ï¼ˆé‡‡æ ·ï¼Œ320 é•¿åº¦ï¼‰ï¼›
   - å…³é—­äº† gradient checkpointingï¼›
   - é…ç½®åˆç†ï¼Œé€‚ç”¨äº Hugging Face çš„ `from_pretrained()` æˆ– `from_config()` åŠ è½½æ¨¡å‹ã€‚

   æˆ‘ä»¬å¯ä»¥é‡ç‚¹å…³æ³¨ä¸€ä¸‹ä¸‹è¿°è¿™ä¸¤ä¸ªå‚æ•°

   >   "tokenizer_class": "BertTokenizer", è¡¨æ˜ä½¿ç”¨äº† BERT åˆ†è¯å™¨
   >   "vocab_size": 21128	è¯è¡¨å¤§å°ä¸º21128

2. vocab.txt

   é‡Œé¢è®°è½½äº†è¯¥æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„æ‰€æœ‰å­—ï¼ˆå…±21120ä¸ªï¼‰

### å¦‚ä½•ç†è§£ `pipeline`

åœ¨ Hugging Face çš„ `transformers` åº“ä¸­ï¼Œ`pipeline` æ˜¯ä¸€ä¸ª**é«˜çº§å°è£…å·¥å…·**ï¼Œå®ƒå¯ä»¥**å¿«é€Ÿè°ƒç”¨å„ç§é¢„è®­ç»ƒæ¨¡å‹å®Œæˆå¸¸è§çš„ NLP ä»»åŠ¡**ï¼Œä¸ç”¨æ‰‹åŠ¨å¤„ç† tokenizerã€æ¨¡å‹åŠ è½½ã€å‰å¤„ç†ã€åå¤„ç†ç­‰ç¹çç»†èŠ‚ã€‚

> `pipeline` æ˜¯ Hugging Face æä¾›çš„â€œå‚»ç“œå¼ä¸€é”®æ¨ç†æ¥å£â€ï¼Œ**è¾“å…¥ä¸€å¥è¯ï¼Œè¿”å›ç»“æœ**ï¼Œæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç­‰ä»»åŠ¡ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€æ˜“çš„åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•çš„åœºæ™¯ä»£ç 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline

cache_dir = r"D:\data\model\models--uer--gpt2-chinese-cluecorpussmall\snapshots\c2c0249d8a2731f269414cc3b22dff021f8e07a3"

model = AutoModelForCausalLM.from_pretrained(cache_dir)
tokenizer = AutoTokenizer.from_pretrained(cache_dir)


from transformers import pipeline

generator = pipeline(
    task = "text-generation",  # æ³¨æ„æ˜¯ "text-generation"ï¼Œä¸æ˜¯ "text-generate"
    model = model,
    tokenizer = tokenizer,
    device = 0  # device=0 è¡¨ç¤ºä½¿ç”¨ç¬¬0å· GPUï¼›å¦‚æœç”¨ CPUï¼Œå¯ä»¥çœç•¥è¿™ä¸ªå‚æ•°
)

output = generator(
    "ä»Šå¤©å¤©æ°”çœŸå¥½å•Š",  # è¾“å…¥å­—ç¬¦ä¸²ç›´æ¥å†™ï¼Œä¸éœ€è¦ `inputs=`
    max_length = 50,
    num_return_sequences = 1
)

print(output)

```

åœ¨ä¸åŠ è°ƒå‚çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆçš„ç»“æœæ˜¯æ¯”è¾ƒç³Ÿç³•çš„ã€‚

* `AutoModelForCausalLM`æ˜¯ Hugging Face Transformers åº“ä¸­çš„ä¸€ä¸ª**è‡ªåŠ¨æ¨¡å‹åŠ è½½å™¨ç±»**ï¼Œç”¨äºåŠ è½½æ”¯æŒ **Causal Language Modelingï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼‰** çš„æ¨¡å‹

  `AutoModelForCausalLM` ä¼šæ ¹æ®ä½ æä¾›çš„æ¨¡å‹é…ç½®ï¼ˆ`config.json`ï¼‰ï¼Œ**è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„**ï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Œ**ç”¨äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡**ã€‚ `AutoTkenizer` åŒç†ï¼Œä¹Ÿæ˜¯æ ¹æ®æ¨¡å‹é…ç½® `config.json` æ¥è‡ªåŠ¨å¤„ç†







### å…¶ä»–æ³¨æ„äº‹é¡¹

* æ¨¡å‹åŠ è½½ä¸€èˆ¬ä½¿ç”¨ `.from_pretrained()` å‡½æ•°ï¼Œä¸”éœ€è¦ä½¿ç”¨**ç»å¯¹è·¯å¾„**ï¼Œä¸ç„¶è¿˜ä¼šå» hugging face ä¸­å»ä¸‹è½½

  ç›®å½•æ˜¯åŒ…å« `config.json` çš„ç›®å½•
  
* æ‰€æœ‰åœ¨ transformers ä¸­çš„æ•°æ®éƒ½æ˜¯ä»¥æ‰¹æ¬¡å½¢å¼å­˜åœ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´å³ä¾¿åªæœ‰ä¸€ä¸ªå¥å­ï¼Œå®ƒä¹Ÿæ˜¯ä»¥ `[[sent]]` è¿™ç§å½¢å¼å­˜åœ¨è€Œé `[sent]`



## 1_tokenizer_sentiment_analysis

###  é‡ç‚¹ç†è§£ tokenizer çš„è¿‡ç¨‹ä»¥åŠé‡è¦çš„è¶…å‚æ•°

* å› ä¸º tokenizer å’Œ model æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼ˆå³ tokenizer çš„è¾“å‡ºæ˜¯ model çš„è¾“å…¥ï¼‰ï¼Œæ‰€ä»¥ä¸€ä¸ªä½¿ç”¨çš„èŒƒå¼æ˜¯ï¼š
  ```python
  batch_input = tokenizer(test_sequence,)
  model(**batch_input)
  ```

  * è¾“å‡ºï¼š`{'input_ids': tensor, 'attention_mask': tensor}`

  * **å·¥ä½œè¿‡ç¨‹**

    * `tokenizer`

      ```python
      test_senteces = ['today is not that bad', 'today is so bad', 'so good']
      batch_input = tokenizer(test_senteces, truncation = True, padding = True, return_tensors = "pt")
      batch_input
      ```

    * `tokenizer.tokenize()`

      è¿™æ˜¯ä¸€ä¸ªåˆ†è¯å‡½æ•°ï¼Œå°±æ˜¯å°†è¾“å…¥çš„å¥å­ç»™åˆ†è¯ï¼Œä¾‹å¦‚

      ```python
      tokenizer.tokenize(test_senteces[0],)
      ```

      ```bash
      ['today', 'is', 'not', 'that', 'bad']
      ```

    * `tokenizer.convert_tokens_to_ids()`

      ç”¨æ¥æŠŠ**ä¸€ä¸ªæˆ–å¤šä¸ªå·²åˆ†å‰²å¥½çš„ tokenï¼ˆå­—ç¬¦ä¸²ï¼‰è½¬æ¢æˆå¯¹åº”çš„è¯è¡¨ IDï¼ˆæ•´æ•°ï¼‰**

      ```python
      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences[0],))
      ```

      ```bash
      [2651, 2003, 2025, 2008, 2919]
      ```

      æ²¡æœ‰å¼€å§‹ ID åºåˆ—å’Œç»“æŸ ID åºåˆ—

    * `tokenizer.encode()`

      ç”¨äºå°† **ä¸€æ¡æ–‡æœ¬ï¼ˆæˆ–ä¸€å¯¹æ–‡æœ¬ï¼‰ç¼–ç æˆ token çš„ ID åºåˆ—**ã€‚

      å®ƒå°†è¾“å…¥çš„æ–‡æœ¬ **è½¬æ¢æˆ token ID çš„åˆ—è¡¨**ï¼ˆæ•´æ•°ï¼‰ã€‚è¿”å›çš„æ˜¯ **çº¯æ•´æ•°åˆ—è¡¨**ï¼Œä¸æ˜¯å­—å…¸ï¼Œä¹Ÿæ²¡æœ‰ attention maskã€‚

      ```python
      tokenizer.encode(test_senteces[0],)
      ```

      ```bash
      [101, 2651, 2003, 2025, 2008, 2919, 102]
      ```

      è¿™é‡Œ 101 å’Œ 102 åˆ†åˆ«æ˜¯å¼€å§‹ç¬¦å’Œç»“æŸç¬¦ï¼›2651 å¯¹åº” todayï¼Œä¾æ¬¡ç±»æ¨ã€‚

      å¯ä»¥å‘ç° `tokenizer.encode()` â‰ˆ `tokenizer.tokenize() + tokenizer.convert_tokens_to_ids()`

    * `tokenizer.decode()`

      ç”¨æ¥å°† **token ID åˆ—è¡¨è½¬æ¢å›å¯è¯»æ–‡æœ¬å­—ç¬¦ä¸²** çš„æ–¹æ³•ã€‚

      ```python
      tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919,  102])
      ```

      ```bash
      '[CLS] today is not that bad [SEP]'
      ```

      `[CLS]` å’Œ `[SEP]` æ˜¯ **BERT** ä»¥åŠå¾ˆå¤šåŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ï¼ˆspecial tokensï¼‰ï¼Œå®ƒä»¬å„è‡ªæœ‰ç‰¹å®šçš„ä½œç”¨ï¼š

      | Token   | ä½œç”¨                       |
      | ------- | -------------------------- |
      | `[CLS]` | èšåˆåºåˆ—ä¿¡æ¯ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ |
      | `[SEP]` | åˆ†å‰²å¥å­æˆ–æ–‡æœ¬ç‰‡æ®µ         |

    **`tokenizer` å·¥ä½œçš„åŸç†å…¶å®å°±æ˜¯ `tokenizer.vocab`ï¼šå­—å…¸ï¼Œå­˜å‚¨äº† token => id çš„æ˜ å°„å…³ç³»ã€‚å½“ç„¶è¿™ä¸ªå­—å…¸ä¸­è¿˜åŒ…å«äº†ä¸€äº›ç‰¹æ®Šçš„ `token`ï¼š`tokenizer.special_tokens_map`**

    **`tokenizer` æ˜¯æœåŠ¡äº `model` çš„**

### å‚æ•°ç†è§£

ä»¥ä¸€ä¸ªä¾‹å­ä¸ºä¾‹

```python
tokenizer(test_sentences, max_length = 32, truncation = True, padding = 'max_length', return_tensors = 'pt')
```

è¿™é‡Œé¢ `max_length` ä¸ `padding` æ˜¯â€ä¸å…¼å®¹â€œçš„ã€‚å½“ä½ åˆ¶å®šäº† `max_length` ä¹‹ååˆæƒ³è¦å¡«å……ï¼Œåˆ™éœ€è¦ä»¤ `padding = 'max_length'`

æˆ–è€…

```python
tokenizer(test_sentences, truncation = True, padding = True, return_tensors = 'pt')
```

å½“æˆ‘ä»¬æŠŠ `max_length` æ¶ˆç­æ‰åï¼Œ`padding` å°±å¯ä»¥æŒ‡å®šä¸º True äº†ï¼Œåº”è¯¥å°±æ˜¯å…ˆéå†ä¸€éæ‰€æœ‰çš„å¥å­ï¼Œç„¶åå°†æœ€é•¿çš„é‚£ä¸ªå¥å­çš„é•¿åº¦ä½œä¸ºâ€œ`max_length`â€ï¼Œ`padding = True` å³ä¸ºå¡«å……

| å€¼             | å«ä¹‰                                      | è¯´æ˜                                 |
| -------------- | ----------------------------------------- | ------------------------------------ |
| `True`         | å¯ç”¨ paddingï¼ˆé»˜è®¤å¯¹è¯¥ batch å†…æœ€é•¿åºåˆ—ï¼‰ | åŠ¨æ€ paddingï¼Œé€‚åˆæ¨¡å‹æ•ˆç‡ä¼˜åŒ–       |
| `"longest"`    | ç­‰ä»·äº `True`                             | åª pad åˆ°å½“å‰ batch ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ |
| `"max_length"` | å¡«å……åˆ° `max_length` æŒ‡å®šçš„å›ºå®šé•¿åº¦        | å¸¸ç”¨äºæ¨¡å‹è®­ç»ƒï¼ˆé™æ€ shapeï¼‰         |
| `False`        | ä¸è¿›è¡Œ padding                            | **æ‰€æœ‰æ ·æœ¬é•¿åº¦éœ€ä¸€è‡´ï¼Œå¦åˆ™æŠ¥é”™**     |

äº‹å®ä¸Š `attention_mask` ä¹Ÿä¸ `padding` ç›¸åŒ¹é…ã€‚`attention_mask` ä¸º 0 çš„éƒ¨åˆ†å³ä¸º `padding` çš„éƒ¨åˆ†



### è¾“å…¥åˆ°æ¨¡å‹ï¼Œç†è§£æ¨¡å‹çš„è¾“å‡º

```python
# ç”±äºæ˜¯æ¨ç†çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¸æ¶‰åŠè®­ç»ƒï¼Œå› æ­¤æ”¾å…¥ no_grad()ä¸­
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
```

å¯ä»¥å°† logits ç†è§£ä¸ºï¼šåœ¨é€åˆ° softmax ä¹‹å‰çš„è¾“å‡ºã€‚

æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸ª logits å†æ¬¡è¿›è¡Œå¤„ç†ã€‚è§ä¸‹ï¼š

```python
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
    scores = F.softmax(outputs.logits, dim = 1)
    print(scores)
    labels = torch.argmax(scores, dim = 1)
    print(labels)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
tensor([[8.4632e-04, 9.9915e-01],
        [9.9980e-01, 1.9531e-04],
        [1.5837e-04, 9.9984e-01]])
tensor([1, 0, 1])
```

å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„ç»“æœä¸º `[1, 0, 1]` è¡¨ç¤ºç§¯æã€æ¶ˆæã€ç§¯æã€‚

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ¨¡å‹çš„é…ç½®

```python
model.config
```

```bash
DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased-finetuned-sst-2-english",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "sst-2",
  "hidden_dim": 3072,
  "id2label": {
    "0": "NEGATIVE",
    "1": "POSITIVE"
  },
  "initializer_range": 0.02,
  "label2id": {
    "NEGATIVE": 0,
    "POSITIVE": 1
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.30.2",
  "vocab_size": 30522
}
```

å¯ä»¥å…³æ³¨ä¸‹å…¶ä¸­çš„ `id2label`ã€‚æˆ‘ä»¬å†åšä¸€ä¸ªæ˜ å°„

```python
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
    scores = F.softmax(outputs.logits, dim = 1)
    print(scores)
    labels = torch.argmax(scores, dim = 1)
    print(labels)
    # tensorçš„æ•°æ®ä¸èƒ½ç›´æ¥å»åšç´¢å¼•ã€‚è¦ä¹ˆ .item() è½¬æ¢è¦ä¹ˆ .tolist() 
    labels = [model.config.id2label[id] for id in labels.tolist()]
    print(labels)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
tensor([[8.4632e-04, 9.9915e-01],
        [9.9980e-01, 1.9531e-04],
        [1.5837e-04, 9.9984e-01]])
tensor([1, 0, 1])
['POSITIVE', 'NEGATIVE', 'POSITIVE']
```



## 2_tokenizer_encode_plus_token_type_ids

å­¦ä¹ ä¸€ä¸‹å‡çº§ç‰ˆçš„ `encode_plus`ï¼Œå®ƒä¼šå¾—åˆ° `token_type_ids`

åˆæ­¥ç†è§£ä¸€ä¸‹è¿™ä¸¤ä¸ªçš„åŒºåˆ«ï¼ˆéœ€è¦å……åˆ†ç†è§£ input_ids, attention_mask, token_type_ids, padding çš„å«ä¹‰ä¸ä½œç”¨ï¼‰ï¼š

| åŠŸèƒ½                    | `encode`        | `encode_plus`         |
| ----------------------- | --------------- | --------------------- |
| è¿”å›ç±»å‹                | list            | dict                  |
| åªè¿”å› input_ids        | âœ…               | âŒï¼ˆåŒ…å«æ›´å¤šå­—æ®µï¼‰     |
| æ”¯æŒ attention_mask     | âŒ               | âœ…                     |
| æ”¯æŒ token_type_ids     | âŒ               | âœ…                     |
| æ”¯æŒ padding/truncation | âŒ               | âœ…                     |
| ç”¨äºæ¨¡å‹è¾“å…¥            | âŒï¼ˆéœ€æ‰‹åŠ¨åŒ…è£…ï¼‰ | âœ…ï¼ˆé€‚åˆç›´æ¥è¾“å…¥æ¨¡å‹ï¼‰ |

### å¤ä¹ 

`bert-base-uncased` æ˜¯ä¸€ä¸ª **è‹±è¯­é¢„è®­ç»ƒ BERT æ¨¡å‹**ï¼Œ**12 å±‚ Transformer ç¼–ç å™¨ç»“æ„**ï¼Œè¾“å…¥æ˜¯å°å†™è‹±æ–‡ï¼Œ**ä¸åŒºåˆ†å¤§å°å†™**ï¼ˆuncasedï¼‰ã€‚å¯ä»¥æ ¹æ®è‡ªå·±è¾“å…¥çš„å¥å­å®Œæˆä¸€äº›ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡

```python
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir = cache_dir)
tokenizer
```

è‡ªå·±æ¢ç´¢ä¸€ä¸‹è¯¥ `tokenizer` çš„ special_tokenã€‚å°†å…¶ç¼–ç ä¹‹åå†å¯¹å…¶è§£ç ï¼ˆå¾ˆç®€å•çš„ä»»åŠ¡ï¼‰



### è®¤è¯†æ–‡æœ¬è¯­æ–™ï¼ˆæç½®ï¼Œå¾…è¡¥ï¼‰

- `newsgroups_train.DESCR`
- `newsgroups_train.data`
- `newsgroups_train.target`
- `newsgroups_train.target_names`





## 3_bert_model_architecture_paramsï¼ˆbert æ¨¡å‹æ¡†æ¶åˆæ¢ï¼‰

* æ‚è®°

embeddings: BertEmbeddings

encode: BertEncoder: layer 0 ~ 11

pooler: BertPooler

è¦å­¦ä¼šæŸ¥çœ‹æ¨¡å‹ç»“æ„ï¼Œçœ‹ä»–çš„å±‚ï¼Œæ€è€ƒä»–åœ¨åšä»€ä¹ˆã€‚

### ä¸€äº›ç†è§£

Bert æ˜¯ transformer çš„ encode éƒ¨åˆ† ï¼ˆäº‹å®ä¸Šè¿˜åŒ…æ‹¬ embedding éƒ¨åˆ†å’Œ pooler éƒ¨åˆ†ï¼‰ï¼Œè€Œ transformer æ˜¯ä¸€ä¸ª encoder - decoderï¼ˆseq2seqï¼‰æ¨¡å‹ã€‚

`BertForSequenceClassification` å°±æ˜¯ä¸€ä¸ª Bert æ¨¡å‹åŠ äº†ä¸€ä¸ªäºŒåˆ†ç±»çš„â€œå¤´â€ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„å®ƒæ˜¯åŸºäº Bert æ„å»ºçš„ä¸€ä¸ªåˆ†ç±»ä¸‹æ¸¸ä»»åŠ¡ã€‚

ä»¥ä¸Šéƒ½æ˜¯é€šè¿‡ä»æ¥å£å±‚é¢ç›´æ¥è§‚å¯Ÿæ¨¡å‹ç»“æ„å¾—åˆ°çš„ã€‚

è¿™ä¸ª Bert æ¨¡å‹åªå–äº† transformer çš„ encode éƒ¨åˆ†ï¼ˆä¹Ÿå³ self-attention å’Œ feed-forward éƒ¨åˆ†ï¼‰

> - `bert: encoder of transformer`
>   - `transformer: encoder-decoder(seq2seq)`
> - `bert`
>   - `embeddings`
>     - `word(token) embedding`
>     - `position embedding`
>     - `token type embedding`ï¼ˆå…¶å®å°±æ˜¯ `segment embedding`ï¼‰
>   - `encoder(12 layer)`
>     - `self attention (kqv)`
>     - `feed forward`
>   - `pooler`



```scss
è¾“å…¥æ–‡æœ¬ â†’ åˆ†è¯ â†’ token_ids â†’
[åµŒå…¥å±‚]
    â””â”€â”€ Token Embeddings
    â””â”€â”€ Segment Embeddings
    â””â”€â”€ Position Embeddings
â†“
[12 å±‚ Transformer Encoder]
    æ¯å±‚ï¼š
        â””â”€â”€ å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰
        â””â”€â”€ Add & Norm
        â””â”€â”€ å‰é¦ˆç½‘ç»œï¼ˆFeedForward Layerï¼‰
        â””â”€â”€ Add & Norm
â†“
[è¾“å‡º]
    â””â”€â”€ æ¯ä¸ª token çš„å‘é‡ï¼ˆHidden Stateï¼‰
    â””â”€â”€ [CLS] token å‘é‡ï¼ˆå¥å­è¡¨ç¤ºï¼‰

```



* å…³äº `CLS`

  `[CLS]` æ˜¯äººä¸ºåŠ çš„ â€”â€” å®ƒçš„å­˜åœ¨æ˜¯ **ä¸ºäº†æä¾›ä¸€ä¸ªä¸“é—¨ç”¨æ¥è¡¨ç¤ºâ€œæ•´å¥è¯â€çš„å‘é‡**ã€‚

  > åŸå› åœ¨äºï¼š
  >
  > - Transformer çš„è¾“å‡ºæ˜¯ï¼š**æ¯ä¸ª token çš„å‘é‡**
  > - ä½†å¾ˆå¤šä»»åŠ¡ï¼ˆä¾‹å¦‚å¥å­åˆ†ç±»ï¼‰**åªéœ€è¦ä¸€ä¸ªå‘é‡ä»£è¡¨æ•´å¥è¯**
  > - é‚£ç”¨å“ªä¸ª token å¥½å‘¢ï¼Ÿ
  >
  > BERT çš„è®¾è®¡è€…å°±è¯´ï¼š
  >
  > > é‚£æˆ‘ä»¬åŠ ä¸€ä¸ªä¸“ç”¨ tokenï¼Œå« `[CLS]`ï¼Œè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¼šæŠŠâ€œå¥å­çš„æ„æ€â€æ”¾åœ¨å®ƒé‡Œé¢ã€‚
  >
  > åœ¨é¢„è®­ç»ƒæ—¶ï¼ŒBERT æœ‰ä¸€ä¸ªä»»åŠ¡å« NSPï¼ˆNext Sentence Predictionï¼‰ï¼š
  >
  > > æ¨¡å‹è¦åˆ¤æ–­ï¼šä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­ï¼Ÿ
  > >  â€”â€” åˆ¤æ–­æ—¶å°±ç”¨ `[CLS]` è¾“å‡ºåšåˆ†ç±»ã€‚
  >
  > å› æ­¤ï¼ŒBERT é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å°±**å¼ºè¿«æ¨¡å‹å­¦ä¼šäº†â€œæŠŠæ•´å¥è¯çš„ä¿¡æ¯èšåˆåˆ° `[CLS]` ä¸Šâ€**ã€‚

### å‚æ•°é‡ç»Ÿè®¡

ä¹‹æ‰€ä»¥å«å¤§æ¨¡å‹ï¼Œæ˜¯å› ä¸ºå‚æ•°æ•°é‡åºå¤§ã€‚base è¿™ä¸ªåŸºç¡€æ¨¡å‹æœ‰12å±‚ï¼Œå¤§çº¦ 1 äº¿ä¸ªå¯è®­ç»ƒå‚æ•°ã€‚

**é»˜è®¤æƒ…å†µä¸‹ï¼Œ`pretrained BERT` æ¨¡å‹çš„æ‰€æœ‰å‚æ•°éƒ½æ˜¯å¯å­¦ä¹ çš„ï¼ˆ`requires_grad=True`ï¼‰**ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆfine-tuneï¼‰ã€‚ä½†ä¹Ÿå¯ä»¥é€‰æ‹©åªè®­ç»ƒéƒ¨åˆ†å‚æ•°ï¼Œè¿™å–å†³äºä½ çš„ä»»åŠ¡éœ€æ±‚å’Œè®­ç»ƒèµ„æºã€‚

```python
total_params = 0
total_learnable_params = 0
total_embedding_params = 0
total_encoder_params = 0
total_pooler_params = 0

for name, param in model.named_parameters():
    print(name, "->", param.shape, "->", param.numel())

    if "embedding" in name:
        total_embedding_params += param.numel()
    
    if "encode" in name:
        total_encoder_params += param.numel()
    
    if "pooler" in name:
        total_pooler_params += param.numel()
    
    if param.requires_grad:
        total_learnable_params += param.numel()
    
    total_params += param.numel()
    
print(total_params)
print(total_learnable_params)
params = [total_embedding_params, total_encoder_params, total_pooler_params]
for param in params:
    print(param/sum(params))
```



```bash
embeddings.word_embeddings.weight -> torch.Size([30522, 768]) -> 23440896
embeddings.position_embeddings.weight -> torch.Size([512, 768]) -> 393216
embeddings.token_type_embeddings.weight -> torch.Size([2, 768]) -> 1536
embeddings.LayerNorm.weight -> torch.Size([768]) -> 768
embeddings.LayerNorm.bias -> torch.Size([768]) -> 768
encoder.layer.0.attention.self.query.weight -> torch.Size([768, 768]) -> 589824
# å±€éƒ¨

109482240
109482240
0.21772649152958506
0.776879099295009
0.005394409175405983
```

åç»­ä¼šæåˆ°ä½•æ—¶ä¼šå†»ç»“ä¸€äº› layerï¼Œä½•æ—¶ä¸å†»ç»“ä»»ä½• layerã€‚



## 4_no_grad_requires_grad

å…¶å®è¿™æ˜¯ Pytorch ä¸­çš„çŸ¥è¯†ï¼Œä½†æ˜¯å’±ä»¬å¯ä»¥å†å¤ä¹ ä¸€ä¸‹

- `torch.no_grad()`
  - å®šä¹‰äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œ**éšå¼åœ°**ä¸è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œ**ä¸ä¼šæ”¹å˜ `requires_grad`**ï¼ˆè™½ç„¶åœ¨è¯¥ç¯å¢ƒä¸‹è®¡ç®—æ¢¯åº¦ä¸ä¼šæ›´æ–°ï¼Œä½†æ˜¯å…¶ä»ç„¶ä¿ç•™ `requires_grad` çš„åŸæœ¬å±æ€§ï¼‰
  - é€‚ç”¨äº `eval` é˜¶æ®µï¼Œæˆ– `model forward` çš„è¿‡ç¨‹ä¸­æŸäº›æ¨¡å—ä¸æ›´æ–°æ¢¯åº¦çš„æ¨¡å—ï¼ˆæ­¤æ—¶è¿™äº›æ¨¡å—ä»…è¿›è¡Œç‰¹å¾æå–ï¼ˆå‰å‘è®¡ç®—ï¼‰ï¼Œä¸åå‘æ›´æ–°ï¼‰
- `param.requires_grad`
  - æ˜¾å¼åœ° `frozen` æ‰ä¸€äº› `moduleï¼ˆlayerï¼‰`çš„æ¢¯åº¦æ›´æ–°
  - `layer/module` çº§åˆ«
  - å¯èƒ½ä¼šæ›´**çµæ´»**ï¼ˆå¾®è°ƒçš„æ—¶å€™å¯ä»¥é˜²æ­¢å‚æ•°è¿‡å¤šå¼•èµ·çš„æ˜¾å­˜çˆ†ç‚¸ï¼‰

### éªŒè¯

```python
def calc_learnable_params(model):
    total_params = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            total_params += param.numel()
    return total_params

print(calc_learnable_params(bert))	# è¾“å‡º109482240

with torch.no_grad():
    print(calc_learnable_params(bert))	# è¾“å‡º109482240ï¼Œè¯´æ˜åœ¨è¯¥ç¯å¢ƒä¸‹ä¸ä¼šè¯¥è¾¹æ¨¡å‹å‚æ•° requires_grad çš„å±æ€§
    
    
for name, param in bert.named_parameters():
    if param.requires_grad:
        param.requires_grad = False
print(calc_learnable_params(bert))		# frozenè¿‡ç¨‹ï¼Œè¾“å‡º 0
    
```



## 5_bert_embedding-output

æŸ¥çœ‹ bertmodel çš„æºç ï¼Œæˆ‘ä»¬å‘ç°å…¶å®šä¹‰äº† `embedding` , `encoder` , `pooler` éƒ¨åˆ†ï¼Œè¿™é‡Œä»‹ç» `embedding` éƒ¨åˆ†çš„**å‰å‘è¿‡ç¨‹**

è¿™é‡Œçš„ `embeddings` å®é™…ä¸Šæ˜¯é€šè¿‡ `nn.embedding` + ç´¢å¼•æŸ¥è¡¨å®ç°çš„

### å°ç»“

- `bert input embedding`ï¼šä¸€ç§åŸºäº `nn.embedding` + ç´¢å¼•çš„æŸ¥è¡¨æ“ä½œï¼ˆlookup tableï¼‰
  - æŸ¥è¡¨ï¼ˆè¿™é‡Œçš„è¯å…¸å¤§å°æ˜¯ 30522, `hidden_dim` ä¸º 768ï¼‰
    - `token embeddings`ï¼š30522 * 768
    - `segment embeddings`ï¼š2 * 768 ~~ï¼ˆå®ƒå¯¹æ•´ä¸ªæ‰¹æ¬¡æ˜¯**å…±äº«**çš„ï¼Œæ‰€ä»¥å®ƒè¿™é‡Œä¸éœ€è¦æ‰¹æ¬¡ï¼‰~~
    - `position embeddings`: 512 * 768
  - åå¤„ç†
    - `layer norm`
    - `dropout`

### ç›¸å…³æºç ä»¥åŠç†è§£å‰å‘è¿‡ç¨‹

ä»¥ä¸‹æ˜¯ `BertEmbeddings` çš„éƒ¨åˆ†æºç ï¼ˆæˆ‘ä»¬åªå…³æ³¨ `init` éƒ¨åˆ†ï¼‰ï¼Œå‘ç°å’Œå°èŠ‚ä¸Šæ€»ç»“çš„ä¸€ç§©ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªåµŒå…¥è¡¨ + ä¸¤ä¸ªåå¤„ç†æ„æˆ

```python
class BertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings."""

    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
        # any TensorFlow checkpoint file
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        # position_ids (1, len position emb) is contiguous in memory and exported when serialized
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)))
        self.register_buffer(
            "token_type_ids", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False
        )
```

å…¶ä»–éœ€è¦æ³¨æ„çš„æ˜¯

1. `embedding` çš„å‘ˆç°å½¢å¼éƒ½æ˜¯æ‰¹çš„ï¼Œå› ä¸ºæ¥å—çš„ `ids` ä¹Ÿæ˜¯æ‰¹çš„å½¢å¼

2. å‰å‘è¿‡ç¨‹ä¸­ï¼Œåå¤„ç†ä¹‹å‰å¾—åˆ°çš„ `embedding` æ˜¯ä¸‰ä¸ª `embedding` ç›¸åŠ ï¼Œä¹Ÿå°±æ˜¯

   ```python
   input_embed = token_embed + seg_embed + pos_embed	# [B, L, D]
   ```

   æœ€ç»ˆå¾—åˆ°çš„ `embedding`  è¿˜è¦ç»è¿‡ä¸¤ä¸ªåå¤„ç†è¿‡ç¨‹



## 6_subword_wordpiece_tokenizer

subwordæ„ä¸ºå­—è¯ï¼Œwordpieceåˆ™æ˜¯å°†ä¸€ä¸ªè¯åˆ†ç‰‡ï¼ˆå°†ä¸€ä¸ªè¯æ‹†åˆ†æˆå¤šä¸ªå­—è¯ï¼‰

é¦–å…ˆæˆ‘ä»¬éœ€è¦æ˜ç™½ï¼Œæ‰€æœ‰çš„è¯æ±‡æ•°é‡éå¸¸å¤šï¼Œæˆ‘ä»¬ä¸å¯èƒ½å°†ä¸–ç•Œä¸Šæ‰€æœ‰çš„è¯æ±‡éƒ½å­˜å…¥å­—å…¸ã€‚`'bert-base-uncased'` è¿™ä¸ªæ¨¡å‹ä¸­å­˜å…¥çš„è¯æ±‡æ•°é‡ä¸º 30522ï¼Œå¹¶æ²¡æœ‰è¦†ç›–æ‰€æœ‰çš„è¯ã€‚é‚£ä¹ˆä¸€å®šä¼šæœ‰ä¸€äº›å¤„ç†çš„æ‰‹æ³•ã€‚

### è¯æ€§ç®€çŸ­åˆ†ç±»

```python
s1 = 'albums sold 124443286539 copies'				# æ•°å­—å‹
s2 = 'technically perfect, melodically correct'		# melodically å°†å½¢å®¹è¯è½¬æ¢æˆå‰¯è¯
s3 = 'featuring a previously unheard track'			# ä¸å¤ªå¸¸è§çš„æ‹¼æ¥å‰ç¼€ unheard
s4 = 'best-selling music artist'					# çŸ­æ¨ªçº¿å½¢å¼ï¼ˆå°½é‡è§„é¿è¿™ç§å½¢å¼ï¼‰
s5 = 's1 d1 o1 and o2'								# å­è¢‹
s6 = 'asbofwheohwbeif'								# æ— æ„ä¹‰çš„å­—ç¬¦
```

é’ˆå¯¹ä»¥ä¸Š 6 ç§ç±»å‹çš„â€œå¥å­â€ï¼Œæ€è€ƒ tokenizer è¯¥å¦‚ä½•è§£å†³é—®é¢˜

å°†è¯æ‹†åˆ†ï¼Œåˆ†ç‰‡å¤„ç†



```python
tokenizer.vocab
tokenizer.ids_to_tokens
```

è¿™ä¸¤ä¸ªä¸€ä¸€æ˜ å°„



### æ ·æœ¬å­—è¯æµ‹è¯•

éœ€è¦æ˜ç¡®çš„æ˜¯ï¼ŒåŸºäºè¯æ±‡è¡¨ï¼Œ `tokenize` `encode` `decode` ä¸€ä½“

1. `tokenize`ï¼šå°† `word => token(s)`

   æ³¨æ„åŒºåˆ† `word` å’Œ `token(s)`ï¼Œå¹¶ä¸” `token(s)` ä¹Ÿæ˜¯ `vocab` ä¸­çš„ `keys`

2. `encode`ï¼šå°† `token(s) => ids`

   é€šè¿‡æŸ¥è¡¨çš„æ–¹å¼

3. decodeï¼šå°† `ids => token => word`

* é’ˆå¯¹æ•°å­—

  ```python
  inputs = tokenizer(s1)
  print(inputs)
  print(tokenizer.convert_ids_to_tokens(inputs["input_ids"]))
  ```

  ```bash
  {'input_ids': [101, 4042, 2853, 13412, 22932, 16703, 20842, 22275, 2683, 4809, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
  ['[CLS]', 'albums', 'sold', '124', '##44', '##32', '##86', '##53', '##9', 'copies', '[SEP]']
  ```

  å¯è§å…¶å…ˆæ˜¯å°† `124443286539` è½¬æ¢æˆ `'124', '##44', '##32', '##86', '##53', '##9'` åŒäº•å·è¡¨ç¤ºæ‹¼æ¥

* å‰¯è¯ç±»å‹

  `melodically => 'melodic', '##ally'`

* å‰ç¼€ç±»å‹

  `unheard => 'un', '##heard'`

* çŸ­æ¨ªçº¿å½¢å¼ï¼ˆç›´æ¥å°†çŸ­æ¨ªçº¿ - ç»™ splitäº†ï¼Œå°†ä¸€ä¸ª `word` å˜æˆäº† 3 ä¸ª tokenï¼‰

  `best-selling => 'best', '-', 'selling'`

  äº‹å®ä¸Šæˆ‘ä»¬å°† `bestselling` è¿™ä¸ªåˆ†è¯ä¼šå¤„ç†æˆ `'best', '##sell', '##ing'`, 

  > æˆ‘ä»¬å»ºè®®å°†çŸ­æ¨ªçº¿ `-` çš„è¿™ç§å¤„ç†æ–¹å¼è§„é¿æ‰ï¼Œè™½ç„¶å‰è€…å’Œåè€…éƒ½å°†ä¸€ä¸ª word è½¬æ¢æˆäº† 3 ä¸ªtokenï¼Œä½†æ˜¯ä» token è§’åº¦çš„ç†è§£ä¸Š`'best', '-', 'selling'` å¯ä»¥çœ‹æˆ**ä¸‰ä¸ªè¯**ï¼Œè€Œåè€…å¯ä»¥çŸ¥é“æ˜¯ä¸€ä¸ªè¯ï¼ˆ##è¡¨æ‹¼æ¥ï¼‰

* å­è¢‹

  `'s1 d1 o1 and o2' => '##1', 'd', '##1', 'o', '##1', 'and', 'o', '##2'`

* æ‘†çƒ‚å­—ç¬¦

  `'asbofwheohwbeif' = > 'as', '##bo', '##f', '##w', '##he', '##oh', '##w', '##bei', '##f'`

æ„Ÿè§‰å¤§æ¦‚èƒ½æ‘¸ç´¢å‡ºè§„å¾‹äº†

### å°ç»“

å†æ¬¡å¼ºè°ƒ

- tokenizer **è½»æ˜“ä¸ä¼š**å°†ä¸€ä¸ªè¯å¤„ç†ä¸º `[UNK] (100)`
- åŸºäºè¯æ±‡è¡¨ï¼Œtokenize, encode, decode ä¸€ä½“
    - tokenizeï¼šword => token(s)ï¼Œå°†wordå°½å¯èƒ½åœ°æ˜ å°„ä¸º vocab ä¸­çš„ keys
    - encode: token => id
    - decode: id => token => word
        - encode å®Œäº†ä¹‹åä¹Ÿä¸æ˜¯ç»ˆç‚¹ï¼ˆwordï¼‰ï¼Œdecode è¿˜è¦èƒ½å¾ˆå¥½åœ°å°† id è¿˜åŸï¼Œå°½å¯èƒ½ä¸è¾“å…¥çš„ word å¯¹é½ï¼›



## 7_model_outputs

### ç–‘é—®

* å¦‚ä½•ç†è§£æœ€åä¸€å±‚ `hidden` çš„è¾“å‡ºï¼ˆ`outputs[0]`ï¼‰ä»¥åŠ `embedding` çš„è¾“å‡ºï¼ˆ`ouputs[2][0]`ï¼‰

  > `outputs[0]` = `last_hidden_state`ï¼šæœ€åä¸€å±‚ Transformer è¾“å‡ºçš„éšè—çŠ¶æ€ï¼Œèåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
  >
  > `outputs[2]` = `hidden_states` ï¼šæ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ï¼š
  >
  > - `hidden_states[0]`ï¼šembedding å±‚è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯è¯å‘é‡+ä½ç½®å‘é‡+segmentå‘é‡åŠ å’Œçš„ç»“æœï¼›
  > - `hidden_states[1]`ï¼šç¬¬1å±‚ Transformer çš„è¾“å‡ºï¼›
  > - ...
  > - `hidden_states[-1]`ï¼šæœ€åä¸€å±‚ Transformer çš„è¾“å‡ºï¼ˆåŒ `outputs[0]`ï¼‰ï¼›
  >
  > æ‰€ä»¥ `outputs[2][0]` å°±æ˜¯**embedding å±‚çš„è¾“å‡º**ã€‚**å®ƒæ˜¯æœ€åˆçš„å‘é‡**è§ä¸Šæ–‡ `embedding = token_embed + seg_embed + pos_embed`

* å¦‚ä½•ç†è§£è¯¥æ¨¡å‹ `bert-base-uncased` çš„ `forward` è¿‡ç¨‹ï¼Ÿ

  `embedding -> encode -> pooler`

### outputs

å½“æˆ‘ä»¬åœ¨é¢„åŠ è½½æ¨¡å‹ï¼ˆ`from_pretrained` é˜¶æ®µï¼‰è¿›è¡Œè®¾ç½® `output_hidden_states = True` æ—¶ã€‚ `len(outputs)` å°†ä¼šå˜ä¸º 3

```python
outputs = model(**token_input)
```

| ç´¢å¼•         | å†…å®¹åç§°            | å«ä¹‰è¯´æ˜                                                     | å½¢çŠ¶                                                         |
| ------------ | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `outputs[0]` | `last_hidden_state` | æœ€åä¸€å±‚ Transformer çš„è¾“å‡ºï¼ˆæ¯ä¸ª token çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼‰å®ƒæ˜¯ token ç²’åº¦å±‚é¢çš„ | `[batch_size, seq_len, hidden_size]` â†’ `[1, 22, 768]`        |
| `outputs[1]` | `pooler_output`     | å¯¹ `[CLS]` token è¿›è¡Œçº¿æ€§å˜æ¢å’Œæ¿€æ´»çš„**å¥å‘é‡**ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰å®ƒæ˜¯**å¥å­ç²’åº¦å±‚é¢**çš„ã€‚ | `[batch_size, hidden_size]` â†’ `[1, 768]`                     |
| `outputs[2]` | `hidden_states`     | åŒ…å« embedding å±‚ + æ¯ä¸€å±‚ Transformer çš„è¾“å‡º                | `13 Ã— [batch_size, seq_len, hidden_size]` â†’ `13 Ã— [1, 22, 768]` |

è¿›è€Œæˆ‘ä»¬æœ‰ä»¥ä¸‹æ¨æµ‹

1. `outputs[0] == outputs[2][-1]`

2. `outputs[1] == model.pooler(outputs[0])`

3. `outputs[2][0] == model.embeddings(token_input["input_ids"], token_input["token_type_ids"])`

4. ```python
   for i in range(len(outputs[2])):
       print(i, outputs[2][i].shape)
   ```

   ```bash
   0 torch.Size([1, 22, 768])
   1 torch.Size([1, 22, 768])
   2 torch.Size([1, 22, 768])
   3 torch.Size([1, 22, 768])
   4 torch.Size([1, 22, 768])
   5 torch.Size([1, 22, 768])
   6 torch.Size([1, 22, 768])
   7 torch.Size([1, 22, 768])
   8 torch.Size([1, 22, 768])
   9 torch.Size([1, 22, 768])
   10 torch.Size([1, 22, 768])
   11 torch.Size([1, 22, 768])
   12 torch.Size([1, 22, 768])
   ```

### æœ€åä¸€å±‚éšè—å±‚è¿›å…¥ `pooler` å±‚æ—¶æ€æ ·å·¥ä½œçš„

å³æ¢ç©¶ `outputs[1] == model.pooler(outputs[0])` 

æˆ‘ä»¬å…ˆæ‰¾åˆ°å…·ä½“çš„æºç ï¼š

```python
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```

`model.pooler(outputs[0])` æœ¬è´¨ä¸Šåšäº†è¿™ä»¶äº‹ï¼š

> BERT ä¸­çš„ `pooler_output`ï¼ˆä¹Ÿå°±æ˜¯ `outputs[1]`ï¼‰ï¼Œ**å°±æ˜¯å– `[CLS]` çš„è¯å‘é‡ï¼Œåšä¸€ä¸ªçº¿æ€§å˜æ¢ + Tanh æ¿€æ´»åå¾—åˆ°çš„ç»“æœ**ï¼Œè¢«è§†ä¸º**æ•´ä¸ªå¥å­çš„è¯­ä¹‰è¡¨ç¤º**ï¼ˆå¥å‘é‡ / sentence embeddingï¼‰ã€‚

è¿™ä¹Ÿæ˜¯ `outputs[1]` çš„çœŸå®ç”Ÿæˆè¿‡ç¨‹ã€‚

> è™½ç„¶è¿™æ˜¯ BERT çš„åŸç”Ÿå¥å‘é‡æ–¹æ¡ˆï¼Œä½†ï¼š
>
> - **å®ƒä¸ä¸€å®šæ•ˆæœæœ€å¥½**ï¼Œå› ä¸º `[CLS]` å‘é‡å¯¹å¥å­çš„ä»£è¡¨èƒ½åŠ›å—è®­ç»ƒç›®æ ‡å½±å“ï¼›
> - å¦‚æœåšæ£€ç´¢ã€èšç±»ã€ç›¸ä¼¼åº¦ä»»åŠ¡ï¼Œå¸¸ç”¨æ›´å¼ºçš„å¥å‘é‡æ–¹æ³•ï¼Œæ¯”å¦‚ï¼š
>   - **å¹³å‡æ± åŒ–ï¼ˆMean Poolingï¼‰**ï¼šå¯¹ `last_hidden_state` å–å¹³å‡ï¼›
>   - **Sentence-BERT**ï¼šç”¨**ä¸“é—¨è®­ç»ƒè¿‡çš„å¥å‘é‡æ¨¡å‹**ï¼Œæå‡è¯­ä¹‰ç›¸ä¼¼åº¦å¯¹é½èƒ½åŠ›ã€‚

==æˆ‘ç›®å‰å¯¹è¿™ç§åšæ³•è¿˜ä¸æ˜¯å¾ˆèƒ½æ¥å—ï¼Œéšç€åç»­åœ°æ›´æ·±å…¥äº†è§£åº”è¯¥ä¼šè±ç„¶å¼€æœ—==



## 8_attn_01

æˆ‘ä»¬ä¼šå¯¹æ¯” model çš„ self-attention çš„è¾“å‡ºä»¥åŠè‡ªå·±å†™çš„è®¡ç®— attention çš„è¾“å‡ºï¼Œè¿›è¡Œå¯¹æ¯”çœ‹æ˜¯å¦ä¸€æ ·ã€‚ç›®çš„æ˜¯ä¸ºäº†äº†è§£ attention è®¡ç®—è¿‡ç¨‹ä¸­çš„ç»†èŠ‚

### ç–‘é—®

* ```python
  from bertviz.transformers_neuron_view import BertModel
  from transformers import BertModel
  ```

  ä»è¿™ä¸¤ä¸ªæ¨¡å—ä¸­å¯¼å…¥çš„ BertModel æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

  > è¿™ä¸¤ä¸ª `BertModel` æ¥è‡ªä¸åŒçš„æ¨¡å—ï¼ŒåŠŸèƒ½å’Œç”¨é€”ä¹Ÿæœ‰æ‰€ä¸åŒï¼š
  >
  > 1. **`from bertviz.transformers_neuron_view import BertModel`**ï¼š
  >    - è¿™æ˜¯ `BertViz` åº“ä¸­çš„ `BertModel`ã€‚`BertViz` æ˜¯ä¸€ä¸ªå¯è§†åŒ–å·¥å…·åŒ…ï¼Œç”¨äºå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåˆ†æå’Œå¯è§†åŒ– BERT æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå†…éƒ¨ç‰¹å¾ã€‚
  >    - è¿™ä¸ª `BertModel` ç±»æ˜¯ `BertViz` ç‰¹å®šçš„æ‰©å±•ç‰ˆæœ¬ï¼Œæä¾›äº†é¢å¤–çš„å¯è§†åŒ–åŠŸèƒ½ï¼Œå¦‚é€šè¿‡æ³¨æ„åŠ›å¾—åˆ†æ¥å±•ç¤ºæ¨¡å‹çš„å·¥ä½œåŸç†ã€‚
  >    - è¯¥æ¨¡å‹ä¸»è¦ç”¨äºç¥ç»ç½‘ç»œå±‚æ¬¡ã€æ³¨æ„åŠ›åˆ†å¸ƒç­‰çš„å¯è§†åŒ–ï¼Œä¸ä¼šæ”¹å˜æ¨¡å‹çš„æœ¬è´¨ç»“æ„ï¼Œä½†å®ƒæœ‰åŠ©äºä½ ç†è§£æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ—¶çš„å†…éƒ¨è¡¨ç°ã€‚
  > 2. **`from transformers import BertModel`**ï¼š
  >    - è¿™æ˜¯ `transformers` åº“ä¸­æ ‡å‡†çš„ BERT æ¨¡å‹å®ç°ã€‚`transformers` æ˜¯ä¸€ä¸ªæµè¡Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº“ï¼Œç”± Hugging Face å¼€å‘ï¼Œæä¾›äº†è®¸å¤šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¦‚ BERTã€GPTã€T5 ç­‰ï¼‰ã€‚
  >    - è¯¥ `BertModel` å®ç°çš„æ˜¯ BERT æ¨¡å‹æœ¬èº«ï¼Œç”¨äºè¿›è¡Œå¸¸è§„çš„ NLP ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é—®ç­”ç­‰ï¼‰ã€‚å®ƒæ²¡æœ‰ä»»ä½•ä¸“é—¨çš„å¯è§†åŒ–åŠŸèƒ½ï¼Œä¸»è¦ç”¨äºæ¨ç†å’Œè®­ç»ƒã€‚

* ä¸ºä»€ä¹ˆåœ¨åš

  ```python
  emb_output[0] @ model.encoder.layer[0].attention.self.query.weight.T
  ```

  è¿™ä¸ªæ“ä½œæ—¶è¦è¿›è¡Œè½¬ç½®ï¼Ÿ
  å› ä¸º pytorch çš„ linear æ¨¡å—ï¼Œåœ¨è¿›è¡Œè®¡ç®—æ—¶æ˜¯è¿™æ ·çš„ï¼š $y = xA^{\top} + b$ ã€‚æˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—æ—¶æ˜¯åœ¨å¤ç° pytorch çš„è®¡ç®—

### model config and load

```python
model.config
```

```python
{
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}
```

æ³¨æ„åˆ°è¿™é‡Œçš„ `"hidden_size": 768`ï¼Œå…¶å®æ˜¯å¤šä¸ªå¤´â€œå åŠ â€å‡ºæ¥çš„ç»“æœã€‚æˆ‘ä»¬æ³¨æ„åˆ°è¿™é‡Œçš„ `"num_attention_heads": 12` ï¼Œé‚£ä¹ˆæ¯ä¸€å±‚æ¯ä¸ªå¤´çš„éšå±‚ç»´åº¦æ˜¯ 768 / 12 = 64ã€‚

æˆ‘ä»¬å¯ä»¥å–å‡ºæ¥è§‚å¯Ÿä¸€ä¸‹ï¼ˆä» `model.encoder` è¿™ä¸ªæ¨¡å—ä¸­æ…¢æ…¢åœ°å–å‡ºæ¥è§‚å¯Ÿä¸€ä¸‹ï¼Œ**å¥½å¥½ä½“ä¼šè¿™ä¸ªè¿‡ç¨‹**ï¼ï¼ï¼ï¼‰

è¦ä¼šè¯»æ‡‚è¿™ä¸ªæ¶æ„ï¼ˆgetï¼‰

```bash
BertEncoder(
  (layer): ModuleList(
    (0): BertLayer(
      (attention): BertAttention(
        (self): BertSelfAttention(
          (query): Linear(in_features=768, out_features=768, bias=True)
          (key): Linear(in_features=768, out_features=768, bias=True)
          (value): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (output): BertSelfOutput(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (intermediate): BertIntermediate(
        (dense): Linear(in_features=768, out_features=3072, bias=True)
      )
      (output): BertOutput(
        (dense): Linear(in_features=3072, out_features=768, bias=True)
        (LayerNorm): BertLayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
...
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
```



```python
model.encoder.layer[0].attention.self.query.weight.T[:, : 64]
```

```bash
tensor([[-0.0164, -0.0326,  0.0105,  ..., -0.0186, -0.0095,  0.0112],
        [ 0.0261,  0.0346,  0.0334,  ...,  0.0482, -0.0285, -0.0349],
        [-0.0263, -0.0423,  0.0109,  ..., -0.0724, -0.0453, -0.0304],
        ...,
        [ 0.0154, -0.0527, -0.0279,  ..., -0.0434,  0.0170,  0.0217],
        [ 0.0768,  0.1393,  0.0258,  ...,  0.0385,  0.0357, -0.0631],
        [ 0.0548,  0.0078, -0.0468,  ...,  0.0423, -0.0408,  0.0212]],
       grad_fn=<SliceBackward0>)
```

æˆ‘ä»¬è¿™æ ·å­ç´¢å¼•å…¶å®æ˜¯å–å‡ºäº†**ç¬¬ä¸€ä¸ªå¤´**çš„ `hidden_embedding`



### model output

```python
config = BertConfig.from_pretrained(model_name, output_attentions=True, 
                                    output_hidden_states=True, 
                                    return_dict=True,
                                    cache_dir = cache_dir)
config.max_position_embeddings = max_length

model = BertModel(config).from_pretrained(model_name, cache_dir = cache_dir)
```

æˆ‘ä»¬è¿›è¡Œè¿™æ ·çš„è®¾ç½® `output_hidden_states=True, return_dict=True` ä¹‹åï¼Œæˆ‘ä»¬çš„ `output` ä¼šè¿”å›ä¸‰ä¸ªå±‚ï¼Œå…¶ä¸­ç¬¬ä¸‰å±‚å’Œ `7_model_outputs` è¿™ä¸€èŠ‚ä¸­è®²å¾—å·®ä¸å¤šï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬è¿›è¡Œäº† `return_dict=True` è¿™ä¸ªæ“ä½œ

æ‰€ä»¥ `type(model_output[2]) == tuple` å¹¶ä¸” `type(model_output[2][0]) == dict`

å½“ç„¶ `model_output` çš„ç¬¬ä¸€å±‚ï¼ˆ0ï¼‰å’Œç¬¬äºŒå±‚ï¼ˆ1ï¼‰éƒ½æ˜¯ `tensor` ï¼Œå’Œä¹‹å‰ç¬¬ä¸ƒç« æ‰€è®²çš„å½¢å¼ä¸€æ ·

```python
print(model_output[2][-1].keys())	#  return_dict=True
```

```bash
dict_keys(['attn', 'queries', 'keys'])
```



### from scratch

æˆ‘ä»¬è€ƒè™‘è®¡ç®—ç¬¬ä¸€å±‚è¾“å‡ºå¾—åˆ°çš„ `attn` ç³»æ•°

```python
# model_output[-1][0]['attn'].shape			# (B, H, N, N)
model_output[-1][0]['attn'][0, 0, :, :]		# (N, N)
```



```bash
tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],
        [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],
        [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],
        ...,
        [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],
        [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],
        [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],
       grad_fn=<SliceBackward0>)
```





* my test

```python
import torch
import torch.nn.functional as F
# è¿™é‡Œæ³¨æ„å–ç¬¬ä¸€ä¸ªå¤´ [: 64]
Q_first_head_first_layer = torch.matmul(emb_output[0], model.encoder.layer[0].attention.self.query.weight.T[:, : 64]) + model.encoder.layer[0].attention.self.query.bias[: 64]
K_first_head_first_layer = torch.matmul(emb_output[0], model.encoder.layer[0].attention.self.key.weight.T[:, : 64]) + model.encoder.layer[0].attention.self.key.bias[: 64]
V_first_head_first_layer = torch.matmul(emb_output[0], model.encoder.layer[0].attention.self.value.weight.T[:, : 64]) + model.encoder.layer[0].attention.self.value.bias[: 64]

scores = torch.matmul(Q_first_head_first_layer, K_first_head_first_layer.T) / math.sqrt(64)
scores = F.softmax(scores, dim = -1)

print(scores)
print(torch.allclose(scores, model_output[-1][0]['attn'][0, 0, :, :]))		# (N, N)
```

```bash
tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],
        [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],
        [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],
        ...,
        [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],
        [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],
        [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],
       grad_fn=<SoftmaxBackward0>)
True
```



## 9_BertSelfLayer å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆmulti head attentionï¼‰çš„åˆ†å—çŸ©é˜µå®ç°

è¿™ä¸ªä¸œè¥¿æˆ‘åœ¨ä½¿ç”¨ dgl å®ç° GAT æ—¶å·²ç»ä»”ç»†åœ°ç†è§£è¿‡äº†ã€‚ç°åœ¨å†æ¥å¤ä¹ ä¸€ä¸‹ã€‚

### è¡¥å……è¯´æ˜

1. æˆ‘ä»¬è§‚å¯Ÿå¦‚ä¸‹å…¬å¼

   $W_q \in \mathbf{R}^{d_e \times d_q} \\W_k \in \mathbf{R}^{d_e \times d_k} \\W_v \in \mathbf{R}^{d_e \times d_v}$

   å…¶ä¸­ $d_e$ æ˜¯è¾“å…¥å±‚çš„ç‰¹å¾ç»´åº¦

   æ ¹æ® transformers çš„å…¬å¼ï¼Œæˆ‘ä»¬è¦æ±‚ $d_q = d_k$ï¼Œä½†æ˜¯å¯¹ $d_v$ çš„å¤§å°ä¸åšå…·ä½“è¦æ±‚ï¼Œä½†æ˜¯æˆ‘ä»¬æœ‰æ—¶å€™ä¸ºäº†å°†ç»´åº¦ç»Ÿä¸€è´¯ç©¿æ•´ä¸ªè¿‡ç¨‹ï¼Œä¸€èˆ¬è¿™ä¸‰ä¸ªç»´åº¦çš„å¤§å°æ˜¯ç›¸åŒçš„

2. è‡ªå·±æ‰‹å†™ä¸€é multi-layer çš„åˆ†å—çŸ©é˜µçš„å®ç°

   **éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤šå¤´çš„å®ç°æ˜¯ softmax ä½œç”¨åœ¨å•å¤´ä¸Šå†æ‹¼æ¥è€Œä¸æ˜¯å…ˆæ‹¼æ¥å† softmax**



## 10_add_norm_residual_conn

å’Œä¸Šé¢ä¸€æ ·ï¼Œå­¦ä¼šè‡ªå·±å®è·µä¸€ä¸‹

### ç–‘é—®

* `model.config` ä¸­çš„ `intermediate_size` çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•ç†è§£å®ƒçš„å¯¹é½ã€‚

  å°±æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œä¸­æ‹‰é«˜çš„ç»´åº¦ï¼Œåœ¨ bert-base-uncased ä¸­æ˜¯ 768 * 4 = 3072

* ç†è§£ `torch.no_grad()`  å’Œ `model.eval()` çš„åŒºåˆ«

  > ### `torch.no_grad()`
  >
  > - **ç›®çš„**ï¼šç¦ç”¨æ¢¯åº¦è®¡ç®—ã€‚
  > - **ä½œç”¨**ï¼šåœ¨è¯¥ä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹ï¼Œæ‰€æœ‰çš„å¼ é‡æ“ä½œéƒ½ä¸ä¼šè®¡ç®—æ¢¯åº¦ï¼Œä»è€ŒèŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å®ƒå¸¸ç”¨äº **æ¨ç†ï¼ˆinferenceï¼‰** é˜¶æ®µï¼Œå› ä¸ºåœ¨æ­¤é˜¶æ®µä½ ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
  > - **ä½¿ç”¨åœºæ™¯**ï¼šä¾‹å¦‚ï¼Œæ¨ç†æˆ–éªŒè¯æ¨¡å‹æ—¶ï¼Œå› ä¸ºä½ ä¸æ‰“ç®—è¿›è¡Œåå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°ã€‚
  >
  > ```python
  > è¾‘with torch.no_grad():
  >     output = model(input)  # æ¨ç†æ—¶ä¸è®¡ç®—æ¢¯åº¦
  > ```
  >
  > ### `model.eval()`
  >
  > - **ç›®çš„**ï¼šå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚
  > - **ä½œç”¨**ï¼šå®ƒä¸»è¦å½±å“æŸäº›ç‰¹å®šå±‚ï¼Œå¦‚ **BatchNorm** å’Œ **Dropout** å±‚ã€‚
  >   - **BatchNorm**ï¼šåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼ŒBatchNorm ä¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œåœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œå®ƒä¼šä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„å…¨å±€å‡å€¼å’Œæ–¹å·®ã€‚
  >   - **Dropout**ï¼šåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼ŒDropout ä¼šéšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼›è€Œåœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼ŒDropout ä¼šè¢«ç¦ç”¨ï¼Œå³æ¯ä¸ªç¥ç»å…ƒéƒ½ä¼šè¢«ä¿ç•™ï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹ã€‚
  >
  > ```python
  > model.eval()  # è¿›å…¥è¯„ä¼°æ¨¡å¼
  > output = model(input)  # è¯„ä¼°é˜¶æ®µ
  > ```

  



### BertLayerå±‚

ä»¥ `layer[0]` ä¸ºä¾‹ï¼ˆå…¶ä»–éƒ½æ˜¯ä¸€æ ·çš„ï¼‰

```bash
(layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
```

å’Œæœ€ç»å…¸çš„é‚£å¼ å›¾å¯¹åº”èµ·æ¥ï¼ˆä»”ç»†æ€è€ƒä¸€ä¸‹ï¼‰

- BertLayer
    - attention: BertAttention
        - self: BertSelfAttention
        - output: BertSelfOutput
    - intermediate: BertIntermediate, 768=>4*768
    - output: BertOutput, 4*768 => 768

### è‡ªå·±å¤ç°ä¸€ä¸‹

å·² get

æ³¨æ„ï¼šåœ¨ `mha_output = layer.attention.self(embeddings)` åœ¨è¿™ä¸€ç•Œé¢ï¼Œå³åœ¨ attention å±‚è®¡ç®—è‡ªæ³¨æ„åŠ›æœºåˆ¶è¾“å‡ºçš„ä¼šæ˜¯ä¸€ä¸ª `tuple` ï¼Œç¬¬ä¸€é¡¹ `mha_output[0]` å­˜å…¥çš„æ˜¯ Multi-Head Attention ã€‚ï¼ˆ`[B, seq_len, heads * heads_hidden]`ï¼‰



## 11_bert_head_pooler_output

### ç†è§£ pooler_outputï¼ˆæµç¨‹ï¼‰

* `pooler_output` æ˜¯æœ€ç»ˆçš„ä¸€ä¸ªè¾“å‡º
* è¾“å…¥çš„ä¸œè¥¿æ˜¯ä»€ä¹ˆï¼Ÿ
* è¾“å‡ºçš„ä¸œè¥¿æ˜¯ä»€ä¹ˆï¼Ÿ
* å…·ä½“æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ
  * çº¿æ€§å±‚ + æ¿€æ´»å±‚



### **forward and pooler output**

```python
bert.eval()
with torch.no_grad():
    output = bert(**inputs)
    # output.keys() odict_keys(['last_hidden_state', 'pooler_output'])
    print(output["pooler_output"].shape)
```

```bash
torch.Size([1, 768])	# (B, D)
```

**pool_output çš„åšæ³•æ˜¯é€‰æ‹©æ¯ä¸ªä¸€å¥å­ï¼ˆBï¼‰ä¸­çš„é¦–ä¸ª token çš„ embedding ï¼ˆin_D = Dï¼‰ï¼Œå°†å…¶é€å…¥ä¸€ä¸ªçº¿æ€§å±‚ + æ¿€æ´»å±‚ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªä»£è¡¨æ•´ä¸ªå¥å­çš„è¾“å‡ºï¼ˆB,  out_D = Dï¼‰**

è¿™æ ·æˆ‘ä»¬æ‰å¯ä»¥ä½¿ç”¨è¿™ä¸ªæœ€ç»ˆçš„è¾“å‡ºåšä¸€äº›å¥å­åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æçš„ä»»åŠ¡ï¼Œ**å°†è½ç‚¹è½åˆ°å¥å­ä¸Š**ã€‚

æˆ‘ä»¬å¯ä»¥è¿›è¡ŒéªŒè¯

### **from scratch**

```python
bert.pooler
```

```bash
BertPooler(
  (dense): Linear(in_features=768, out_features=768, bias=True)
  (activation): Tanh()
)
```

```python
my_output = bert.pooler.activation(bert.pooler.dense(output["last_hidden_state"][0, 0, :]))
print(my_output.shape)
torch.equal(my_output, output["pooler_output"][0])
```

```bash
torch.Size([768])
True
```

### bert Head

Bert è¿™ä¸ªæ¶æ„ï¼Œå®ƒçš„ embedding å’Œ encoder æ˜¯ä¸ä¼šå˜çš„ï¼Œè¿™ä¹Ÿæ˜¯å®ƒçš„ç²¾åæ‰€åœ¨ã€‚å®ƒåé¢è·Ÿäº†ä¸åŒçš„ headï¼Œæ„ä¸ºç€å®ƒæ¥äº†ä¸åŒçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬åªéœ€è¦æ”¹åé¢çš„ head å³å¯ã€‚

Bertmodel çš„é»˜è®¤ head æ˜¯ poolerã€‚

```bash
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
```





## 12_masked_lm

è¿™æ˜¯å¾ˆç»å…¸çš„è¯­è¨€æ¨¡å‹ï¼šmasked_language_modelï¼šæ©ç è¯­è¨€æ¨¡å‹

### å°èŠ‚

* ç†è§£ mlm ä»»åŠ¡ä¸­ BertOnlyMLMHead çš„ç»„ç»‡ç»“æ„å’Œå·¥ä½œæµç¨‹
  * decoder æ˜¯å°† 768 æ˜ å°„åˆ°è¯æ±‡è¡¨ 30522 ä¸Š
* å¯¹å¤šåˆ†ç±»ä»»åŠ¡è¿‡ç¨‹ï¼ˆä¸‹æ¸¸ä»»åŠ¡ï¼‰å’Œç½‘ç»œç»“æ„ï¼ˆ`mlm.cls`ï¼‰ã€æ„é€  `labels` ä¸ `input_ids` è¿‡ç¨‹ï¼ˆå³ `masking` è¿‡ç¨‹ï¼‰ã€è®¡ç®—æŸå¤±è¿‡ç¨‹ã€ç¿»è¯‘è¿‡ç¨‹æ›´åŠ æ¸…æ™°é€å½»äº†ã€‚

### mlm

```python
mlm = BertForMaskedLM.from_pretrained(model_name, output_hidden_states=True, cache_dir = cache_dir)
mlm
```

```bash
# å‰é¢çš„çœç•¥å’Œä¹‹å‰è€ç”Ÿå¸¸è°ˆçš„ embedding å’Œ encoder ä¸€æ ·ï¼Œåªçœ‹ head å±‚
(cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (transform_act_fn): GELUActivation()
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=768, out_features=30522, bias=True)
    )
  )
```

ä» `in_features` å’Œ `out_feats` å¯ä»¥çœ‹å‡ºï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¯¹åº”æ­¤è¡¨çš„å¤šåˆ†ç±»ä»»åŠ¡ã€‚

è§£ç å™¨å°±ç›¸å½“äºè¾“å‡ºå±‚äº†ï¼Œè¿™ä¸ªç½‘ç»œä¹Ÿå¾ˆå¥½ç†è§£ã€‚



### masking è¿‡ç¨‹

æ³¨æ„ï¼š

* å¼€å§‹ token ä¸ ç»“æŸ token ä¸è¦è®¾ç½®æˆæ©ç ï¼Œè¿™æ˜¯çº¦å®šä¿—ç§°çš„ã€‚



å­¦ä¼šç”Ÿæˆ `inputs["labels"]`

```python
# ä»¥å…¶ä¸­çš„ç¬¬ä¸€ä¸ªå¥å­ä¸ºä¾‹
inputs["labels"] = inputs["input_ids"].detach().clone()
mask = torch.rand(inputs["labels"].shape) < 0.15
mask_arr = (mask) * (inputs["labels"] != 101) * (inputs["labels"] != 102)	# ä¸èƒ½å°†å¼€å§‹å’Œç»“æŸç¬¦è®¾ç½®ä¸ºæ©ç 
selection = torch.flatten(mask_arr[0].nonzero()).tolist()	# å–ç¬¬ä¸€ä¸ªå¥å­
inputs["input_ids"][0][selection] = 103
inputs
```

```bash
{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,
          2006,   103,  3424,  1011,   103,  4132,  1010,  2019,  3988,  2698,
          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,   103,  2433,
          1996, 18179,  1012,  2162,  3631,   103,  1999,  2258,  6863,  2043,
         22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,  3792,
          1010,  2074,  2058,  1037,   103,  2044,  5367,   103,  1055, 17331,
           103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,
          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,
          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,
          1996, 18179,  1012,  2162,  3631,  2041,  1999,  2258,  6863,  2043,
         22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,  3792,
          1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055, 17331,
          1012,   102]])}
```



### forward and calcuate loss

```python
mlm.eval()
with torch.no_grad():
    output = mlm(**inputs)
output.keys()
```

```bash
odict_keys(['loss', 'logits', 'hidden_states'])
```

`output["logits"]` å…¶å®å°±æ˜¯è¿˜æ²¡æœ‰ç»è¿‡ softmax çš„æœ€ç»ˆè¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡ŒéªŒè¯



```python
tmp = mlm.cls(output["hidden_states"][-1])
torch.equal(tmp, output["logits"])
```

```bash
True
```



æˆ–è€…æˆ‘ä»¬å†ç»†åŒ–ä¸€ä¸‹

```python
mlm.eval()
with torch.no_grad():
    transformed = mlm.cls.predictions.transform(last_hidden_state)
    print(transformed.shape)
    logits = mlm.cls.predictions.decoder(transformed)
    print(logits.shape)
logits == output["logits"]
```

```bash
tensor([[[True, True, True,  ..., True, True, True],
         [True, True, True,  ..., True, True, True],
         [True, True, True,  ..., True, True, True],
         ...,
         [True, True, True,  ..., True, True, True],
         [True, True, True,  ..., True, True, True],
         [True, True, True,  ..., True, True, True]]])
```



### **loss and translate**

è¦å­¦ä¼šè¿™ä¸ªè¿‡ç¨‹ï¼ï¼

1. å¯¹äº¤å‰ç†µçš„ç†è§£åŠ æ·±äº†
2. ä¼šè®¡ç®— loss å’Œç¿»è¯‘äº†



å…·ä½“è¿‡ç¨‹å»çœ‹ `10_masked_lm.ipynb` è¿™ä¸ªæ–‡ä»¶
