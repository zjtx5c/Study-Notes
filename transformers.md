# Transformers

## 01_fine_tune_transformers_on_classification

è¿™ä¸€èŠ‚ç¡¬è´§å¤ªå¤šäº†ï¼Œå¾—æ²‰ä¸‹å¿ƒæ¥æ…¢æ…¢æ¶ˆåŒ–ã€‚

### è¦ç‚¹ä¸ç–‘é—®

* å¯è§†åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹åˆ†ææ–‡æœ¬é•¿åº¦ä¸ç±»åˆ«é¢‘ç‡
* å­¦ä¼šåœ¨ huggingface ä¸Šä¸‹è½½å’Œä¸Šä¼ 

* `AutoModel` å’Œ `AutoModelForSequenceClassification` çš„åŒºåˆ«

  | ç‰¹æ€§         | `AutoModel`                                      | `AutoModelForSequenceClassification`                         |
  | ------------ | ------------------------------------------------ | ------------------------------------------------------------ |
  | **ç›®æ ‡ä»»åŠ¡** | é€šç”¨çš„ç¼–ç å™¨æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼ˆæ— ä»»åŠ¡ç‰¹å®šå±‚ï¼‰ | ä¸“ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…å«åˆ†ç±»å¤´ï¼ˆå¦‚çº¿æ€§å±‚ï¼‰                   |
  | **æ¨¡å‹ç»“æ„** | åªæœ‰ç¼–ç å™¨éƒ¨åˆ†ï¼Œæ²¡æœ‰ä»»åŠ¡ç‰¹å®šçš„å¤´ï¼ˆå¦‚åˆ†ç±»å¤´ï¼‰     | åŒ…å«äº†åˆ†ç±»å¤´ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡               |
  | **ç”¨é€”**     | ç”¨äºè·å–ç¼–ç å™¨è¾“å‡ºæˆ–è¿›è¡Œè‡ªå®šä¹‰ä»»åŠ¡               | ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€æ–°é—»åˆ†ç±»ç­‰ï¼‰                   |
  | **è¾“å‡º**     | è¾“å‡ºçš„æ˜¯æ¨¡å‹çš„éšè—çŠ¶æ€æˆ–åµŒå…¥å‘é‡                 | è¾“å‡ºçš„æ˜¯åˆ†ç±»ç»“æœï¼ˆlogitsï¼Œç»è¿‡åˆ†ç±»å¤´çš„è¾“å‡ºï¼‰                 |
  | **åŠ è½½æ–¹å¼** | `AutoModel.from_pretrained('model_name')`        | `AutoModelForSequenceClassification.from_pretrained('model_name')` |

### **text classification**ï¼ˆæ–‡æœ¬åˆ†æï¼‰

- ä¹Ÿå« sequence classification
- sentiment analysis
    - æƒ…æ„Ÿåˆ†æï¼Œå°±æ˜¯ä¸€ç§æ–‡æœ¬/åºåˆ—åˆ†ç±»
        - ç”µå•†è¯„è®º
        - social webï¼šweibo/tweet

#### emotions æ•°æ®é›†

ç†è§£è·å– hugging face æ•°æ®é›†çš„å¸¸ç”¨ç»„ç»‡ç»“æ„

**Hugging Face `datasets`åº“**ä¸­**ç»å¤§å¤šæ•°æ•°æ®é›†**ï¼ˆåŒ…æ‹¬ä½ è‡ªå·±ç”¨ Python æ„å»ºçš„ `Dataset`ï¼‰éƒ½æ˜¯é‡‡ç”¨**è¿™ç§ç»“æ„ç»„ç»‡çš„**ï¼š

> åˆ’åˆ†è¯è®­ç»ƒé›†ã€éªŒè¯é›†ä¸æµ‹è¯•é›†ï¼ˆ8ï¼š1ï¼š1ï¼‰
>
> **ä¸€ç»„å…·æœ‰ç›¸åŒå­—æ®µï¼ˆfeaturesï¼‰å’Œç›¸åŒè¡Œæ•°çš„æ•°æ®åˆ—**ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªå­—æ®µï¼ˆä¾‹å¦‚ textã€labelï¼‰ã€‚
>
> ```bash
> DatasetDict({
>     train: Dataset({
>         features: ['text', 'label'],
>         num_rows: 16000
>     })
>     validation: Dataset({
>         features: ['text', 'label'],
>         num_rows: 2000
>     })
>     test: Dataset({
>         features: ['text', 'label'],
>         num_rows: 2000
>     })
> })
> ```
>



#### tokenize the whole dataset

å­¦ä¹ ä¸€ä¸‹è¿™é‡Œçš„ä¼˜é›…å†™æ³•ï¼šå†…å®¹åŒ…æ‹¬ `map()` å‡½æ•°ï¼ˆäº‹å®ä¸Šè¿™ä¸ªæ˜¯ `datasets` åº“ä¸­çš„ `Dataset.map()` æ–¹æ³•ï¼‰ + å¯¹è¿­ä»£å™¨çš„ç†è§£

* `map(function, iterable)`

  > `function`ï¼šä½ æƒ³å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨çš„å‡½æ•°
  >
  > `iterable`ï¼šå¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å…ƒç»„ç­‰ï¼‰

æˆ‘ä»¬è€ƒè™‘å¯¹ `emotions` æ•°æ®é›†ä¸­çš„ `train` `val` `test` æ•°æ®é›†éƒ½è¿›è¡Œ `tokenize`ï¼Œè€ƒè™‘ä¸€ä¸‹è¯¥æ€ä¹ˆå†™ï¼Ÿé¦–å…ˆéœ€è¦æ˜ç¡®çš„æ˜¯ï¼Œ`emotions` æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ã€‚é‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ `map` è¿™ä¸ªæ–¹æ³•äº†

äº‹å®ä¸Šï¼Œ**æ˜¯åœ¨ä½¿ç”¨ ğŸ¤— Hugging Face çš„ `datasets` åº“ ä¸­çš„ `Dataset.map()` æ–¹æ³•ï¼Œå¯¹ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›† `emotions` è¿›è¡Œæ‰¹å¤„ç†å¼çš„é¢„å¤„ç†æˆ– tokenizationã€‚**

```python
emotions
for e in emotions:
    print(e)
```

```bash
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 16000
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 2000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 2000
    })
})

train
validation
test
```



æˆ‘ä»¬ä½¿ç”¨ `datasets` åº“ä¸­çš„ `Dataset.map()` æ–¹æ³•ï¼Œè¿›è¡Œæ‰¹å¤„ç†å¼çš„é¢„å¤„ç†æˆ– `tokenization`

```python
def batch_tokenize(batch):
    return tokenizer(batch["text"], padding = True, truncation = True)
    
emotions_encoded = emotions.map(batch_tokenize, batched=True, batch_size = None)
emotions_encoded
```

```bash
DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 16000
    })
    validation: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 2000
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 2000
    })
})
```

> `batch_tokenize`
>
> - æ˜¯ä½ è‡ªå·±å®šä¹‰çš„å‡½æ•°ï¼Œæˆ–ç”¨ tokenizer åŒ…è£…è¿‡çš„å‡½æ•°ï¼›
> - ç”¨æ¥å¯¹ä¸€æ‰¹æ–‡æœ¬ï¼ˆå¦‚ `batch["text"]`ï¼‰è¿›è¡Œå¤„ç†ï¼Œä¾‹å¦‚ tokenizerï¼š
>
> ```python
> def batch_tokenize(batch):
>     return tokenizer(batch["text"], padding=True, truncation=True)
> ```
>
> `batched=True`
>
> - è¡¨ç¤º `batch_tokenize` æ˜¯**æ‰¹å¤„ç†å‡½æ•°**ï¼›
> - Hugging Face ä¼šæŠŠæ•°æ®åˆ†æˆå°æ‰¹æ¬¡ï¼ˆé»˜è®¤ 1000 æ¡ï¼‰ï¼Œç„¶åä¼ ç»™ä½ çš„å‡½æ•°ï¼›
> - æ¯æ¬¡ä¼ å…¥çš„ `batch` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¾‹å¦‚ï¼š
>
> ```python
> {
>   "text": [...1000 æ¡æ–‡æœ¬...],
>   "label": [...å¯¹åº”æ ‡ç­¾...]
> }
> ```
>
> `batch_size=None`
>
> - è¡¨ç¤ºä½¿ç”¨é»˜è®¤æ‰¹å¤§å°ï¼ˆå¤§çº¦ 1000 æ¡ï¼‰ï¼›
> - ä½ ä¹Ÿå¯ä»¥å†™ `batch_size=32` ä¹‹ç±»çš„ã€‚



å¦å¤–è¿˜æœ‰
```python
emotions_encoded.set_format('torch', columns=['label', 'input_ids', 'attention_mask'])
```

æ˜¯ Hugging Face `datasets` åº“ä¸­ `Dataset` å¯¹è±¡çš„æ–¹æ³•ï¼Œç”¨äºå°†æŒ‡å®šçš„åˆ—è½¬æ¢ä¸º **PyTorch å¼ é‡ï¼ˆtensorï¼‰æ ¼å¼**ï¼Œæ–¹ä¾¿ç”¨äºæ¨¡å‹è®­ç»ƒã€‚ï¼ˆ**åŸæ¥ä¸€èˆ¬æ˜¯ list ç±»å‹**ï¼‰

> æŠŠ `emotions_encoded` æ•°æ®é›†ä¸­çš„ `'label'`, `'input_ids'`, `'attention_mask'` ä¸‰åˆ—è½¬æ¢æˆ `torch.Tensor` ç±»å‹ï¼Œä»¥ä¾¿å’Œ PyTorch æ¨¡å‹å¯¹æ¥ã€‚
>
> æˆ‘ä»¬ä¸éœ€è¦å¯¹ `text` è¿›è¡Œ tensor ç±»å‹çš„è½¬æ¢ï¼Œå› ä¸ºå®ƒä¸æ˜¯æˆ‘ä»¬è®­ç»ƒçš„å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥è®¤ä¸ºå®ƒæ˜¯ç»™æˆ‘ä»¬çœ‹çš„è€Œä¸æ˜¯ç»™æœºå™¨çœ‹çš„ã€‚





### **fine-tune transformers**

#### distilbert-base-uncased

* `distilbert` æ˜¯å¯¹ `bert` çš„ `distill` è€Œæ¥
  * æ¨¡å‹ç»“æ„æ›´ä¸ºç®€å•ï¼Œ
  * `bert-base-uncased` å‚æ•°é‡ï¼š109482240
  * `distilbert-base-uncased` å‚æ•°é‡ï¼š66362880

å¯ä»¥æ¯”è¾ƒä¸€ä¸‹å®ƒç›¸è¾ƒäº Bertï¼Œå°‘äº†å“ªäº›ä¸œè¥¿ã€‚

æˆ‘ä»¬å¯¼å…¥ä¸€ä¸ªåšä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹

```python
from transformers import AutoModelForSequenceClassification
model_ckpt = 'distilbert-base-uncased'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# model = AutoModelForSequenceClassification.from_pretrained(model_ckpt)
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_classes).to(device)
model
```

è§‚å¯Ÿå…¶ `model` ç»“æ„

```bash
DistilBertForSequenceClassification(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0-5): 6 x TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=6, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
```

å¯ä»¥å‘ç°ï¼Œå‰é¢å°±æ˜¯ä¸€ä¸ª `distilbert-base-uncased`ï¼Œè€Œåé¢åˆ™æ˜¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„ `head`ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ã€‚å®ƒä¼šè¿”å›ä¸€ä¸ªæç¤º

> ```bash
> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']
> You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
> ```

`['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']` è¿™**ä¸‰ä¸ªä¸‹æ¸¸å±‚çš„æ¨¡å‹æ˜¯æ²¡æœ‰è®­ç»ƒè¿‡çš„ï¼ï¼** 

æˆ‘ä»¬éœ€è¦ `fine tune`



#### triner

> `transformers.Trainer` æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ª**é€šç”¨è®­ç»ƒå¾ªç¯å°è£…å™¨**ï¼Œå†…éƒ¨é›†æˆäº†ï¼š
>
> - æ•°æ®åŠ è½½ï¼ˆDataLoaderï¼‰
> - å‰å‘ä¼ æ’­ & æŸå¤±è®¡ç®—
> - åå‘ä¼ æ’­ & æ¢¯åº¦è£å‰ª
> - Optimizer & Scheduler æ›´æ–°
> - æ¨¡å‹ä¿å­˜ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•
> - æ”¯æŒå¤š GPU / æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰ç­‰åŠŸèƒ½



ä½¿ç”¨ç¤ºä¾‹

```python
from transformers import TrainingArguments, Trainer
batch_size = 16  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒ/éªŒè¯ batch å¤§å°è®¾ä¸º 16
logging_steps = len(emotions_encoded['train']) // batch_size  # æ¯ä¸ª epoch æ—¥å¿—è®°å½•ä¸€æ¬¡

model_name = cache_dir_model + f'{model_ckpt}_emotion_ft_0416'  # æ¨¡å‹è¾“å‡ºè·¯å¾„ï¼Œå‘½ååŒ…å«æ¨¡å‹åå’Œæ—¥æœŸæ ‡è¯†

training_args = TrainingArguments(
    output_dir=model_name,                          # æ¨¡å‹ä¿å­˜ç›®å½•
    num_train_epochs=4,                             # æ€»è®­ç»ƒè½®æ•°ä¸º 4
    learning_rate=2e-5,                             # åˆå§‹å­¦ä¹ ç‡è®¾ä¸º 2e-5ï¼ˆå¸¸ç”¨äº BERT å¾®è°ƒï¼‰
    weight_decay=0.01,                              # æƒé‡è¡°å‡ç³»æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    per_device_train_batch_size=batch_size,         # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒ batch å¤§å°
    per_device_eval_batch_size=batch_size,          # æ¯ä¸ªè®¾å¤‡çš„éªŒè¯ batch å¤§å°
    evaluation_strategy="epoch",                    # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œä¸€æ¬¡è¯„ä¼°
    disable_tqdm=False,                             # æ˜¾ç¤º tqdm è¿›åº¦æ¡
    logging_steps=logging_steps,                    # æ¯ä¸ª epoch æ—¥å¿—è¾“å‡ºä¸€æ¬¡
    push_to_hub=True,                               # è®­ç»ƒå®Œè‡ªåŠ¨å°†æ¨¡å‹æ¨é€åˆ° Hugging Face Hubï¼ˆéœ€ç™»å½•ï¼‰
    log_level="error"                               # æ—¥å¿—ç­‰çº§è®¾ç½®ä¸º errorï¼Œä»…è¾“å‡ºé”™è¯¯ä¿¡æ¯
)

```



- traineré»˜è®¤è‡ªåŠ¨å¼€å¯ torch çš„å¤šgpuæ¨¡å¼ï¼Œ
    - `per_device_train_batch_size`: è¿™é‡Œæ˜¯è®¾ç½®æ¯ä¸ªgpuä¸Šçš„æ ·æœ¬æ•°é‡ï¼Œ
    - ä¸€èˆ¬æ¥è¯´ï¼Œå¤šgpuæ¨¡å¼å¸Œæœ›å¤šä¸ªgpuçš„æ€§èƒ½å°½é‡æ¥è¿‘ï¼Œå¦åˆ™æœ€ç»ˆå¤šgpuçš„é€Ÿåº¦ç”±æœ€æ…¢çš„gpuå†³å®šï¼Œ
        - æ¯”å¦‚å¿«gpu è·‘ä¸€ä¸ªbatchéœ€è¦5ç§’ï¼Œè·‘10ä¸ªbatch 50ç§’ï¼Œæ…¢çš„gpuè·‘ä¸€ä¸ªbatch 500ç§’ï¼Œåˆ™å¿«gpuè¿˜è¦ç­‰æ…¢gpuè·‘å®Œä¸€ä¸ªbatchç„¶åä¸€èµ·æ›´æ–°weightsï¼Œé€Ÿåº¦åè€Œæ›´æ…¢äº†ã€‚
    - åŒç† `per_device_eval_batch_size` ç±»ä¼¼
- `learning_rate`/`weight_decay`
    - é»˜è®¤ä½¿ç”¨ AdamW çš„ä¼˜åŒ–ç®—æ³•



æˆ‘ä»¬è®¾ç½®å¥½äº† `training_args ` å‚æ•°ç®¡ç†å™¨ä¹‹åï¼Œç›´æ¥è®¾ç½®è®­ç»ƒå™¨ã€‚éœ€è¦è®°ä½çš„æ˜¯æˆ‘ä»¬éœ€è¦ä¼ å…¥ä»¥ä¸‹ä¸œè¥¿

1. æ¨¡å‹`model`
2. åˆ†è¯å™¨ `tokenizer`
3. æ•°æ® `train_dataset` ä¸ `eval_dataset`
4. è¶…å‚æ•° `args = training_args`
5. æŒ‡å®šç”¨äºåœ¨éªŒè¯é›†ä¸Šè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•° `compute_metrics`

ä¾‹å¦‚ï¼š

```python
from transformers_utils import compute_classification_metrics	# è¿™ä¸ªæ˜¯è‡ªå®šä¹‰çš„ä¸€ä¸ª å·¥å…·åŒ…
trainer = Trainer(model=model, 
                  tokenizer=tokenizer,
                  train_dataset=emotions_encoded['train'],
                  eval_dataset=emotions_encoded['validation'],
                  args=training_args, 
                  compute_metrics=compute_classification_metrics)
```

æˆ‘ä»¬é‡ç‚¹ç†è§£ä¸€ä¸‹è¿™ä¸ª `compute_metrics`ï¼Œä¸€èˆ¬æ ¼å¼å¦‚ä¸‹ï¼š

```python
def compute_classification_metrics(pred):
    preds = np.argmax(pred.predictions, axis=1)
    labels = pred.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted"),
        ...
    }

```

`trainer` ä¼šåœ¨æ¯æ¬¡ `evaluate()` æˆ–æ¯ä¸ª `epoch` åï¼Œ**è°ƒç”¨è¿™ä¸ªå‡½æ•°æ¥æ‰“å°/è®°å½•æŒ‡æ ‡**

Hugging Face ä¸­ `compute_metrics(pred)` çš„å‚æ•° `pred` æ˜¯ä¸€ä¸ª `EvalPrediction` å¯¹è±¡ï¼Œå…¶æœ¬è´¨ç»“æ„å¦‚ä¸‹ï¼š

```python
EvalPrediction = namedtuple("EvalPrediction", ["predictions", "label_ids"])
```

è€Œ `trainer.predict()` è¿”å›çš„ `PredictionOutput` ç»“æ„å¦‚ä¸‹ï¼š

```python
PredictionOutput = namedtuple("PredictionOutput", ["predictions", "label_ids", "metrics"])
```

ä½ ä¼šå‘ç°å®ƒä»¬ç»“æ„æ˜¯é«˜åº¦é‡åˆçš„ï¼Œå·®åˆ«åªåœ¨äºï¼š

| å­—æ®µ        | EvalPredictionï¼ˆè¯„ä¼°ç”¨ï¼‰  | PredictionOutputï¼ˆé¢„æµ‹ç”¨ï¼‰ |
| ----------- | ------------------------- | -------------------------- |
| predictions | âœ…                         | âœ…                          |
| label_ids   | âœ…                         | âœ…                          |
| metrics     | âŒï¼ˆcompute_metrics è¾“å‡ºï¼‰ | **âœ…ï¼ˆè°ƒç”¨åè¿”å›ï¼‰**        |

==**è¿™è¯´æ˜ï¼š`PredictionOutput` å…¶å®æ˜¯ `EvalPrediction` + `compute_metrics` çš„è¾“å‡ºç»“æœã€‚**==

å¯ç®—æ˜¯ç†è§£äº†



##### Hugging Face çš„ Trainerï¼Œåœ¨æ¯æ¬¡è®­ç»ƒï¼ˆ`.train()`ï¼‰ã€è¯„ä¼°ï¼ˆ`.evaluate()`ï¼‰ æˆ–é¢„æµ‹ï¼ˆ`.predict()`ï¼‰ ä¹‹åä¼šè¿”å›ä»€ä¹ˆæ•°æ®ç±»å‹

> âœ… `trainer.train()` çš„è¿”å›å€¼
>
> ```python
> train_output = trainer.train()
> ```
>
> è¿”å›å€¼æ˜¯ä¸€ä¸ª **`transformers.TrainOutput` å¯¹è±¡**ï¼Œä½ å¯ä»¥å½“æˆä¸€ä¸ª dict-like ç»“æ„çœ‹å¾…ï¼Œå®ƒåŒ…å«ï¼š
>
> ```python
> TrainOutput(
>     global_step=1234,
>     training_loss=0.4567,
>     metrics={
>         'train_runtime': 342.45,
>         'train_samples_per_second': 34.5,
>         'train_steps_per_second': 3.6,
>         'total_flos': 123456789.0,
>         'train_loss': 0.4567,
>         ...
>     }
> )
> ```
>
> ä½ å¯ä»¥é€šè¿‡ï¼š
>
> ```python
> train_output.metrics["train_loss"]  # æˆ–
> train_output.global_step
> ```
>
> è®¿é—®è¿™äº›å†…å®¹ã€‚
>
> 
>
>  âœ… `trainer.evaluate()` çš„è¿”å›å€¼
>
> ```python
> eval_metrics = trainer.evaluate()
> ```
>
> è¿”å›çš„æ˜¯ä¸€ä¸ª **æ ‡å‡† Python å­—å…¸ `dict`**ï¼Œå†…å®¹ç”±ä½ æä¾›çš„ `compute_metrics` å†³å®šï¼Œå¯èƒ½åŒ…æ‹¬ï¼š
>
> ```python
> {
>     'eval_loss': 0.2235,
>     'eval_accuracy': 0.897,
>     'eval_f1': 0.89,
>     'eval_runtime': 45.1,
>     'eval_samples_per_second': 100.2,
>     'epoch': 1.0
> }
> ```
>
> è¯´æ˜ï¼š
>
> - `eval_loss` æ˜¯äº¤å‰ç†µæˆ–ä½ å®šä¹‰çš„ loss
> - å…¶ä»– `eval_*` é¡¹æ¥è‡ªä½ çš„ `compute_metrics` å‡½æ•°è¿”å›
> - `eval_runtime` ç­‰ä¸º Trainer è‡ªåŠ¨è®°å½•çš„æ•ˆç‡æŒ‡æ ‡
>
> 
>
> âœ… `trainer.predict()` çš„è¿”å›å€¼
>
> ```python
> predictions = trainer.predict(test_dataset)
> ```
>
> è¿”å›çš„æ˜¯ä¸€ä¸ª **`PredictionOutput` å‘½åå…ƒç»„**ï¼Œç»“æ„å¦‚ä¸‹ï¼šï¼ˆæ³¨æ„å’Œä¸Šæ–‡ç»“åˆèµ·æ¥ç†è§£ï¼‰
>
> ```python
> PredictionOutput(
>     predictions=array([...]),     # é€šå¸¸æ˜¯ logits
>     label_ids=array([...]),       # çœŸå®æ ‡ç­¾
>     metrics={'test_accuracy': 0.88, 'test_f1': 0.86, ...}
> )
> ```
>
> ä½ å¯ä»¥è®¿é—®ï¼š
>
> ```python
> predictions.predictions   # æ¨¡å‹è¾“å‡ºçš„ logits
> predictions.label_ids     # åŸå§‹æ ‡ç­¾
> predictions.metrics       # compute_metrics è¿”å›çš„å­—å…¸
> ```
>
> è¿™é‡Œçš„ç»“æ„å’Œä¼ å…¥çš„ `compute_metric` è®¡ç®—é€»è¾‘å·®ä¸å¤š
>
> âœ… æ€»ç»“å¯¹æ¯”è¡¨ï¼š
>
> | å‡½æ•°                 | è¿”å›ç±»å‹                         | ä¸»è¦å†…å®¹                            |
> | -------------------- | -------------------------------- | ----------------------------------- |
> | `trainer.train()`    | `TrainOutput`                    | global_step, training_loss, metrics |
> | `trainer.evaluate()` | `dict`                           | loss + è¯„ä¼°æŒ‡æ ‡                     |
> | `trainer.predict()`  | `PredictionOutput`ï¼ˆnamedtupleï¼‰ | predictions + labels + metrics      |



##### å¯¹éªŒè¯é›†è¿›è¡Œæ‰¹é‡å‰å‘ä¼ æ’­

å­¦ä¼šåœ¨ hugging-face ä¸­ä½¿ç”¨æ‰¹é‡è®­ç»ƒï¼Œæˆ‘ä»¬ä¸¾ä¸€ä¸ªä¾‹å­

```python
from torch.nn.functional import cross_entropy
def forward_pass_with_label(batch):
    # å°†æ‰€æœ‰çš„å’Œè®­ç»ƒæœ‰å…³çš„è¾“å…¥è½¬æ¢æˆ tensorï¼Œæ¯”å¦‚ input_ids
    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_name}

    with torch.no_grad():
        output = model(**input) # ä¼šè¿”å› logits å’Œ loss ä¹‹ç±»
        pred_label = torch.argmax(output.logits, dim = -1)
        loss = cross_entropy(output.logits, batch["label"].to(device),
                             reduction = "none")
    
    return {"loss": loss.cpu().numpy(),
            "predicated_label": pred_label.cpu().numpy()}
    
    
    
emotions_encoded["validation"] = emotions_encoded["validation"].map(
    forward_pass_with_label, batched=True, batch_size=16)
emotions_encoded['validation']

```

```bash
Dataset({
    features: ['text', 'label', 'input_ids', 'attention_mask', 'loss', 'predicted_label'],
    num_rows: 2000
})
```



#### to huggingface hub

ä¼ åˆ° hugging-face ä¸Š



## 02_transformer_architecture_self_attention

* [é“¾æ¥](https://www.bilibili.com/video/BV14s4y127kk?spm_id_from=333.788.videopod.sections&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

åŒºåˆ† self-attention ä¸­ä»€ä¹ˆæ˜¯ attention-weights ï¼Œä»€ä¹ˆæ˜¯ attention-scores

q, k, v çš„æ¥æºå¯ä»¥è®¤ä¸ºæ˜¯ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ

å…¶ä»–çš„ç›´æ¥çœ‹ book å°±è¡Œäº†



* è®¡ç®— `scaled_dot_product_attention` çš„åšæ³•ï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œæ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒæœºåˆ¶ä¹‹ä¸€ï¼‰

  ```python
  # batch_size, seq_len, hidden_size
  def scaled_dot_product_attention(query, key, value):
      # hidden_size
      dim_k = key.size(-1)
      # batch_size, seq_len, seq_len
      attn_scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)
      attn_weights = F.softmax(attn_scores, dim=-1)
      return torch.bmm(attn_weights, value)
  ```



### summary

- attention mechanism
- encoder vs. decoder
    - seq2seq
        - seq of tokens (input) => seq of tokens (output)
        - æ³¨æ„ï¼Œè¿™ä¸¤ä¸ª tokens æ˜¯ä¸ä¸€æ ·çš„ï¼Œä¸€ä¸ªé’ˆå¯¹ input ä¸€ä¸ªé’ˆå¯¹ output
    - tasks
        - machine translation
    - encoder
        - seq of tokens => seq of embedding vectors(hidden state/context)
    - decoder ï¼ˆ**è¿™ä¸ªåç»­å†çœ‹ï¼Œç°åœ¨è¿˜ä¸æ˜¯å¾ˆæ‡‚**ï¼‰
        - encoder's hidden state => seq of tokens
            - iteratively generateï¼ˆè¿­ä»£å¼çš„ generateï¼‰
                - until EOS (end of seq) or reach max length limit
                - one token at a timeï¼ˆæ¯æ¬¡åªç”Ÿæˆä¸€ä¸ª tokenï¼‰



### encoder - decoder

- encoder only: seq of text => rich representation (bidirectional attention)
    - taskï¼ˆè¾¹ç•Œè¶Šæ¥è¶Šæ¨¡ç³Šäº†ï¼Œç›®å‰åŸºæœ¬ GPT Bert T5 è¿™äº›éƒ½èƒ½åšï¼‰
        - text classification
        - NER
    - models
        - BERT
        - RoBERTa
        - DistilBERT
    - å®Œå½¢å¡«ç©ºï¼ˆbidirectionalï¼‰åŸºäºä¸Šä¸‹æ–‡å»è¾“å‡ºå½“å‰çš„ä¸€ä¸ªè¯
        - representation of a given token depends both on
            - left (before the token)
            - right (after the token)
- decoder only (causal or autoregressive attention)è‡ªå›å½’ï¼Œå½“å‰çš„è¾“å‡ºä¾èµ–äºè¿‡å»çš„è¾“å‡º
    - gpt
    - è¯è¯­æ¥é¾™
        - representation of a given token depends only on the left contextï¼›ï¼ˆåªä¾èµ–äºä¹‹å‰çš„ï¼‰
- encoder-decoder both
    - tasks
        - machine translation
        - summarization
    - models
        - t5
        - bart



### encoder

- encoder layer: encoding the contextual information (conv) (å¯ä»¥ç†è§£ä¸º cv ä¸­çš„å·ç§¯)
    - input: seq of embeddings
        - multi-head self attention
        - ffn(fc)
    - output: 
        - same shape as `input`
    - contextual information (contextualized embeddings )ï¼ˆè¯­å¢ƒåŒ–åµŒå…¥ï¼‰
        - apple: company-like or fruit-like ?
           - keynote/phone/Jobs
           - banana, food, fruit
        - flies
           - time flies like an arrowï¼šsoars
           - fruit flies like a bananaï¼šinsect
- skip connection (residual connection) & layer normalization
    - é«˜æ•ˆè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æŠ€å·§ï¼›



### self attention

- each token (embedding) (åœ¨ç»è¿‡ self attention ä¹‹å)
    - ä¸æ˜¯ fixed embeddings 
    - è€Œæ˜¯ weighted average of each embedding of the whole input sequenceï¼ˆ**å’Œä¸Šä¸‹æ–‡è¯­å¢ƒæœ‰å…³çš„åµŒå…¥äº†ï¼ˆåŠ æƒï¼‰ï¼Œä¸å†æ˜¯ä¸€ä¸ªå›ºå®šçš„ embedding äº†**ï¼‰
- a seq of token embeddingï¼š$x_1, x_2, \cdots, x_n$ï¼Œç»è¿‡ self attention å¾—åˆ° a seq of updated embeddingsï¼Œ$x'_1, x'_2, \cdots, x'_n$

$$
x'_i=\sum_{j=1}^n w_{ji}x_j
$$

- $w_{ji}$
    - attention weights, 
    - $\sum_{j}w_{ji}=1$
    - $w\in\mathcal R^{n\times n}$ï¼Œæ–¹é˜µï¼Œ$seq_{len} \times seq_{len}$



**è®­ç»ƒå¥½çš„åµŒå…¥æ˜¯å…·æœ‰ä¸Šä¸‹æ–‡è¯­å¢ƒçš„åµŒå…¥ï¼Œå…·ä½“æ¥è¯´æ˜¯ä¸Šä¸‹æ–‡å…¶ä»–è¯åµŒå…¥çš„åŠ æƒå’Œï¼Œè¿™ä¸ªæƒå°±æ˜¯æ³¨æ„åŠ›æƒé‡**

- Project each token embedding into three vectors called query, key, and value.
    - W_q, W_k, W_vï¼šlearnable parameters
- Compute attention **scores**. 
    - dot-product(**query, key**) => attention scores;
    - a sequence with $n$ input tokens there is a corresponding $\mathcal R^{n\times n}$ matrix of attention scores.
- Compute attention weightsï¼ˆ$w_{j,i}$ï¼‰ from attention scores
    - Dot products çš„ç»“æœå¯èƒ½æ˜¯ä»»æ„å¤§çš„æ•°ï¼Œä¼šè®©æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹éå¸¸ä¸ç¨³å®š
    - å°† attention scores ä¹˜ä»¥ä¸€ä¸ª scaling factor 
    - softmax å½’ä¸€åŒ–ï¼š$\sum_{j}w_{ji}=1$
- update the final embedding of the token (**value**)
    - $x'_i=\sum_{j=1}^n w_{ji}v_j$
