# transfroms

## å®‰è£…é—®é¢˜

æˆ‘ä»¬éœ€è¦ä½¿ç”¨ hugging faceï¼Œä½†æ˜¯è¢«å¢™ï¼Œå³ä¾¿ç§‘å­¦ä¸Šç½‘ä¹Ÿæ— æ³•å®‰è£…æ¨¡å‹ã€‚å…·ä½“è§£å†³åŠæ³•å¯ä»¥æŸ¥çœ‹[é•œåƒç½‘ç«™](https://hf-mirror.com/)çš„æ•™ç¨‹ã€‚æˆ‘å·²ç»è®¾ç½®ä¸ºå…¨å±€é•œåƒäº†ï¼Œä½†å¥½åƒæ²¡æœ‰ã€‚

~~æ„Ÿè§‰åªæœ‰æ‰‹åŠ¨ä¸‹è½½äº†~~

ç»ˆäºè¡Œäº†ï¼

è§£å†³æ–¹æ³•æ˜¯åœ¨å¯¼å…¥ç›¸å…³çš„ `transformers` **ä¹‹å‰**åŠ ä¸Šè¿™ä¸¤å¥ä»£ç å³å¯è§£å†³é—®é¢˜

```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

ä»¥ä¸‹æ˜¯åŸå› è§£é‡Šï¼š

> è¿™æ˜¯å› ä¸º Hugging Face çš„ `transformers` åº“åœ¨**é¦–æ¬¡å¯¼å…¥æ—¶**ï¼Œå°±ä¼šè¯»å–ç¯å¢ƒå˜é‡æ¥ç¡®å®šä½ ä½¿ç”¨çš„é•œåƒæˆ–è®¿é—®åœ°å€ã€‚
>
> ğŸ” è§£é‡ŠåŸå› ï¼š
>
> å½“ä½ æ‰§è¡Œï¼š
>
> ```python
> from transformers import AutoTokenizer, AutoModel
> ```
>
> `transformers` åº“ä¼šç«‹åˆ»ï¼š
>
> - æ£€æŸ¥ `HF_ENDPOINT` ç¯å¢ƒå˜é‡ã€‚
> - å¦‚æœä½ é…ç½®äº†ï¼Œå®ƒå°±ä¼šä»æŒ‡å®šçš„é•œåƒåœ°å€ï¼ˆå¦‚ `https://hf-mirror.com`ï¼‰ä¸‹è½½æ¨¡å‹æˆ–æ•°æ®ã€‚
> - å¦‚æœæ²¡æœ‰é…ç½®ï¼Œå®ƒé»˜è®¤ä½¿ç”¨å®˜æ–¹åœ°å€ï¼ˆ`https://huggingface.co`ï¼‰ï¼Œä¸­å›½å¤§é™†ç”¨æˆ·å¾€å¾€è®¿é—®å›°éš¾æˆ–è¶…æ—¶ã€‚
>
> âœ… æ‰€ä»¥æ­£ç¡®çš„é¡ºåºæ˜¯ï¼š
>
> å¿…é¡»åœ¨å¯¼å…¥ `transformers` **ä¹‹å‰**è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæ¯”å¦‚è¿™æ ·ï¼š
>
> ```
> pythonå¤åˆ¶ç¼–è¾‘import os
> os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
> 
> from transformers import AutoTokenizer, AutoModel  # å¿…é¡»æ”¾åœ¨åé¢
> ```
>
> å¦åˆ™ï¼Œ`transformers` æ¨¡å—åŠ è½½æ—¶å·²ç»é”™è¿‡äº†è¯»å–ç¯å¢ƒå˜é‡çš„æœºä¼šï¼Œé…ç½®å°±ä¸ä¼šç”Ÿæ•ˆäº†ã€‚



## ç®€å•å…¥é—¨

### ä¸‹è½½å¥½çš„æ¨¡å‹æ–‡ä»¶æœ‰å“ªäº›

æˆ‘ä»¬ä»¥ `uer/gpt2-chinese-cluecorpussmall` è¿™ä¸ªæ¨¡å‹ä¸ºä¾‹ï¼Œè§‚å¯Ÿä»¥ä¸‹å…¶ä¸‹è½½å®Œæˆä¹‹åéƒ½æœ‰å“ªäº›ä¸œè¥¿ã€‚

åœ¨å¿«ç…§ `snapshots` æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€äº›æ–‡ä»¶

1. `config.json`

   ```json
   {
     "activation_function": "gelu_new",
     "architectures": [
       "GPT2LMHeadModel"
     ],
     "attn_pdrop": 0.1,
     "embd_pdrop": 0.1,
     "gradient_checkpointing": false,
     "initializer_range": 0.02,
     "layer_norm_epsilon": 1e-05,
     "model_type": "gpt2",
     "n_ctx": 1024,
     "n_embd": 768,
     "n_head": 12,
     "n_inner": null,
     "n_layer": 12,
     "n_positions": 1024,
     "output_past": true,
     "resid_pdrop": 0.1,
     "task_specific_params": {
       "text-generation": {
         "do_sample": true,
         "max_length": 320
       }
     },
     "tokenizer_class": "BertTokenizer",
     "vocab_size": 21128
   }
   ```

   è¿™æ˜¯ä¸€ä¸ª**è‡ªå®šä¹‰ GPT2 æ¨¡å‹é…ç½®**ï¼Œç‰¹ç‚¹å¦‚ä¸‹ï¼š

   - æ¨¡å‹ç»“æ„å’Œå±‚æ•°åŸºæœ¬å’Œ GPT2-base ä¿æŒä¸€è‡´ï¼ˆ12å±‚ï¼Œ768ç»´ï¼Œ12å¤´ï¼‰ï¼›
   - ä½¿ç”¨äº† **BertTokenizer** å’Œ **ä¸­æ–‡è¯è¡¨å¤§å°ï¼ˆ21128ï¼‰** â†’ è¯´æ˜å¯èƒ½æ˜¯ **ç”¨äºä¸­æ–‡çš„ GPT2**ï¼›
   - å¯ç”¨äº†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„å…¸å‹è®¾ç½®ï¼ˆé‡‡æ ·ï¼Œ320 é•¿åº¦ï¼‰ï¼›
   - å…³é—­äº† gradient checkpointingï¼›
   - é…ç½®åˆç†ï¼Œé€‚ç”¨äº Hugging Face çš„ `from_pretrained()` æˆ– `from_config()` åŠ è½½æ¨¡å‹ã€‚

   æˆ‘ä»¬å¯ä»¥é‡ç‚¹å…³æ³¨ä¸€ä¸‹ä¸‹è¿°è¿™ä¸¤ä¸ªå‚æ•°

   >   "tokenizer_class": "BertTokenizer", è¡¨æ˜ä½¿ç”¨äº† BERT åˆ†è¯å™¨
   >   "vocab_size": 21128	è¯è¡¨å¤§å°ä¸º21128

2. vocab.txt

   é‡Œé¢è®°è½½äº†è¯¥æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„æ‰€æœ‰å­—ï¼ˆå…±21120ä¸ªï¼‰

### å¦‚ä½•ç†è§£ `pipeline`

åœ¨ Hugging Face çš„ `transformers` åº“ä¸­ï¼Œ`pipeline` æ˜¯ä¸€ä¸ª**é«˜çº§å°è£…å·¥å…·**ï¼Œå®ƒå¯ä»¥**å¿«é€Ÿè°ƒç”¨å„ç§é¢„è®­ç»ƒæ¨¡å‹å®Œæˆå¸¸è§çš„ NLP ä»»åŠ¡**ï¼Œä¸ç”¨æ‰‹åŠ¨å¤„ç† tokenizerã€æ¨¡å‹åŠ è½½ã€å‰å¤„ç†ã€åå¤„ç†ç­‰ç¹çç»†èŠ‚ã€‚

> `pipeline` æ˜¯ Hugging Face æä¾›çš„â€œå‚»ç“œå¼ä¸€é”®æ¨ç†æ¥å£â€ï¼Œ**è¾“å…¥ä¸€å¥è¯ï¼Œè¿”å›ç»“æœ**ï¼Œæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç­‰ä»»åŠ¡ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€æ˜“çš„åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•çš„åœºæ™¯ä»£ç 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline

cache_dir = r"D:\data\model\models--uer--gpt2-chinese-cluecorpussmall\snapshots\c2c0249d8a2731f269414cc3b22dff021f8e07a3"

model = AutoModelForCausalLM.from_pretrained(cache_dir)
tokenizer = AutoTokenizer.from_pretrained(cache_dir)


from transformers import pipeline

generator = pipeline(
    task = "text-generation",  # æ³¨æ„æ˜¯ "text-generation"ï¼Œä¸æ˜¯ "text-generate"
    model = model,
    tokenizer = tokenizer,
    device = 0  # device=0 è¡¨ç¤ºä½¿ç”¨ç¬¬0å· GPUï¼›å¦‚æœç”¨ CPUï¼Œå¯ä»¥çœç•¥è¿™ä¸ªå‚æ•°
)

output = generator(
    "ä»Šå¤©å¤©æ°”çœŸå¥½å•Š",  # è¾“å…¥å­—ç¬¦ä¸²ç›´æ¥å†™ï¼Œä¸éœ€è¦ `inputs=`
    max_length = 50,
    num_return_sequences = 1
)

print(output)

```

åœ¨ä¸åŠ è°ƒå‚çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆçš„ç»“æœæ˜¯æ¯”è¾ƒç³Ÿç³•çš„ã€‚

* `AutoModelForCausalLM`æ˜¯ Hugging Face Transformers åº“ä¸­çš„ä¸€ä¸ª**è‡ªåŠ¨æ¨¡å‹åŠ è½½å™¨ç±»**ï¼Œç”¨äºåŠ è½½æ”¯æŒ **Causal Language Modelingï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼‰** çš„æ¨¡å‹

  `AutoModelForCausalLM` ä¼šæ ¹æ®ä½ æä¾›çš„æ¨¡å‹é…ç½®ï¼ˆ`config.json`ï¼‰ï¼Œ**è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„**ï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Œ**ç”¨äºæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡**ã€‚ `AutoTkenizer` åŒç†ï¼Œä¹Ÿæ˜¯æ ¹æ®æ¨¡å‹é…ç½® `config.json` æ¥è‡ªåŠ¨å¤„ç†







### å…¶ä»–æ³¨æ„äº‹é¡¹

* æ¨¡å‹åŠ è½½ä¸€èˆ¬ä½¿ç”¨ `.from_pretrained()` å‡½æ•°ï¼Œä¸”éœ€è¦ä½¿ç”¨**ç»å¯¹è·¯å¾„**ï¼Œä¸ç„¶è¿˜ä¼šå» hugging face ä¸­å»ä¸‹è½½

  ç›®å½•æ˜¯åŒ…å« `config.json` çš„ç›®å½•



## tokenizer_sentiment_analysis

###  é‡ç‚¹ç†è§£ tokenizer çš„è¿‡ç¨‹ä»¥åŠé‡è¦çš„è¶…å‚æ•°

* å› ä¸º tokenizer å’Œ model æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼ˆå³ tokenizer çš„è¾“å‡ºæ˜¯ model çš„è¾“å…¥ï¼‰ï¼Œæ‰€ä»¥ä¸€ä¸ªä½¿ç”¨çš„èŒƒå¼æ˜¯ï¼š
  ```python
  batch_input = tokenizer(test_sequence,)
  model(**batch_input)
  ```

  * è¾“å‡ºï¼š`{'input_ids': tensor, 'attention_mask': tensor}`

  * **å·¥ä½œè¿‡ç¨‹**

    * `tokenizer`

      ```python
      test_senteces = ['today is not that bad', 'today is so bad', 'so good']
      batch_input = tokenizer(test_senteces, truncation = True, padding = True, return_tensors = "pt")
      batch_input
      ```

    * `tokenizer.tokenize()`

      è¿™æ˜¯ä¸€ä¸ªåˆ†è¯å‡½æ•°ï¼Œå°±æ˜¯å°†è¾“å…¥çš„å¥å­ç»™åˆ†è¯ï¼Œä¾‹å¦‚

      ```python
      tokenizer.tokenize(test_senteces[0],)
      ```

      ```bash
      ['today', 'is', 'not', 'that', 'bad']
      ```

    * `tokenizer.convert_tokens_to_ids()`

      ç”¨æ¥æŠŠ**ä¸€ä¸ªæˆ–å¤šä¸ªå·²åˆ†å‰²å¥½çš„ tokenï¼ˆå­—ç¬¦ä¸²ï¼‰è½¬æ¢æˆå¯¹åº”çš„è¯è¡¨ IDï¼ˆæ•´æ•°ï¼‰**

      ```python
      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences[0],))
      ```

      ```bash
      [2651, 2003, 2025, 2008, 2919]
      ```

      æ²¡æœ‰å¼€å§‹ ID åºåˆ—å’Œç»“æŸ ID åºåˆ—

    * `tokenizer.encode()`

      ç”¨äºå°† **ä¸€æ¡æ–‡æœ¬ï¼ˆæˆ–ä¸€å¯¹æ–‡æœ¬ï¼‰ç¼–ç æˆ token çš„ ID åºåˆ—**ã€‚

      å®ƒå°†è¾“å…¥çš„æ–‡æœ¬ **è½¬æ¢æˆ token ID çš„åˆ—è¡¨**ï¼ˆæ•´æ•°ï¼‰ã€‚è¿”å›çš„æ˜¯ **çº¯æ•´æ•°åˆ—è¡¨**ï¼Œä¸æ˜¯å­—å…¸ï¼Œä¹Ÿæ²¡æœ‰ attention maskã€‚

      ```python
      tokenizer.encode(test_senteces[0],)
      ```

      ```bash
      [101, 2651, 2003, 2025, 2008, 2919, 102]
      ```

      è¿™é‡Œ 101 å’Œ 102 åˆ†åˆ«æ˜¯å¼€å§‹ç¬¦å’Œç»“æŸç¬¦ï¼›2651 å¯¹åº” todayï¼Œä¾æ¬¡ç±»æ¨ã€‚

      å¯ä»¥å‘ç° `tokenizer.encode()` â‰ˆ `tokenizer.tokenize() + tokenizer.convert_tokens_to_ids()`

    * `tokenizer.decode()`

      ç”¨æ¥å°† **token ID åˆ—è¡¨è½¬æ¢å›å¯è¯»æ–‡æœ¬å­—ç¬¦ä¸²** çš„æ–¹æ³•ã€‚

      ```python
      tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919,  102])
      ```

      ```bash
      '[CLS] today is not that bad [SEP]'
      ```

      `[CLS]` å’Œ `[SEP]` æ˜¯ **BERT** ä»¥åŠå¾ˆå¤šåŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ï¼ˆspecial tokensï¼‰ï¼Œå®ƒä»¬å„è‡ªæœ‰ç‰¹å®šçš„ä½œç”¨ï¼š

      | Token   | ä½œç”¨                       |
      | ------- | -------------------------- |
      | `[CLS]` | èšåˆåºåˆ—ä¿¡æ¯ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ |
      | `[SEP]` | åˆ†å‰²å¥å­æˆ–æ–‡æœ¬ç‰‡æ®µ         |

    **`tokenizer` å·¥ä½œçš„åŸç†å…¶å®å°±æ˜¯ `tokenizer.vocab`ï¼šå­—å…¸ï¼Œå­˜å‚¨äº† token => id çš„æ˜ å°„å…³ç³»ã€‚å½“ç„¶è¿™ä¸ªå­—å…¸ä¸­è¿˜åŒ…å«äº†ä¸€äº›ç‰¹æ®Šçš„ `token`ï¼š`tokenizer.special_tokens_map`**

    **`tokenizer` æ˜¯æœåŠ¡äº `model` çš„**

### å‚æ•°ç†è§£

ä»¥ä¸€ä¸ªä¾‹å­ä¸ºä¾‹

```python
tokenizer(test_sentences, max_length = 32, truncation = True, padding = 'max_length', return_tensors = 'pt')
```

è¿™é‡Œé¢ `max_length` ä¸ `padding` æ˜¯â€ä¸å…¼å®¹â€œçš„ã€‚å½“ä½ åˆ¶å®šäº† `max_length` ä¹‹ååˆæƒ³è¦å¡«å……ï¼Œåˆ™éœ€è¦ä»¤ `padding = 'max_length'`

æˆ–è€…

```python
tokenizer(test_sentences, truncation = True, padding = True, return_tensors = 'pt')
```

å½“æˆ‘ä»¬æŠŠ `max_length` æ¶ˆç­æ‰åï¼Œ`padding` å°±å¯ä»¥æŒ‡å®šä¸º True äº†ï¼Œåº”è¯¥å°±æ˜¯å…ˆéå†ä¸€éæ‰€æœ‰çš„å¥å­ï¼Œç„¶åå°†æœ€é•¿çš„é‚£ä¸ªå¥å­çš„é•¿åº¦ä½œä¸ºâ€œ`max_length`â€ï¼Œ`padding = True` å³ä¸ºå¡«å……

| å€¼             | å«ä¹‰                                      | è¯´æ˜                                 |
| -------------- | ----------------------------------------- | ------------------------------------ |
| `True`         | å¯ç”¨ paddingï¼ˆé»˜è®¤å¯¹è¯¥ batch å†…æœ€é•¿åºåˆ—ï¼‰ | åŠ¨æ€ paddingï¼Œé€‚åˆæ¨¡å‹æ•ˆç‡ä¼˜åŒ–       |
| `"longest"`    | ç­‰ä»·äº `True`                             | åª pad åˆ°å½“å‰ batch ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ |
| `"max_length"` | å¡«å……åˆ° `max_length` æŒ‡å®šçš„å›ºå®šé•¿åº¦        | å¸¸ç”¨äºæ¨¡å‹è®­ç»ƒï¼ˆé™æ€ shapeï¼‰         |
| `False`        | ä¸è¿›è¡Œ padding                            | æ‰€æœ‰æ ·æœ¬é•¿åº¦éœ€ä¸€è‡´ï¼Œå¦åˆ™æŠ¥é”™         |

äº‹å®ä¸Š `attention_mask` ä¹Ÿä¸ `padding` ç›¸åŒ¹é…ã€‚`attention_mask` ä¸º 0 çš„éƒ¨åˆ†å³ä¸º `padding` çš„éƒ¨åˆ†



### è¾“å…¥åˆ°æ¨¡å‹ï¼Œç†è§£æ¨¡å‹çš„è¾“å‡º

```python
# ç”±äºæ˜¯æ¨ç†çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¸æ¶‰åŠè®­ç»ƒï¼Œå› æ­¤æ”¾å…¥ no_grad()ä¸­
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
```

å¯ä»¥å°† logits ç†è§£ä¸ºï¼šåœ¨é€åˆ° softmax ä¹‹å‰çš„è¾“å‡ºã€‚

æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸ª logits å†æ¬¡è¿›è¡Œå¤„ç†ã€‚è§ä¸‹ï¼š

```python
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
    scores = F.softmax(outputs.logits, dim = 1)
    print(scores)
    labels = torch.argmax(scores, dim = 1)
    print(labels)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
tensor([[8.4632e-04, 9.9915e-01],
        [9.9980e-01, 1.9531e-04],
        [1.5837e-04, 9.9984e-01]])
tensor([1, 0, 1])
```

å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„ç»“æœä¸º `[1, 0, 1]` è¡¨ç¤ºç§¯æã€æ¶ˆæã€ç§¯æã€‚

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ¨¡å‹çš„é…ç½®

```python
model.config
```

```bash
DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased-finetuned-sst-2-english",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "sst-2",
  "hidden_dim": 3072,
  "id2label": {
    "0": "NEGATIVE",
    "1": "POSITIVE"
  },
  "initializer_range": 0.02,
  "label2id": {
    "NEGATIVE": 0,
    "POSITIVE": 1
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.30.2",
  "vocab_size": 30522
}
```

å¯ä»¥å…³æ³¨ä¸‹å…¶ä¸­çš„ `id2label`ã€‚æˆ‘ä»¬å†åšä¸€ä¸ªæ˜ å°„

```python
with torch.no_grad():
    outputs = model(**batch_input)
    print(outputs)
    scores = F.softmax(outputs.logits, dim = 1)
    print(scores)
    labels = torch.argmax(scores, dim = 1)
    print(labels)
    # tensorçš„æ•°æ®ä¸èƒ½ç›´æ¥å»åšç´¢å¼•ã€‚è¦ä¹ˆ .item() è½¬æ¢è¦ä¹ˆ .tolist() 
    labels = [model.config.id2label[id] for id in labels.tolist()]
    print(labels)
```

```bash
SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],
        [ 4.7508, -3.7899],
        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)
tensor([[8.4632e-04, 9.9915e-01],
        [9.9980e-01, 1.9531e-04],
        [1.5837e-04, 9.9984e-01]])
tensor([1, 0, 1])
['POSITIVE', 'NEGATIVE', 'POSITIVE']
```

