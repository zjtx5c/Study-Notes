## 论文复现日志

### 前言

立志成为会造轮子的掉包侠，会写丹方的炼丹师，全栈精通的开发者

* 项目结构：一般都是工程上的最佳实践，需要多看别人的代码自己总结。可以参考知乎的[这篇](https://www.zhihu.com/question/406133826)
  常用的一个结构可以是这样

  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----models/：模型
  --------model.py
  ----configs/：配置文件
  --------config.py
  ----datasets/：加载数据
  --------data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```

  若项目比较简单，也可以是这样
  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----model.py
  ----config.py
  ----data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```



### 	LICAP

#### 预备知识点

* 对比学习，无监督学习包括对比式学习与生成式学习，**对比学习是无监督学习的一种**

  * > 在重要节点识别任务中，对比学习的主要思想是：
    >
    > 1. **构造正样本（Positive Sample）**：通常是**同一节点的不同视图**，或者**同类别的重要节点**。
    > 2. **构造负样本（Negative Sample）**：通常是**普通节点**，或者**不同类别的节点**。
    > 3. **优化目标**：让重要节点的表示尽可能靠近，而普通节点远离它们，以便更好地区分节点的重要性。
    >
    > 常见的方法包括：
    >
    > - **基于全局结构的对比**：让重要节点在全局网络中保持一致性。
    > - **基于局部子图的对比**：让重要节点的局部结构与其重要性保持一致。
    > - **基于动态变化的对比**：在动态图中，学习重要节点随时间变化的稳定性。

* 二项分布概率质量函数

#### 理解数据

##### `fe15k_rel.pk`

* `fb15k_rel.pk`：这是一个知识图谱数据，表示**图谱的结构特征**，以字典的形式保存，涵盖了 `edges: tuple`， `edge_type: tensor`, `labels: tensor`, `feature: tensor`, `invalid_masks: tensor`

  具体的形式如下
  ```python
  {'edges': (tensor([ 345, 9796,  848,  ..., 5338, 4277, 1098]),
    tensor([10667,  1985,  6425,  ..., 12077, 11101, 13898])),
   'edge_types': tensor([  0,   1,   2,  ...,   9,  26, 166]),
   'labels': tensor([ 9504., 10199.,  9030.,  ..., 41222., 31642., 44935.]),
   'features': tensor([[-0.2106,  0.3095,  0.6127,  ..., -0.0039, -0.2529,  0.1002],
           [-0.2618,  0.6114, -0.2634,  ...,  0.3543, -0.0417, -0.0791],
           [ 0.1157,  0.0885, -0.1008,  ...,  0.1172,  0.3553, -0.1994],
           ...,
           [ 0.2731,  0.4277,  0.4038,  ...,  0.4685,  0.4484,  0.4826],
           [-0.2515, -0.2345,  0.9372,  ..., -0.1647,  0.3712, -0.0693],
           [ 0.2306, -0.1211, -0.1945,  ...,  0.5564,  0.0363, -0.1344]]),
   'invalid_masks': tensor([0, 0, 0,  ..., 0, 0, 0])}
  ```

  * 节点数量为 14,951，边的数量为 592,213，谓词种类为 1,345，有将近 14,105 (94.3%) 的节点是有节点特征的（64维嵌入）
  * `edges`: `tuple` ，表示源点与汇点的索引
  * `edge_types: tensor`，表示每条边的**谓词种类**
  * `labels: tensor`，表示节点重要性（node importance）的得分，通过从维基百科对收集到的真实世界重要性值（如FB15K的页面浏览量）进行对数转换来获得。（有时间去了解一下具体是怎么收集的）
  * `features：tensor  (num_nodes, 64)`，表示每个节点的词嵌入特征 ，通过 `node2vec` 算法得到节点特征
  * `invalid_masks`, 指示哪些**节点**或边需要被忽略或排除在计算之外。经过验证 `invalid_masks` 为 `1` 的节点其 `labels` 为 `0`, 我们不使用这些数据进行训练

##### `fb_lang.pk`

* `fb_lang.pk` ：这是一个仅关于节点的数据，表示**图谱节点的语义特征**，是一个二维 `np.array` 
  具体形式如下

  ```python
  array([[-0.07588089, -0.15141135, -0.13396032, ..., -0.08600746,
          -0.17194201,  0.53592861],
         [-0.02278661, -0.29997835, -0.0073817 , ...,  0.02436821,
          -0.02998137,  0.46098486],
         [-0.15221825, -0.06115236, -0.34801662, ..., -0.12346848,
          -0.14316572,  0.39701506],
         ...,
         [ 0.01512893,  0.186928  , -0.18825455, ..., -0.17741039,
          -0.02106529,  0.62954003],
         [-0.21668366, -0.01483662, -0.10437213, ...,  0.15704495,
          -0.13830011,  0.55147076],
         [-0.05817803,  0.12917031, -0.13514727, ..., -0.02458374,
          -0.02240661,  0.38673902]])
  ```

  论文中提到其通过 Transformer-XL 生成，维度为 **128**。但是实际数据上却是768 维？语义信息通常通过自然语言处理技术（如Transformer-XL）从节点的文本描述中提取，用于补充知识图谱的结构信息（如节点间的连接关系）。例如，在电影知识图谱（TMDB5K）中，语义信息可能包括电影的剧情简介、演员或导演的文本描述；在学术知识图谱（GA16K）中，可能是论文的标题或摘要。

  **语义信息和结构信息被作为两个独立的输入通道（例如在 RGTN 模型中），共同用于节点重要性估计任务。**



#### 理解预处训练过程

* 我将学习到交叉验证、早停机制、划分正例样本与负例样本、分桶机制

* 设置参数与超参数

* 设置各种保存路径，一般都会保存在 `checkpoints` 中

* 开始进入交叉验证大循环：
  * 进入 `load_data()` 函数开始装载与提取数据（涉及三种情况（1）只提取节点语义特征（2）提取两者特征并将它们拼接（3）提取两者特征但不将它们拼接）
    * 提取出对应的特征、掩码等数据
    * 处理节点的入度：对其进行 norm 处理（归一化因子），然后将归一化因子保存到**图的节点数据中**（`g.ndata['norm'] = norm`），并使用 `g.apply_edges()` 将每一个汇点的 `norm` 添加到对应的边上。
    * 标签的对数变换
    * 数据集划分（70% for train, 10% for val, 20% for test）
      * 五折交叉验证，==**这个操作手法需要学习一下（交叉验证）**==
    * 返回 `hg, edge_types, edge_norm, rel_num, node_feats, labels, train_idx, val_idx, test_idx`

  * 进入 `find_imp_idx()` 函数寻找重要节点（这是该论文 `LICAP` 的核心概念，用于指导**对比学习**的样本生成策略）。==**这里的操作手法需要学习一下（划分正负例样本）**==

  * 进入 `et_imp_bin_idx2node_idx()` 函数，根据节点标签的数值范围，将目标节点 `target_idx` 分桶，并记录哪些桶是有效的。其主要作用是**对重要节点进行分段索引**，可能用于后续的分析或采样。==**这里的操作手法需要学习一下（分桶操作）**==

  * 进入 `return_bin_idx2bin_coeff()` 函数**计算每个 bin（桶）的权重系数**，并存储在 `bin_coeff` 字典中。其核心逻辑是使用**二项分布概率质量函数（PMF，Probability Mass Function）**来计算不同 bin 之间的权重。

    * 理解其原理，理解 $k$ 和 $n$ 是如何选取的

    * 应用场景如下：

      **在重要节点识别中**，可能用于**根据 bin 的位置，为不同 bin 赋予不同的加权系数** 即衡量不同 bin 之间的**相对影响力**。
    
      **在对比学习或采样任务中**，可以作为权重分布，引导模型更关注某些 bin。
    
      通过利用二项分布的 **概率质量函数** 来衡量节点间（当然这里是 bin ）的相关性和影响力，尤其在一些 **图神经网络** 或 **基于距离的加权策略** 中得到广泛应用。
    
  * 进入 `return_imp_node_coeff()` 函数，**提取重要节点对应的系数**：根据 `imp_idx` 中的节点索引，找到每个节点对应的 bin 索引，进而提取每个节点所属 bin 的系数。**拼接系数**：将所有提取的系数拼接成一个大张量，并返回。
  
  * 处理自环问题并且更新 `edge_types` ，`rel_num` `n_edges`
  
  * 开始进行 模型 的预训练
  
    * 创建三个损失模块
    
    * 隐藏层维度为8，头为8，谓词嵌入的维度为10
    
    * 进入谓词嵌入 GAT 模型 `Predicate_GAT`（==**这个一定要掌握！！**==）
    
      * `in_dim = 64`, `hidden_dim = 8`, `out_dim = 64`, `rel_dim = 10`
    
    * 进入早停模块初始化
    
    * 进入 `return_centroids()` 函数: **计算一组节点嵌入（embeddings）的质心（centroid）**,之所以被称之为质心是因为它会分别计算每个 `bin` 中对应节点特征的均值，即为该 `bin` 的质心
    
      > **计算 bin 内所有节点的质心**（均值）。
      >
      > **存入 `bin2centroids`**，用于单独查询每个 bin 的质心。
      >
      > **拼接 `imp_bin_centroids`**，形成所有 bin 质心的矩阵。
      >
      > **跳过空 bin**，避免计算错误。
  

#### 理解两个损失函数



#### 一些工具或者技巧

* 设置随机种子

* 简单早停模块

* 梯度裁剪

  * 防止梯度爆炸，用于提高训练稳定性，常用于深度网

  * `torch.nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = 10, norm_type = 2)`

    > 梯度裁剪的缩放公式如下：
    >
    > $g_i = g_i \times \frac{\text{max\_norm}}{\|\mathbf{g}\|_2}$
    >
    > 其中：
    >
    > * $g_i$ 是梯度的某个分量。
    >   
    > * $\|\mathbf{g}\|_2$ 是所有梯度的 **L2 范数**（即欧几里得范数）：
    >   
    >   $\|\mathbf{g}\|_2 = \sqrt{\sum_i g_i^2}$
    >
    > * **如果** $\|\mathbf{g}\|_2 \leq \text{max\_norm}$，则不进行缩放。
    >
    > * **如果** $\|\mathbf{g}\|_2 > \text{max\_norm}$，则所有梯度按照这个比例缩放，使得梯度范数不会超过 `max_norm`。

