## 论文复现日志

### 前言

立志成为会造轮子的掉包侠，会写丹方的炼丹师，全栈精通的开发者



#### 如何想 idea？

[李沐分享](https://www.bilibili.com/video/BV1ea41127Bq?buvid=XYC6E55C7A1288F6E616688AAB8E564D963BF&from_spmid=main.space-contribution.0.0&is_story_h5=false&mid=zsw0awYQcKQ0Z8%2FLSPc5OQ%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=01e2a6f7-c6e5-4a5a-98ee-d4f813b3639d&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1723199264&unique_k=d7FYToH&up_id=1567748478&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

$\text{研究价值} = \text{新意度} \times \text{有效性} \times \text{问题大小}$

* 写论文要追求什么？

1. 深刻，你是否揭示了一些比较本质的东西
2. 优美，你的论文的证明与描述是否有美感

* 如何定义有新意

1. 不要去堆砌无谓的复杂性，模型可以越简单越好
2. 论文的作者必须要呕心沥血才能发表文章？

##### 随便想想的 idea

idea：在对比学习的基础上，考虑将网络做深，从多个GNN模型中学习知识并通过，结合 cka 以及蒸馏的方法不断对上游目标进行矫



#### 项目结构的设置

* 项目结构：一般都是工程上的最佳实践，需要多看别人的代码自己总结。可以参考知乎的[这篇](https://www.zhihu.com/question/406133826)
  常用的一个结构可以是这样

  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----models/：模型
  --------model.py
  ----configs/：配置文件
  --------config.py
  ----datasets/：加载数据
  --------data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```

  若项目比较简单，也可以是这样
  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----model.py
  ----config.py
  ----data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```

### 总结

#### 读取与存放文件

* 理解序列化与反序列化

  `pickle` 是一个**用于序列化和反序列化 Python 对象的模块**。**序列化是将 Python 对象转换为字节流（例如，存储在文件中），而反序列化是将字节流重新转换为 Python 对象。**

* `pickle.load()` 与 `pickle.dump()`

> `pk.load`（通常是 `pickle.load` 的简写）是与 `pickle.dump` 对应的操作，它用于从文件中**反序列化数据**，**将存储在文件中的字节流转换回 Python 对象。**
>
> 具体来说，`pickle.load` 从一个已经打开的文件中读取数据，并返回反序列化后的 Python 对象。示例如下：
>
> ```python
> with open('file.pkl', 'rb') as f:
>     obj = pickle.load(f)
> ```
>
> 在这个例子中，`file.pkl` 是一个包含序列化数据的文件，`pickle.load(f)` 会将文件中的字节流读取并转换为原来的 Python 对象，保存在 `obj` 中。这里 `rb` 表示以二进制读取模式打开文件。
>
> ___
>
> 
>
> `pk.dump` 是 `pickle.dump` 的简写，它将一个对象序列化并写入一个文件或类似文件的对象中。语法如下：
>
> ```python
> import pickle
> 
> with open('file.pkl', 'wb') as f:
>     pickle.dump(obj, f)
> ```
>
> 在这个例子中，`obj` 是你想要序列化的 Python 对象，`f` 是一个已经打开的文件对象（以**二进制写模式** `'wb'` 打开）。`pickle.dump` 会将 `obj` 写入文件 `file.pkl` 中。
>
> `pickle` 模块通常用于**将模型、数据结构等保存在磁盘上，以便后续加载和使用。**



* 关于 `.pt` 文件与 `.pkl` 文件

> **区别总结**
>
> 1. **用途**：
>    - `.pt` 主要用于 PyTorch **模型和张量**的保存与加载。
>    - `.pkl` 用于 Python 中广泛的对象序列化，包括非 PyTorch 对象。
> 2. **文件格式和方法**：
>    - `.pt` 是通过 `torch.save` 和 `torch.load` 保存和加载的，**专为 PyTorch 对象设计。**
>    - `.pkl` 是通过 `pickle.dump` 和 `pickle.load` 保存和加载的，**通用于 Python 对象。**
> 3. **兼容性**：
>    - `.pt` 适用于 PyTorch 环境，更能保证 PyTorch 模型的兼容性。
>    - `.pkl` 是更通用的文件格式，适用于任何 Python 对象，但不专门优化处理 PyTorch 模型。
>
> 简而言之，`.pt` 是 PyTorch 特有的文件格式，而 `.pkl` 是 Python 的通用序列化格式。

#### 设置随机种子

在 `PyTorch` 及 `DGL` 相关的代码中，为了确保实验的可复现性，一般需要在以下几个地方设置随机种子

（1）python 内置库（2）Numpy（3）Pytorch（4）PyTorch 计算相关（5）DGL（6）操作系统

```python
def set_random_seed(seed=0):
    '''
    设置随机种子，以保证实验的可复现性
    :param seed: int, 要使用的随机种子
    :return: None
    '''
    # Python 内置随机库
    random.seed(seed)

    # NumPy 随机种子
    np.random.seed(seed)

    # 设置 Python 运行时的哈希种子（影响某些哈希操作的随机性）
    os.environ['PYTHONHASHSEED'] = str(seed)

    # PyTorch CPU 端随机数
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        # 设置当前 GPU 的随机数种子
        torch.cuda.manual_seed(seed)
        # 设置所有可用 GPU 的随机数种子
        torch.cuda.manual_seed_all(seed)

        # 让 CuDNN 以确定性方式执行计算，以确保结果可复现
        torch.backends.cudnn.deterministic = True
        # 关闭 CuDNN 的自动优化算法，以防止影响结果可复现性
        torch.backends.cudnn.benchmark = False
        # 禁用 CuDNN 加速，以最大程度保证实验可复现
        torch.backends.cudnn.enabled = False

    # DGL 相关随机种子
    dgl.random.seed(seed)

```

#### 设置最佳模型路径

* 需要用到的函数

```python
os.path.join()
os.path.exists()
'_'.join([f'{k}{v}' for k, v in parameters_dic.items()])
```



一般最佳模型都是保存在 `ckpt` 文件夹下，比如说 `project_name/ckpt/...` ，若有预训练模型，则保存在对应的预训练的文件夹下则 `preject_name/pretrain/ckpt/...`。若要训练多个模型，则可以这样 `project_name/ckpt/mode1/...` 或者 `project_name/pre_train/ckpt/model1/...`

```
--project_name
----ckpt
------model1
------model2
```

```
--project_name
----pretrain
------ckpt
--------model1
--------model2
```

后面的路径可以根据自己的超参数自己添加，比如早停忍耐轮次，重要性节点的采样比率等等。最后一层可以放上最后的数据集，如下所示

```
--project_name
----pretrain
------ckpt
--------model1
----------imp_ratio_0.1
------------patience_20
--------------fb15k
```

使用字符串配合 `args.` 解析命令参数来创建文件夹。具体上实现的思路是：分别写出每一层的**相对目录**，**最后拼接上去**，以我们上述举的例子为例

比如我们现在要保存预训练的模型 `model1` , 重要性节点的采样比率`imp_ration` 设置为0.1，早停忍耐轮次`patience` 设定为20。那么我们可以这样
`pretrain_root = 'pretrain/ckpt/'`  **（0）先设置根目录**

`model_path = str(args.model) + '/'`		**（1）设置模型相对路径**

`imp_ratio_path = 'imp_ratio_' + str(args.imp_ratio) + '/'`		**（2）设置节点重要性相对路径**

`patient_path = 'patience_' + str(args.patience) + '/'`				**（3）设置早停忍耐轮次相对路径**

`dataset_path = dataset_name + '/'` 			**（4）设置数据集相对路径**

最后将整个路径拼接起来：

```python
out_save_path = os.path.join(pretrain_root, model_path, imp_ratio_path, patience_path, dataset_path)
if not os.path.exists(out_save_path):
    os.makedirs(out_save_path, exist_ok = True)
```



这种方法，当参数过多是可能会很长。因此，此基础上，我们可以将其他的超参数（比如 `lr`， 交叉验证的轮次 `cross_id`， 损失 `loss` 的系数）添加到最终的模型名中为模型命名如：

`tmp = dataset_name + '_struct_pregat_pretrained_lr' + str(args.lr) + '_loss_' + str(args.eta) + '_' + str(cross_id) + '.pkl' `

`out_pretrained_path = os.path.join(out_save_path, tmp)`

上述这种方法较为冗长，我们可以**使用字典管理参数**

```python
# 定义模型文件名的关键参数
model_name_parts = {
    "dataset": dataset_name,
    "model": "pregat_pretrained",
    "lr": args.lr,
    "loss": args.eta,
    "cross": cross_id
}

# 生成文件名（去掉值为 None 或空字符串的参数）
model_filename = "_".join([f"{k}{v}" for k, v in model_name_parts.items() if v is not None]) + ".pkl"

# 最终模型保存路径
out_pretrained_path = os.path.join(out_save_path, model_filename)
```



#### 设置 cuda 并将数据传到 cuda

* 需要用到的函数

```python
# 设置 GPU 编号
torch.cuda.set_device()	# 这里可以填 cuda = 0, cuda = 1, 是说用机器的第几块 GPU

# 将张量移动到 cuda 上
.cuda()		# 只能移到默认 GPU（cuda:0） 或你手动指定的 GPU。
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
.to(device)	# 更加通用
```

* 需要思考有哪些数据要放到 cuda 上

  * 模型的参数：模型的参数（如权重、偏置等）应该放在 GPU 上进行计算和更新。尤其在训练过程中，这些参数会经过反向传播并需要在 GPU 上更新。
    ```python
    model.to(device)  # 将模型移到 GPU 或 CPU，device 是 "cuda" 或 "cpu"
    ```

    

  * 训练数据：训练数据（例如**输入特征、标签**等）也需要放到 GPU 上，特别是当训练过程需要频繁地在 GPU 和 CPU 之间传输数据时，可能会成为性能瓶颈。**所有输入数据（如图片、文本数据）和对应的标签都应被放在 GPU 上**，以提高训练效率。

    ```python
    inputs = inputs.to(device)  # 使用 .to() 更通用
    labels = labels.to(device)  # 使用 .to() 更通用
    ```

    还有比如说与我目前方向相关的图数据：

    ```python
    g = g.int().to(device)
    ```

    

  * 损失函数中的相关数据：损失函数中的相关数据（例如，模型预测的输出和真实标签）也应放在 GPU 上进行计算。否则，如果损失函数中的数据仍然在 CPU 上，PyTorch 会自动将它们传输到 GPU，但这会导致性能损失。

  * 优化器的状态（通常由优化器自动管理）：优化器（如 `Adam`, `SGD`）本身的参数通常不会显式地放到 GPU 上，因为它会自动跟踪模型参数的梯度。然而，**如果你自己实现了优化器，或优化器的状态变量需要与模型参数一起计算，** 这些状态（例如动量等）也需要放到 GPU 上。例如，在使用 `torch.optim` 时，优化器会**自动处理 GPU 迁移**：
    ```python
    optimizer = torch.optim.Adam(model.parameters())  # 优化器会自动使用模型所在设备的参数
    ```

#### 交叉验证

* 框架如下

```python
for cross_id in range(args.cross_num):
    '''
    中间处理
    包括预处理、模型保存路径设置、其他提取数据、数据分割
    cuda 设置
    '''
   if cross_id == 0:
            print('---------Dataset Statistics---------')
            print('一些数据信息')
            print('-------------Pretraining------------') # (或者这里提示 training)
        
    print(f'cross: {cross_id}')
    print('------------')

   	print('Model Pretraining')
    '''
    其他处理
    '''
```

#### 早停机制（用类实现）

* 最基础的需要记录 7 项

  (1) early_stop (2) patience (3) counter (4) best_score (5) best_epoch (6) save_path (7) min_epoch

* 具体实现

```python
class EarlyStopping_simple:
    def __init__(self, patience=50, save_path=None, min_epoch=-1):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = None
        self.save_path = save_path
        self.min_epoch = min_epoch

    def step(self, acc, epoch, model):
        score = acc
        if epoch < self.min_epoch:
            return self.early_stop
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            self.save_checkpoint(model)
        elif score < self.best_score:
            self.counter += 1         
            if self.counter >= self.patience:
                self.early_stop = True
                print()
                print(f'EarlyStop at Epoch {epoch} with patience {self.patience}')
        else:
            self.best_score = score
            self.best_epoch = epoch
            self.save_checkpoint(model)
            self.counter = 0
        return self.early_stop

    def save_checkpoint(self, model):
        '''Saves model when validation loss decrease.'''
        torch.save(model.state_dict(), self.save_path)
```



#### 损失函数与模型之间的代码组织

我们可以先观察一下 `licap` 之间对损失函数以及模型之间的代码处理

```python
class Predicate_GAT(nn.Module):
    def __init__(self, args, g, in_dim, hidden_dim , out_dim, num_heads, rel_num, rel_dim, 
                 loss_function=None,
                 loss_1_fcn=None,
                 loss_2_fcn=None,
                 train_important_idx=None,
                 train_normal_idx=None,
                 imp_bin_idx2node_idx=None,
                 imp_node_idx2bin_idx=None,
                 imp_node_coeff=None):
        super(Predicate_GAT, self).__init__()
        self.layer1 = MultiHead_Predicate_GAT_Layer(g , in_dim, hidden_dim, rel_dim, num_heads)
        self.layer2 = MultiHead_Predicate_GAT_Layer(g, hidden_dim*num_heads, out_dim, rel_dim, 1)

        self.loss_fn = loss_function
        self.loss_1_fcn = loss_1_fcn
        self.loss_2_fcn = loss_2_fcn
        

        self.train_important_idx = train_important_idx
        self.train_normal_idx = train_normal_idx
        
        self.imp_bin_idx2node_idx = imp_bin_idx2node_idx
        self.imp_node_idx2bin_idx = imp_node_idx2bin_idx
        self.imp_node_coeff = imp_node_coeff

        self.loss_eta1 = args.loss_eta1
        self.loss_eta2 = args.loss_eta2

        # Relation Embedding
        self.rel_emb = nn.Embedding(rel_num, rel_dim) 


    def forward(self, node_feats, edge_types, idx=None):

        edge_feats = self.rel_emb(edge_types) # [num_edges, rel_dim]

        h = self.layer1(node_feats, edge_feats)
        h = F.elu(h)
        h = self.layer2(h, edge_feats)

        # caculate_loss            
        embed_all = h # [N_node, dim]
                     
        imp_bin_centroids, bin2centroids = return_centroids(embed_all, self.imp_bin_idx2node_idx, self.train_important_idx) # [bin_num, dim]
                
        imp_node_centroid = return_imp_node_centroid(self.imp_node_idx2bin_idx, bin2centroids, self.train_important_idx) # [imp_num, dim]    
        embed_normal = embed_all[self.train_normal_idx] ### mod sample by constant
                
        loss_1 = self.loss_eta1 * self.loss_1_fcn(embed_all[self.train_important_idx], embed_normal)

        loss_2 = self.loss_eta2 * self.loss_2_fcn(embed_all[self.train_important_idx], imp_bin_centroids, imp_node_centroid, self.imp_node_coeff)

        loss = loss_1 + loss_2

        return h, loss, loss_1, loss_2
```

对于比较复杂的损失，我们直接写一个类对其进行封装处理

这段代码将损失的计算直接搬到了模型的 `forward` 函数中，也就是说，模型每一次前向传播计算处理完数据之后，直接进行了损失计算，即我们在 `Predicate_GAT` 类中封装了损失计算逻辑，对于比较复杂的损失计算，这种设计时合理的。

同时我们需要在**定义模型实例时**传入计算损失函数**需要用到的辅助数据**如 `imp_bin_idx2node_idx` 等），这些是与任务特定信息相关的参数。将这些数据封装在模型的类中，是一种常见的做法，可以在整个训练过程中**保持一致性**。损失函数的计算依赖于其 `forward` ，而 `forward` 需要使用的数据则是在定义模型实例时**传入的辅助数据**



==事实上，我个人认为，将损失函数的计算写在外部是更好的选择==

> 将损失计算移到外部进行，是一种常见的做法，尤其适用于那些**损失函数较复杂**或者需要灵活调整的情况。它有助于代码的模块化、灵活性和复用性，并且使得模型的实现更加专注于核心任务，即图数据的处理和嵌入计算，放在外面可以减小这个类的实例的**调入调出的开销，提高内存的使用率**。我们可以根据任务需求选择是否采用这种结构化的方式。

即，我们可以直接在训练循环中，使用模型的输出（`embed_all`）来计算损失，如下所示

```python
# 假设已经实例化了模型 Predicate_GAT 和损失函数
model = Predicate_GAT(args, g, in_dim, hidden_dim, out_dim, num_heads, rel_num, rel_dim)

# 训练过程
for epoch in range(num_epochs):
    model.train()
    
    # 获取节点特征和边特征
    node_feats = ...
    edge_types = ...
    
    # 前向传播得到嵌入表示
    embed_all = model(node_feats, edge_types)
    
    # 损失计算
    imp_bin_centroids, bin2centroids = return_centroids(embed_all, imp_bin_idx2node_idx, train_important_idx)
    imp_node_centroid = return_imp_node_centroid(imp_node_idx2bin_idx, bin2centroids, train_important_idx)
    embed_normal = embed_all[train_normal_idx]
    
    loss_1 = loss_eta1 * loss_1_fcn(embed_all[train_important_idx], embed_normal)
    loss_2 = loss_eta2 * loss_2_fcn(embed_all[train_important_idx], imp_bin_centroids, imp_node_centroid, imp_node_coeff)
    
    loss = loss_1 + loss_2
    
    # 反向传播和优化
    loss.backward()
    optimizer.step()

```





### 经验与造轮子

#### 损失函数

* 最终得到的这个标量损失最后一步**一般是由一组向量推来**，**一般情况下我们使用 `.mean` 来得到损失而不是用 `.sum` 得到损失**（注意，我们一般情况下不对向量进行反向传播，即我们要门对其做 `.mean` 处理，要么对其做 `.sum` 处理）

  * 思考为什么

  > （1）保持梯度稳定性
  >
  > 如果使用 `.sum()`，那么损失的大小会随 batch size 增大而增大，导致梯度也变大。例如：
  >
  > ```python
  > import torch
  > 
  > x = torch.randn(10, requires_grad=True)  # 10个样本
  > loss = x**2  # 每个样本的损失
  > loss_sum = loss.sum()  # 直接求和
  > loss_mean = loss.mean()  # 计算均值
  > 
  > loss_sum.backward()  # 计算梯度
  > print(x.grad)  # 梯度较大
  > 
  > x.grad.zero_()  # 清空梯度
  > 
  > loss_mean.backward()  # 计算梯度
  > print(x.grad)  # 梯度较小，数值稳定
  > ```
  >
  > **现象：** `loss_sum` 的梯度比 `loss_mean` 大 **10 倍** （即 `batch_size` 倍），因为 `.sum()` 直接把所有样本的损失加起来，导致梯度变大。
  >
  > 如果 batch size 变化，梯度的大小也会变得不稳定，而 `.mean()` 可以保持梯度在一个稳定的范围内，使得学习率不需要调整。
  >
  > （2）与学习率无关
  >
  > 在梯度下降中，学习率 (`lr`) 决定了参数更新的步长。如果我们使用 `.sum()`，梯度的数值会随着 batch size 改变，导致学习率需要调整：
  >
  > - 用 `.sum()`，如果 batch size 变大，梯度变大，可能导致梯度爆炸，**需要手动缩小学习率**。
  > - 用 `.mean()`，梯度大小不变，学习率可以保持一致，不需要频繁调整。
  >
  > 因此，`.mean()` 使得**同样的学习率适用于不同的 batch size**，从而提高了模型的稳定性和可移植性。
  >
  > （3）使不同 batch 的损失具有可比性
  >
  > 假设我们有两个 batch，batch size 分别是 32 和 16，如果使用 `.sum()`，那么 batch1 的总损失比 batch2 的损失大了一倍，这不合理，因为两个 batch 的样本**平均损失是一样的**。但如果使用 `.mean()`，尽管两个 `batch` 的 `batch_size` 不一样，但是它们的梯度却没有与 `batch_size` 产生强线性关系，使得模型训练更稳定。
  
* 自己写了一个同构图上的多头谓词 `GAT` （更加并行）==**核心是理解注意力系数的更新操作（需要一点线代的洞察力！！）**==

```python
class GAT(nn.Module):
    def __init__(self, g, in_dim, out_dim, num_heads, rel_dim):
        super(GAT, self).__init__()
        self.g = g
        self.fc = nn.Linear(in_dim, num_heads * out_dim)
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.num_heads = num_heads

        self.attn_l = nn.Parameter(torch.empty(1, num_heads, out_dim))
        self.attn_mid = nn.Parameter(torch.empty(1, num_heads, rel_dim))
        self.attn_r = nn.Parameter(torch.empty(1, num_heads, out_dim))
        self.reset_parameters()

        
    def reset_parameters(self):
        gain = nn.init.calculate_gain('relu')
        nn.init.xavier_normal_(self.fc.weight, gain = gain)
        nn.init.xavier_normal_(self.attn_l, gain = gain)
        nn.init.xavier_normal_(self.attn_mid, gain = gain)
        nn.init.xavier_normal_(self.attn_r, gain = gain)

        
    def edge_attention(self, edges):
        return {'e': F.leaky_relu(edges.src['e_l'] + edges.edata['e_mid'] + edges.dst['e_r'])}  # (E, num_heads, 1)
    
    
    def message_func(self, edges):
        return {'e': edges.data['e'], 'z': edges.src['z']}

    
    def reduce_func(self, nodes):
        h = torch.sum(nodes.mailbox['e'] * nodes.mailbox['z'], dim = 1) # (N, num_heads, out_dim)
        return {'h': h}


    def forward(self, h, p):
        z = self.fc(h).view(-1, self.num_heads, self.out_dim)   # (N, num_heads, out_dim)
        p = p.unsqueeze(dim = 1).repeat(1, self.num_heads, 1)   # (E, num_heads, out_dim)
        e_l = (z * self.attn_l).sum(dim = -1, keepdim = True)  # (N, num_heads, 1)
        e_mid = (p * self.attn_mid).sum(dim = -1, keepdim = True)  # (E, num_heads, 1)
        e_r = (z * self.attn_r).sum(dim = -1, keepdim = True)  # (N, num_heads, 1)
        g.ndata.update({'e_l': e_l, 'e_r': e_r, 'z': z})
        g.edata.update({'e_mid': e_mid})
        g.apply_edges(self.edge_attention)

        e = g.edata['e']    # (E, num_heads, 1)
        g.edata['e'] = dgl.ops.edge_softmax(g, e)

        g.update_all(
            self.message_func,
            self.reduce_func
        )

        return g.ndata.pop('h') # (N, num_heads, out_dim)
    

class MultiHeadGAT(nn.Module):
    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads, rel_dim, rel_num, merge = 'cat'):
        super(MultiHeadGAT, self).__init__()
        self.layer1 = GAT(g, in_dim, hidden_dim, num_heads, rel_dim)
        self.layer2 = GAT(g, num_heads * hidden_dim, out_dim, 1, rel_dim)
        self.p_embed = nn.Embedding(rel_num, rel_dim)

    def forward(self, node_feats, edge_types):
        edge_feats = self.p_embed(edge_types)
        h = self.layer1(node_feats, edge_feats) # (N, num_heads, out_dim)
        h = h.view(-1, self.layer1.num_heads * self.layer1.out_dim)
        h = F.elu(h)
        h = self.layer2(h, edge_feats)  # (N, num_heads, out_dim) (N, 1, out_dim)
        h = torch.mean(h, dim = -1)
        return h
```



### 	LICAP

与以往知识图谱上的 NIE 方法不同，这项工作引入了现实世界中对重要节点（即具有高重要性分数的节点）偏好的潜在先验知识。**为了注入这种先验知识，提出了一种新颖的采样策略，结合对比学习，利用可用的节点重要性分数预训练节点嵌入**。还需注意的是，所提出的 LICAP 是一种**插件式方法**，可以集成到以前的 NIE 方法中，而不是一种新的特定 NIE 方法。

创新点：

1. 提升了 `embedding` 的质量

2. label 感知分组将连续分数划分成有序区间以**生成对比样本**；分箱机制，**顶层节点优先 分层采样机制**

   这么做的其中一个原因是，我们完全不需要在预训练阶段就估计节点重要性，这一任务将在下游实现；我们在预训练阶段的目的仅是为了得到更好的嵌入表

   这样一来，原本并非为回归问题设计的对比学习，现在可以采用对比样本对嵌入进行预训练。

3. 谓词 GNN
4. 通过联合最小化两个对比损失优化节点嵌入。



细节：

1. 顶层节点优先，分层采样机制

   top-bin 与 non-top-bin 根据 `important ratio`  $\gamma$ 划分，代码中是取 0.1

   ==理解一下 InfoNCE 损失函数==

2. 划分为第一级对比样本与第二级对比样本（分别对应 $L_1$ 和 $L_2$ 的 loss）

   在顶层区间中，将每个节点与顶层原型（即图 3 中的三角形）进行正对比。顶层原型是通过对所有顶层节点的嵌入进行元素级平均得到的。对于负对比，我们将从非顶层区间中随机采样的非顶层节点，作为每个顶层节点的负样本对。

   对于更细区间中的每个顶级节点（这里的更细区间是指将**顶层区间**划分为更细区间），其正对比样本被定义为该更细区间的原型（例如，图 4 中包含红色节点的更细区间的红色三角形），这同样是通过对该更细区间内节点的嵌入进行元素级平均得到的。至于负对比样本，它们分别也是其余更细区间的原型，我们设计了一种重新加权负采样机制，以保持这些更细区间之间的相对顺序。这种重新加权负采样机制基于这样一种直觉：来自距离更远区间的样本更不相似（或更不接近）。因此，我们引入一个接近系数 $\beta$ 来对负对比对进行重新加权，使得接近系数越大，相应的负样本对项在分母中的影响就越大。

   第一级对比样本和 $L_1$ 损失旨在更好地将顶层节点与非顶层节点区分开来，而第二级对比样本和 $L_2$ 损失则试图通过保持更细区间之间的相对顺序，来更好地区分顶层区间内的顶层节点。





疑惑点：

1. 这么做对比学习的目的是什么？

2. 这个接近系数 $\beta$ 到底有什么用？为什么要引入它，是用来区分什么的
   个人感觉是，正样本对都在同一层，所以正样本对之间是没有差异的，但是负样本对在不同层，要体现层与层之间的差异，用一个接近系数 $\beta$ 来衡量

3. 为什么要设计 $L_1$ 和 $L_2$ 损失，如果只是想找更优质的节点，直接降序排序得到不就行了吗？这么做的意义是什么？其更深层的原理和机制是什么。论文中提到要通过使用InfoNCE损失来结合对顶级节点偏好的先验知识来预训练节点嵌入，如何理解？我到现在还是无法理解这种损失到底是在做什么？？（还是得还之前 `deepwalk` 的账啊）

   突然之间秒懂，是要根据损失去调整嵌入特征 $\mathbf{h}_i$， 获得更优质的嵌入特征！比如获得的特征能够更好地区分 top-bin 中地节点和 non-top-bin 中的节点；同时也能更好地区分 top-bin 中其他不同层之间的节点，**通过对比学习和结合先验知识，生成更优质的节点嵌入，为后续的节点重要性估计任务提供更好的基础。**



#### 预备知识点

* 预训练范式

  * > 预训练范式（Pre-training Paradigm）是深度学习领域的一种模型训练方法，核心思想是通过**分阶段训练**来提升模型性能：
    > **先在大规模通用数据上进行无监督或自监督的预训练**，学习通用特征表示；**再在特定任务的小规模数据上进行微调**（Fine-tuning），适应下游任务。这一范式显著降低了数据标注成本，推动了自然语言处理（NLP）、计算机视觉（CV）等领域的突破。

* 对比学习，无监督学习包括对比式学习与生成式学习，**对比学习是无监督学习的一种**

  * > 在重要节点识别任务中，对比学习的主要思想是：
    >
    > 1. **构造正样本（Positive Sample）**：通常是**同一节点的不同视图**，或者**同类别的重要节点**。
    > 2. **构造负样本（Negative Sample）**：通常是**普通节点**，或者**不同类别的节点**。
    > 3. **优化目标**：让重要节点的表示尽可能靠近，而普通节点远离它们，以便更好地区分节点的重要性。
    >
    > 常见的方法包括：
    >
    > - **基于全局结构的对比**：让重要节点在全局网络中保持一致性。
    > - **基于局部子图的对比**：让重要节点的局部结构与其重要性保持一致。
    > - **基于动态变化的对比**：在动态图中，学习重要节点随时间变化的稳定性。
    
  * InfoNCE 将一个正样本与多个负样本进行区分，已成为更受欢迎的对比学习损失函数。该文章在节点重要性估计（NIE）问题中采用对比学习，通过使用 InfoNCE 损失函数，融入对重要节点（top nodes）偏好的先验知识，来预训练节点嵌入。

* 二项分布概率质量函数

* 相较于之前的相关工作，licap 这个工作使用了新的数据集 `GA16K` 和额外的回归度量作为基准

#### 理解数据

##### `fe15k_rel.pk`

* `fb15k_rel.pk`：这是一个知识图谱数据，表示**图谱的结构特征**，以字典的形式保存，涵盖了 `edges: tuple`， `edge_type: tensor`, `labels: tensor`, `feature: tensor`, `invalid_masks: tensor`

  具体的形式如下
  ```python
  {'edges': (tensor([ 345, 9796,  848,  ..., 5338, 4277, 1098]),
    tensor([10667,  1985,  6425,  ..., 12077, 11101, 13898])),
   'edge_types': tensor([  0,   1,   2,  ...,   9,  26, 166]),
   'labels': tensor([ 9504., 10199.,  9030.,  ..., 41222., 31642., 44935.]),
   'features': tensor([[-0.2106,  0.3095,  0.6127,  ..., -0.0039, -0.2529,  0.1002],
           [-0.2618,  0.6114, -0.2634,  ...,  0.3543, -0.0417, -0.0791],
           [ 0.1157,  0.0885, -0.1008,  ...,  0.1172,  0.3553, -0.1994],
           ...,
           [ 0.2731,  0.4277,  0.4038,  ...,  0.4685,  0.4484,  0.4826],
           [-0.2515, -0.2345,  0.9372,  ..., -0.1647,  0.3712, -0.0693],
           [ 0.2306, -0.1211, -0.1945,  ...,  0.5564,  0.0363, -0.1344]]),
   'invalid_masks': tensor([0, 0, 0,  ..., 0, 0, 0])}
  ```

  * 节点数量为 14,951，边的数量为 592,213，谓词种类为 1,345，有将近 14,105 (94.3%) 的节点是有节点特征的（64维嵌入）
  * `edges`: `tuple` ，表示源点与汇点的索引
  * `edge_types: tensor`，表示每条边的**谓词种类**
  * `labels: tensor`，表示节点重要性（node importance）的得分，通过从维基百科对收集到的真实世界重要性值（如FB15K的页面浏览量）进行对数转换来获得。（有时间去了解一下具体是怎么收集的）
  * `features：tensor  (num_nodes, 64)`，表示每个节点的词嵌入特征 ，通过 `node2vec` 算法得到节点特征
  * `invalid_masks`, 指示哪些**节点**或边需要被忽略或排除在计算之外。经过验证 `invalid_masks` 为 `1` 的节点其 `labels` 为 `0`, 我们不使用这些数据进行训练

##### `fb_lang.pk`

* `fb_lang.pk` ：这是一个仅关于节点的数据，表示**图谱节点的语义特征**，是一个二维 `np.array` 
  具体形式如下

  ```python
  array([[-0.07588089, -0.15141135, -0.13396032, ..., -0.08600746,
          -0.17194201,  0.53592861],
         [-0.02278661, -0.29997835, -0.0073817 , ...,  0.02436821,
          -0.02998137,  0.46098486],
         [-0.15221825, -0.06115236, -0.34801662, ..., -0.12346848,
          -0.14316572,  0.39701506],
         ...,
         [ 0.01512893,  0.186928  , -0.18825455, ..., -0.17741039,
          -0.02106529,  0.62954003],
         [-0.21668366, -0.01483662, -0.10437213, ...,  0.15704495,
          -0.13830011,  0.55147076],
         [-0.05817803,  0.12917031, -0.13514727, ..., -0.02458374,
          -0.02240661,  0.38673902]])
  ```

  论文中提到其通过 Transformer-XL 生成，维度为 **128**。但是实际数据上却是768 维？语义信息通常通过自然语言处理技术（如Transformer-XL）从节点的文本描述中提取，用于补充知识图谱的结构信息（如节点间的连接关系）。例如，在电影知识图谱（TMDB5K）中，语义信息可能包括电影的剧情简介、演员或导演的文本描述；在学术知识图谱（GA16K）中，可能是论文的标题或摘要。

  **语义信息和结构信息被作为两个独立的输入通道（例如在 RGTN 模型中），共同用于节点重要性估计任务。**



#### 理解预处训练过程

* 可能需要用到的函数

```python
np.round()
np.where()		# 注意返回的是一个 `tuple` 需要使用 [0] 进行索引
np.intersect1d(arr1, arr2, assume_unique = True)	# assume_unique = True 表示假设arr1和arr2都是唯一的，可以加速计算
np.random.choice(arr, size)

torch.sort()	# 两个返回值，分别是值和索引
```



* 我将学习到交叉验证、早停机制、划分正例样本与负例样本、分桶机制

* 设置参数与超参数，设置随机种子

* 设置各种保存路径，一般都会保存在 `checkpoints` 中

* 开始进入交叉验证大循环：
  * 进入 `load_data()` 函数开始装载与提取数据（涉及三种情况（1）只提取节点语义特征（2）提取两者特征并将它们拼接（3）提取两者特征但不将它们拼接）
    * 提取出对应的特征、掩码等数据
    * 处理节点的入度：对其进行 norm 处理（归一化因子），然后将归一化因子保存到**图的节点数据中**（`g.ndata['norm'] = norm`），并使用 `g.apply_edges()` 将每一个汇点的 `norm` 添加到对应的边上。
    * 标签的对数变换
    * 数据集划分（70% for train, 10% for val, 20% for test）
      * 五折交叉验证，==**这个操作手法需要学习一下（交叉验证）**==
    * 返回 `hg, edge_types, edge_norm, rel_num, node_feats, labels, train_idx, val_idx, test_idx`

  * 进入 `find_imp_idx()` 函数寻找重要节点（这是该论文 `LICAP` 的核心概念，用于指导**对比学习**的样本生成策略）。==**这里的操作手法需要学习一下（划分正负例样本）**==

    * 这里最后刻意将 `normal` 按照节点 50 % 采样。（从代码中看， normal 节点的数量大概是 important 节点数量的两倍）

  * 进入 `et_imp_bin_idx2node_idx()` 函数，根据节点标签的数值范围，将目标节点 `target_idx` 分桶，并记录哪些桶是有效的。其主要作用是**对重要节点进行分段索引**，可能用于后续的分析或采样。==**这里的操作手法需要学习一下（分桶操作）**==

  * 进入 `return_bin_idx2bin_coeff()` 函数**计算每个 bin（桶）的权重系数**，并存储在 `bin_coeff` 字典中。其核心逻辑是使用**二项分布概率质量函数（PMF，Probability Mass Function）**来计算不同 bin 之间的权重。

    * 理解其原理，理解 $k$ 和 $n$ 是如何选取的

    * 应用场景如下：
  
      **在重要节点识别中**，可能用于**根据 bin 的位置，为不同 bin 赋予不同的加权系数** 即衡量不同 bin 之间的**相对影响力**。
  
      **在对比学习或采样任务中**，可以作为权重分布，引导模型更关注某些 bin。
  
      通过利用二项分布的 **概率质量函数** 来衡量节点间（当然这里是 bin ）的相关性和影响力，尤其在一些 **图神经网络** 或 **基于距离的加权策略** 中得到广泛应用。
  
  * 进入 `return_imp_node_coeff()` 函数，**提取重要节点对应的系数**：根据 `imp_idx` 中的节点索引，找到每个节点对应的 bin 索引，进而提取每个节点所属 bin 的系数。**拼接系数**：将所有提取的系数拼接成一个大张量，并返回。
  
  * 处理自环问题并且更新 `edge_types` ，`rel_num` `n_edges`
  
  * 开始进行 模型 的预训练
  
    * 创建三个损失模块
  
    * 隐藏层维度为8，头为8，谓词嵌入的维度为10
  
    * 进入谓词嵌入 GAT 模型 `Predicate_GAT`（==**这个一定要掌握！！**==）
  
      * `in_dim = 64`, `hidden_dim = 8`, `out_dim = 64`, `rel_dim = 10`
  
    * 进入早停模块初始化
  
    * 进入 `return_centroids()` 函数: **计算一组节点嵌入（embeddings）的质心（centroid）**,之所以被称之为质心是因为它会分别计算每个 `bin` 中对应节点特征的均值，即为该 `bin` 的质心
  
      > **计算 bin 内所有节点的质心**（均值）。
      >
      > **存入 `bin2centroids`**，用于单独查询每个 bin 的质心。
      >
      > **拼接 `imp_bin_centroids`**，形成所有 bin 质心的矩阵。
      >
      > **跳过空 bin**，避免计算错误。
  

#### 理解两个损失函数

* 自己尝试写一下，要很熟悉矩阵以及张量操作
* 太猛了，大概懂了，先从损失函数入手，在训练时处理损失函数需要的数据
* 思考一下 $\text{Loss}_2$ 中的系数设置从直觉上以及数学上设计的合理性
  * 数学上和哲学上都是合理的，首先 $\text{Loss} = -\log \frac{A}{A+B}$ 就是一个恒大于 0 的数，当 $A >> B$ 的时候，损失越接近 $0$ ，系数 $\beta_{m,n}$ 的出现使得物理上距离更远的两个 `bin` 在数值得出的 `B`相较于 `A` 会更小，符合损失函数所要传达的意思。



#### 理解 licap 的时间复杂度





#### 理解实验 base line 的选取设置以及具体的超参设置

有三种方法

* 无监督方法：核心是 **PageRank（PR）**和**个性化 PageRank（PPR）**算法，它们在不依赖训练标签的情况下评估节点重要性。
  * 使用 `NetworkX` 包
* 非 GNN 的监督方法：**线性回归（LR）**和**多层感知机（MLP）**是两种经典的机器学习模型。由于节点重要性估计实际上是一个回归问题，所以它们可被应用于该任务。
  * 使用 `scikit-learn` 包
* 基于图神经网络（GNN）的监督方法包含两个子集，这些方法能够同时利用图的拓扑结构和节点特征。
  - **第一类：通用 GNN 模型**：是经过修改以适用于节点重要性估计问题的通用 GNN 模型。其中，选择 GCN（图卷积网络）作为谱域 GNN 模型的代表，它通过对图的拉普拉斯矩阵进行谱分解来学习节点表示；GraphSAGE（图采样与聚合编码器）作为空域 GNN 模型的代表，它通过采样和聚合邻居节点特征来生成节点嵌入；RGCN（关系图卷积网络）作为处理关系图数据的 GNN 模型代表，专门用于处理具有多种关系类型的图数据，能够更好地捕捉关系信息。
    - 使用 `DGL` 包并设置一个 64 维的 隐藏层
  - **第二类：特定于 NIE 的 GNN 模型**：采用 GENI 和 RGTN 这两种在知识图谱节点重要性估计任务中达到当前最优性能的模型进行比较和基准测试。GENI 利用图注意力机制和邻居分数聚合来推断节点重要性；RGTN 则使用关系图变压器，在学习过程中融入节点语义信息（即节点的文本描述）来进行节点重要性估计 。
    - 使用这个[仓库](https://github.com/GRAPH-0/RGTN-NIE)中的代码实现

| 超参数                | 取值范围或具体值             | 备注                                                        |
| --------------------- | ---------------------------- | ----------------------------------------------------------- |
| 重要比率 $\gamma$     | [0.01, 0.05, 0.1, 0.2]       | 用于LICAP中对节点进行分层采样（important-ratio）            |
| 损失平衡比率 $\eta_2$ | [0.5, 1, 10, 50]             | 与 $\eta_1$（值为1）一起平衡两种对比损失（loss-eta2）       |
| InfoNCE温度参数 $T$   | 0.05                         | 用于计算对比损失                                            |
| PreGAT或GAT隐藏层维度 | 8                            | 设置模型隐藏层维度                                          |
| PreGAT或GAT头数       | 8                            | 采用多头注意力机制的头数                                    |
| PreGAT谓词维度        | 10                           | 设置谓词嵌入的维度                                          |
| 学习率                | [0.0005, 0.001, 0.005, 0.01] | 对LICAP增强的NIE方法和原始NIE方法均进行搜索（weight-decay） |



| 数据集处理及评估相关设置 | 详情                                                         |
| ------------------------ | ------------------------------------------------------------ |
| 结构特征提取方法         | 使用Node2Vec从输入图中提取每个节点的结构特征                 |
| 语义特征获取方式         | FB15K和TMDB5K语义特征从RGTN GitHub库克隆；GA16K语义特征通过运行Transformer-XL获取 |
| 预训练结构特征维度       | 64                                                           |
| 预训练语义特征维度       | 128（实际上代码中给出的是768，好奇怪）                       |
| 分组箱长度               | FB15K：1.0；TMDB5K：0.5；GA16K：1.0                          |
| 评估方式                 | 五折交叉验证，报告回归指标和排序指标                         |



#### 理解实验的评估指标

在节点重要性估计的评估中，需要同时考虑重要性得分预测和重要性得分排序。因此，我们同时使用回归指标和排序指标，其正式定义如下。

-  **回归指标**：由于节点重要性估计问题的真实标签是非负实数，因此使用回归指标进行评估是很自然的。我们使用两个常用的回归指标，即均方根误差（RMSE）和平均绝对误差（MedianAE），来量化预测的重要性得分与相应真实标签的接近程度。值越低表示性能越好。    
  - RMSE衡量预测得分与真实得分之间的平均距离。假设有$n$个样本，用$s^{*}$和$s$分别表示预测得分和实际得分，其正式公式为：$RMSE =\sqrt{\frac{\sum_{i = 1}^{n}(s_{i}^{*} - s_{i})^{2}}{n}}$   
  - MedianAE衡量预测得分与真实得分之间绝对差异的中位数。与RMSE相比，它对异常值更具鲁棒性，因为它关注的是中位数预测误差，而不是所有样本的总体误差。MedianAE的正式定义如下：$MedianAE = median(|s_{1}^{*} - s_{1}|, \cdots, |s_{n}^{*} - s_{n}|)$ 
-  **排序指标**：为了从另一个角度评估性能，我们遵循文献[4]，额外采用两个排序指标：归一化折损累计增益（NDCG）和斯皮尔曼等级相关系数（SPEARMAN）。值越高表示性能越好。    
  - NDCG衡量排序列表的质量，它是折损累计增益（DCG）的扩展。DCG同时考虑了项目的相关性及其在排序中的位置。在根据预测得分$s^{*}$对具有真实得分$s$的项目列表进行排序后，用$s_{r}$表示第$r$个项目的真实得分。位置$k$处的DCG（DCG@k）[4]可以定义为：$DCG @ k=\sum_{r = 1}^{k} \frac{s_{r}}{\log _{2}(r + 1)}$    
  -  NDCG通过将DCG除以理想DCG（IDCG）进一步进行归一化。位置$k$处的IDCG（IDCG@k）是前$k$个位置中可能的最大DCG。根据这些定义，位置$k$处的NDCG（NDCG@k）可以通过以下公式计算：$NDCG @ k=\frac{DCG @ k}{IDCG@ k}$   
  -  SPEARMAN是衡量预测得分$s^{*}$与真实得分$s$之间等级相关系数的指标。在将得分$s^{*}$和$s$分别转换为相应的等级$r^{*}$和$r$后，我们将$r^{*}$和$r$的平均值定义为$\bar{r}^{*}$和$\bar{r}$。SPEARMAN可以表示为：$SPEARMAN =\frac{\sum_{r}(r^{*}-\overline{r^{*}})(r-\overline{r})}{\sqrt{\sum_{r}(r^{*}-r^{*})^{2}} \sqrt{\sum_{r}(r-\overline{r})^{2}}}$    
  -  OVER是预测的重要节点与真实重要节点之间的重叠率。用$S_{top - k }^{*}$表示预测的前$k$个重要节点的集合，$S_{top - k}$表示真实的前$k$个重要节点的集合。$|·|$表示集合的基数。OVER@k的计算公式为：$OVER @ k=\frac{|S_{top - k}^{*} \cap S_{top - k}|}{k}$ 





#### 理解实验的具体内容

**研究问题设定**  
提出7个研究问题，涵盖LICAP对提升最先进NIE方法性能的作用、关键组件的有效性、预训练不同特征的效果、超参数影响、对高重要性节点的感知能力以及对其他NIE方法性能的提升效果等方面。这些问题为后续实验和分析提供了明确方向。

**知识图谱上的节点重要性估计**  
对比LICAP与多种基线方法的性能。结果显示，LICAP与RGTN结合在所有指标上均优于基线方法。  
- 无监督方法（如PR和PPR）由于无法利用标签进行监督，性能较差；  
- GNN-基于方法能够同时利用节点特征和图结构，表现更优。  
在基线方法中，RGTN表现突出，但LICAP能显著提升RGTN在回归指标上的性能，充分展现了LICAP的优势。

**消融研究：LICAP对比损失的影响**  
通过构建基于RGTN的LICAP变体进行对比实验，验证了两个对比损失的有效性。  
实验结果表明，同时包含 $L_1$ 和 $L_2$ 损失的LICAP - $L_1 + L_2$ 表现最佳。  

- 进一步分析发现，$L_1$ 单独作用时效果通常优于 $L_2$，这是因为 $L_2$ 单独使用时过于关注细节，使得学习过程困难。  
- 此外，随机采样的LICAP - r.d.方法效果较差，这证明了顶部节点优先采样策略的有效性，突出了利用先验知识对提升NIE方法性能的重要性。

**消融研究：谓词感知GAT的影响**  
比较LICAP - PreGAT、LICAP - GAT和原始RGTN的性能。  
研究发现LICAP - PreGAT性能最优，尤其在具有更多谓词的FB15K数据集上优势更为明显。这表明谓词感知机制能够有效区分不同的谓词，捕捉其中有意义的信息，从而提升模型性能。如果在谓词数量较少的知识图上，则 PreGAT 将近退化为 GAT，效果可能并不好（**但事实上，我发现即便在谓词数量更多的 fb15k 数据集上，PreGAT的提升也并不明显...**）

**预训练不同输入通道**  
探究LICAP对不同输入通道（结构特征、语义特征、两者结合）的预训练效果。  

RGTN明确地将**结构特征**和**语义特征**视为两个单独的通道。

研究结果显示，单独预训练结构或语义特征通常比同时预训练两者效果好。这可能是因为RGTN的两个输入通道需要一定的差异性，以更好地利用多源特征。  

这表明按照相同的 LICAP 架构同时预训练结构和语义嵌入，可能会损害模型的学习能力。原因可能在于，RGTN 的两个输入通道需要在一定程度上具有差异性，**这样才有利于模型充分利用多源特征**。而使用相同的 LICAP 架构对两者进行预训练，**可能导致两个通道的嵌入特征过于相似，无法有效发挥多源特征的优势**，进而影响模型学习效果。

尽管如此，与原始方法相比，三种变体都能提升性能，说明LICAP在不同输入通道设置下均能发挥积极作用。

**超参数分析**  
分析损失平衡比率 $\eta_{2}/\eta_{1}$ 和 重要比率 $\gamma$ 对模型性能的影响。  
研究发现，排序指标 SPEARMAN 比回归指标 RMSE 对超参数更鲁棒。  

- **排序对预测异常值相对不敏感**，而RMSE作为回归指标受异常值影响较大。  
- 此外，不同数据集的最佳 $\gamma$ 值不同，**这是由于各数据集在图结构和知识图谱主题等方面存在差异。**

**可视化LICAP预训练嵌入**  
通过可视化LICAP预训练前后的节点嵌入，**直观地发现预训练后高重要性节点在空间中更聚集。这一现象表明LICAP能更好地感知高重要性节点，实现了研究的预期目标，进一步证明了LICAP的有效性。**

**LICAP应用于多种方法**  
将LICAP与更多NIE方法结合进行实验。结果显示，**LICAP能提升MLP的性能，但对LR的效果不稳定**，可能是因为LR的线性假设与知识图谱上的NIE问题兼容性不佳，而MLP的非线性激活函数能更好地与LICAP协作。  
对于GNN-基于的方法（如GCN、RGCN、GraphSAGE和GENI），LICAP也有不同程度的性能提升。  （==个人感觉这里说得太牵强了，实验效果其实并没有那么好==）

- 其中，RGCN和GENI与LICAP结合后在不同数据集上的表现相对更优，**这是因为LICAP考虑了关系信息，与同样关注关系的RGCN和GENI更适配。**  
- 此外，LICAP对非GNN方法的性能提升幅度相对更大，但结合LICAP的GNN方法凭借GNN骨干网络在图数据处理上的优势，最终仍取得更好的结果。



#### 作者对未来工作的展望

对于未来的工作

* 将 LICAP 扩展到动态图上的 NIE 问题是一个有趣的研究方向。
* 此外，研究标签分布如何影响 LICAP 和 NIE 方法的性能也很有价值。
* 另一个有前景的方向是将 LICAP 的思想迁移到其他与图相关的回归或排名问题中。



#### 实验的复现

* 感觉是数据造假

### FB15K

| Methods    | RMSE ↓ | MedianAE ↓ | NDCG@100 ↑ | SPEARMAN ↑ | OVER@100 ↑ |
| ---------- | ------ | ---------- | ---------- | ---------- | ---------- |
| PR         |        |            |            |            |            |
| PPR        |        |            |            |            |            |
| LR         |        |            |            |            |            |
| MLP        |        |            |            |            |            |
| GCN        |        |            |            |            |            |
| RGCN       |        |            |            |            |            |
| GraphSAGE  |        |            |            |            |            |
| GENI       |        |            |            |            |            |
| RGTN       |        |            |            |            |            |
| RGTN+LICAP |        |            |            |            |            |

### TMDB5K

| Methods    | RMSE ↓ | MedianAE ↓ | NDCG@100 ↑ | SPEARMAN ↑ | OVER@100 ↑ |
| ---------- | ------ | ---------- | ---------- | ---------- | ---------- |
| PR         |        |            |            |            |            |
| PPR        |        |            |            |            |            |
| LR         |        |            |            |            |            |
| MLP        |        |            |            |            |            |
| GCN        |        |            |            |            |            |
| RGCN       |        |            |            |            |            |
| GraphSAGE  |        |            |            |            |            |
| GENI       |        |            |            |            |            |
| RGTN       |        |            |            |            |            |
| RGTN+LICAP |        |            |            |            |            |





#### 一些工具或者技巧

* 针对真实世界KG的标签（如FB15K的页面浏览量和GA16K的引用计数），作者对其进行了对数转换（猜测是数据太大了，后面的损失函数与生成嵌入要直接用到它，不能使用过大的数据？？）==这个要深刻理解==

* 在计算 $L_1$ 损失的时候，代码中将 `embed_important` 与 `embed_normal`  （分别对应了论文中公式（1）的 $h_i$ 与 $h_j$）进行了归一化操作，即 

  ```python
  embed_important = F.normalize(embed_important)
  embed_normal = F.normalize(embed_normal)
  ```

  这是为了防止后续在进行 `torch.exp()`  操作的同时指数爆炸，经过实验直接变成 `nan` 了（这是因为分母或分子为 `inf` 导致结果为 `nan`）


* 设置随机种子

* 简单早停模块

* 梯度裁剪

  * 防止梯度爆炸（**在反向传播时使用！！**），用于提高训练稳定性，常用于深度网

    * 需要在 `loss.backward()` 之后、`optimizer.step()` 之前执行，而不是在 `forward` 里执行。

  * `torch.nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = 10, norm_type = 2)`

    > 梯度裁剪的缩放公式如下：
    >
    > $g_i = g_i \times \frac{\text{max\_norm}}{\|\mathbf{g}\|_2}$
    >
    > 其中：
    >
    > * $g_i$ 是梯度的某个分量。
    >   
    > * $\|\mathbf{g}\|_2$ 是所有梯度的 **L2 范数**（即欧几里得范数）：
    >   
    >   $\|\mathbf{g}\|_2 = \sqrt{\sum_i g_i^2}$
    >
    > * **如果** $\|\mathbf{g}\|_2 \leq \text{max\_norm}$，则不进行缩放。
    >
    > * **如果** $\|\mathbf{g}\|_2 > \text{max\_norm}$，则所有梯度按照这个比例缩放，使得梯度范数不会超过 `max_norm`。

