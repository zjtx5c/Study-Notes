## 论文复现日志

### 前言

立志成为会造轮子的掉包侠，会写丹方的炼丹师，全栈精通的开发者



#### 如何想 idea？

[李沐分享](https://www.bilibili.com/video/BV1ea41127Bq?buvid=XYC6E55C7A1288F6E616688AAB8E564D963BF&from_spmid=main.space-contribution.0.0&is_story_h5=false&mid=zsw0awYQcKQ0Z8%2FLSPc5OQ%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=01e2a6f7-c6e5-4a5a-98ee-d4f813b3639d&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1723199264&unique_k=d7FYToH&up_id=1567748478&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

$\text{研究价值} = \text{新意度} \times \text{有效性} \times \text{问题大小}$

* 写论文要追求什么？

1. 深刻，你是否揭示了一些比较本质的东西
2. 优美，你的论文的证明与描述是否有美感

* 如何定义有新意

1. 不要去堆砌无谓的复杂性，模型可以越简单越好
2. 论文的作者必须要呕心沥血才能发表文章？

##### 随便想想的 idea

idea：在对比学习的基础上，考虑将网络做深，从多个GNN模型中学习知识并通过，结合 cka 以及蒸馏的方法不断对上游目标进行矫



#### 项目结构的设置

* 项目结构：一般都是工程上的最佳实践，需要多看别人的代码自己总结。可以参考知乎的[这篇](https://www.zhihu.com/question/406133826)
  常用的一个结构可以是这样

  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----models/：模型
  --------model.py
  ----configs/：配置文件
  --------config.py
  ----datasets/：加载数据
  --------data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```

  若项目比较简单，也可以是这样
  ```
  --project_name/
  ----data/：数据
  ----checkpoints/：保存训练好的模型
  ----logs/：日志
  ----model_hub/：预训练模型权重
  --------chinese-bert-wwm-ext/：
  ----utils/：辅助模块，可以是日志、评价指标计算等等
  --------utils.py
  --------metrics.py
  ----model.py
  ----config.py
  ----data_loader.py
  ----main.py：主程序，包含训练、验证、测试和预测
  
  作者：西西嘛呦
  链接：https://www.zhihu.com/question/406133826/answer/2898344659
  来源：知乎
  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
  ```

### 总结

#### 读取与存放文件

* 理解序列化与反序列化

  `pickle` 是一个**用于序列化和反序列化 Python 对象的模块**。**序列化是将 Python 对象转换为字节流（例如，存储在文件中），而反序列化是将字节流重新转换为 Python 对象。**

* `pickle.load()` 与 `pickle.dump()`

> `pk.load`（通常是 `pickle.load` 的简写）是与 `pickle.dump` 对应的操作，它用于从文件中**反序列化数据**，**将存储在文件中的字节流转换回 Python 对象。**
>
> 具体来说，`pickle.load` 从一个已经打开的文件中读取数据，并返回反序列化后的 Python 对象。示例如下：
>
> ```python
> with open('file.pkl', 'rb') as f:
>     obj = pickle.load(f)
> ```
>
> 在这个例子中，`file.pkl` 是一个包含序列化数据的文件，`pickle.load(f)` 会将文件中的字节流读取并转换为原来的 Python 对象，保存在 `obj` 中。这里 `rb` 表示以二进制读取模式打开文件。
>
> ___
>
> 
>
> `pk.dump` 是 `pickle.dump` 的简写，它将一个对象序列化并写入一个文件或类似文件的对象中。语法如下：
>
> ```
> python复制编辑import pickle
> 
> with open('file.pkl', 'wb') as f:
>     pickle.dump(obj, f)
> ```
>
> 在这个例子中，`obj` 是你想要序列化的 Python 对象，`f` 是一个已经打开的文件对象（以**二进制写模式** `'wb'` 打开）。`pickle.dump` 会将 `obj` 写入文件 `file.pkl` 中。
>
> `pickle` 模块通常用于**将模型、数据结构等保存在磁盘上，以便后续加载和使用。**



* 关于 `.pt` 文件与 `.pkl` 文件

> **区别总结**
>
> 1. **用途**：
>    - `.pt` 主要用于 PyTorch 模型和张量的保存与加载。
>    - `.pkl` 用于 Python 中广泛的对象序列化，包括非 PyTorch 对象。
> 2. **文件格式和方法**：
>    - `.pt` 是通过 `torch.save` 和 `torch.load` 保存和加载的，**专为 PyTorch 对象设计。**
>    - `.pkl` 是通过 `pickle.dump` 和 `pickle.load` 保存和加载的，**通用于 Python 对象。**
> 3. **兼容性**：
>    - `.pt` 适用于 PyTorch 环境，更能保证 PyTorch 模型的兼容性。
>    - `.pkl` 是更通用的文件格式，适用于任何 Python 对象，但不专门优化处理 PyTorch 模型。
>
> 简而言之，`.pt` 是 PyTorch 特有的文件格式，而 `.pkl` 是 Python 的通用序列化格式。

#### 设置随机种子

在 `PyTorch` 及 `DGL` 相关的代码中，为了确保实验的可复现性，一般需要在以下几个地方设置随机种子

（1）python 内置库（2）Numpy（3）Pytorch（4）PyTorch 计算相关（5）DGL（6）操作系统

```python
def set_random_seed(seed=0):
    '''
    设置随机种子，以保证实验的可复现性
    :param seed: int, 要使用的随机种子
    :return: None
    '''
    # Python 内置随机库
    random.seed(seed)

    # NumPy 随机种子
    np.random.seed(seed)

    # 设置 Python 运行时的哈希种子（影响某些哈希操作的随机性）
    os.environ['PYTHONHASHSEED'] = str(seed)

    # PyTorch CPU 端随机数
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        # 设置当前 GPU 的随机数种子
        torch.cuda.manual_seed(seed)
        # 设置所有可用 GPU 的随机数种子
        torch.cuda.manual_seed_all(seed)

        # 让 CuDNN 以确定性方式执行计算，以确保结果可复现
        torch.backends.cudnn.deterministic = True
        # 关闭 CuDNN 的自动优化算法，以防止影响结果可复现性
        torch.backends.cudnn.benchmark = False
        # 禁用 CuDNN 加速，以最大程度保证实验可复现
        torch.backends.cudnn.enabled = False

    # DGL 相关随机种子
    dgl.random.seed(seed)

```

#### 设置最佳模型路径

* 需要用到的函数

```python
os.path.join()
os.path.exists()
'_'.join([f'{k}{v}' for k, v in parameters_dic.items()])
```



一般最佳模型都是保存在 `ckpt` 文件夹下，比如说 `project_name/ckpt/...` ，若有预训练模型，则保存在对应的预训练的文件夹下则 `preject_name/pretrain/ckpt/...`。若要训练多个模型，则可以这样 `project_name/ckpt/mode1/...` 或者 `project_name/pre_train/ckpt/model1/...`

```
--project_name
----ckpt
------model1
------model2
```

```
--project_name
----pretrain
------ckpt
--------model1
--------model2
```

后面的路径可以根据自己的超参数自己添加，比如早停忍耐轮次，重要性节点的采样比率等等。最后一层可以放上最后的数据集，如下所示

```
--project_name
----pretrain
------ckpt
--------model1
----------imp_ratio_0.1
------------patience_20
--------------fb15k
```

使用字符串配合 `args.` 解析命令参数来创建文件夹。具体上实现的思路是：分别写出每一层的**相对目录**，**最后拼接上去**，以我们上述举的例子为例

比如我们现在要保存预训练的模型 `model1` , 重要性节点的采样比率`imp_ration` 设置为0.1，早停忍耐轮次`patience` 设定为20。那么我们可以这样
`pretrain_root = 'pretrain/ckpt/'`  **（0）先设置根目录**

`model_path = str(args.model) + '/'`		**（1）设置模型相对路径**

`imp_ratio_path = 'imp_ratio_' + str(args.imp_ratio) + '/'`		**（2）设置节点重要性相对路径**

`patient_path = 'patience_' + str(args.patience) + '/'`				**（3）设置早停忍耐轮次相对路径**

`dataset_path = dataset_name + '/'` 			**（4）设置数据集相对路径**

最后将整个路径拼接起来：

```python
out_save_path = os.path.join(pretrain_root, model_path, imp_ratio_path, patience_path, dataset_path)
if not os.path.exists(out_save_path):
    os.makedirs(out_save_path, exist_ok = True)
```



这种方法，当参数过多是可能会很长。因此，此基础上，我们可以将其他的超参数（比如 `lr`， 交叉验证的轮次 `cross_id`， 损失 `loss` 的系数）添加到最终的模型名中为模型命名如：

`tmp = dataset_name + '_struct_pregat_pretrained_lr' + str(args.lr) + '_loss_' + str(args.eta) + '_' + str(cross_id) + '.pkl' `

`out_pretrained_path = os.path.join(out_save_path, tmp)`

上述这种方法较为冗长，我们可以**使用字典管理参数**

```python
# 定义模型文件名的关键参数
model_name_parts = {
    "dataset": dataset_name,
    "model": "pregat_pretrained",
    "lr": args.lr,
    "loss": args.eta,
    "cross": cross_id
}

# 生成文件名（去掉值为 None 或空字符串的参数）
model_filename = "_".join([f"{k}{v}" for k, v in model_name_parts.items() if v is not None]) + ".pkl"

# 最终模型保存路径
out_pretrained_path = os.path.join(out_save_path, model_filename)
```



#### 设置 cuda 并将数据传到 cuda

* 需要用到的函数

```python
# 设置 GPU 编号
torch.cuda.set_device()	# 这里可以填 cuda = 0, cuda = 1, 是说用机器的第几块 GPU

# 将张量移动到 cuda 上
.cuda()		# 只能移到默认 GPU（cuda:0） 或你手动指定的 GPU。
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
.to(device)	# 更加通用
```

* 需要思考有哪些数据要放到 cuda 上

  * 模型的参数：模型的参数（如权重、偏置等）应该放在 GPU 上进行计算和更新。尤其在训练过程中，这些参数会经过反向传播并需要在 GPU 上更新。
    ```python
    model.to(device)  # 将模型移到 GPU 或 CPU，device 是 "cuda" 或 "cpu"
    ```

    

  * 训练数据：训练数据（例如**输入特征、标签**等）也需要放到 GPU 上，特别是当训练过程需要频繁地在 GPU 和 CPU 之间传输数据时，可能会成为性能瓶颈。**所有输入数据（如图片、文本数据）和对应的标签都应被放在 GPU 上**，以提高训练效率。

    ```python
    inputs = inputs.to(device)  # 使用 .to() 更通用
    labels = labels.to(device)  # 使用 .to() 更通用
    ```

    还有比如说与我目前方向相关的图数据：

    ```python
    g = g.int().to(device)
    ```

    

  * 损失函数中的相关数据：损失函数中的相关数据（例如，模型预测的输出和真实标签）也应放在 GPU 上进行计算。否则，如果损失函数中的数据仍然在 CPU 上，PyTorch 会自动将它们传输到 GPU，但这会导致性能损失。

  * 优化器的状态（通常由优化器自动管理）：优化器（如 `Adam`, `SGD`）本身的参数通常不会显式地放到 GPU 上，因为它会自动跟踪模型参数的梯度。然而，**如果你自己实现了优化器，或优化器的状态变量需要与模型参数一起计算，** 这些状态（例如动量等）也需要放到 GPU 上。例如，在使用 `torch.optim` 时，优化器会**自动处理 GPU 迁移**：
    ```python
    optimizer = torch.optim.Adam(model.parameters())  # 优化器会自动使用模型所在设备的参数
    ```

#### 交叉验证

* 框架如下

```python
for cross_id in range(args.cross_num):
    '''
    中间处理
    包括预处理、模型保存路径设置、其他提取数据、数据分割
    cuda 设置
    '''
   if cross_id == 0:
            print('---------Dataset Statistics---------')
            print('一些数据信息')
            print('-------------Pretraining------------')
        
        print(f'cross: {cross_id}')
        print('------------')
        
        print('Model Pretraining')
        
        '''
        其他处理
        '''
    
```

#### 早停机制（用类实现）

* 最基础的需要记录 7 项

  (1) early_stop (2) patience (3) counter (4) best_score (5) best_epoch (6) save_path (7) min_epoch

* 具体实现

```python
class EarlyStopping_simple:
    def __init__(self, patience=50, save_path=None, min_epoch=-1):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = None
        self.save_path = save_path
        self.min_epoch = min_epoch

    def step(self, acc, epoch, model):
        score = acc
        if epoch < self.min_epoch:
            return self.early_stop
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            self.save_checkpoint(model)
        elif score < self.best_score:
            self.counter += 1         
            if self.counter >= self.patience:
                self.early_stop = True
                print()
                print(f'EarlyStop at Epoch {epoch} with patience {self.patience}')
        else:
            self.best_score = score
            self.best_epoch = epoch
            self.save_checkpoint(model)
            self.counter = 0
        return self.early_stop

    def save_checkpoint(self, model):
        '''Saves model when validation loss decrease.'''
        torch.save(model.state_dict(), self.save_path)
```

### 经验

#### 损失函数

* 最终得到的这个标量损失最后一步**一般是由一组向量推来**，**一般情况下我们使用 `.mean` 来得到损失而不是用 `.sum` 得到损失**（注意，我们一般情况下不对向量进行反向传播，即我们要门对其做 `.mean` 处理，要么对其做 `.sum` 处理）

  * 思考为什么

  > （1）保持梯度稳定性
  >
  > 如果使用 `.sum()`，那么损失的大小会随 batch size 增大而增大，导致梯度也变大。例如：
  >
  > ```python
  > import torch
  > 
  > x = torch.randn(10, requires_grad=True)  # 10个样本
  > loss = x**2  # 每个样本的损失
  > loss_sum = loss.sum()  # 直接求和
  > loss_mean = loss.mean()  # 计算均值
  > 
  > loss_sum.backward()  # 计算梯度
  > print(x.grad)  # 梯度较大
  > 
  > x.grad.zero_()  # 清空梯度
  > 
  > loss_mean.backward()  # 计算梯度
  > print(x.grad)  # 梯度较小，数值稳定
  > ```
  >
  > **现象：** `loss_sum` 的梯度比 `loss_mean` 大 **10 倍** （即 `batch_size` 倍），因为 `.sum()` 直接把所有样本的损失加起来，导致梯度变大。
  >
  > 如果 batch size 变化，梯度的大小也会变得不稳定，而 `.mean()` 可以保持梯度在一个稳定的范围内，使得学习率不需要调整。
  >
  > （2）与学习率无关
  >
  > 在梯度下降中，学习率 (`lr`) 决定了参数更新的步长。如果我们使用 `.sum()`，梯度的数值会随着 batch size 改变，导致学习率需要调整：
  >
  > - 用 `.sum()`，如果 batch size 变大，梯度变大，可能导致梯度爆炸，**需要手动缩小学习率**。
  > - 用 `.mean()`，梯度大小不变，学习率可以保持一致，不需要频繁调整。
  >
  > 因此，`.mean()` 使得**同样的学习率适用于不同的 batch size**，从而提高了模型的稳定性和可移植性。
  >
  > （3）使不同 batch 的损失具有可比性
  >
  > 假设我们有两个 batch，batch size 分别是 32 和 16，如果使用 `.sum()`，那么 batch1 的总损失比 batch2 的损失大了一倍，这不合理，因为两个 batch 的样本**平均损失是一样的**。但如果使用 `.mean()`，尽管两个 `batch` 的 `batch_size` 不一样，但是它们的梯度却没有与 `batch_size` 产生强线性关系，使得模型训练更稳定。

### 	LICAP

与以往知识图谱上的 NIE 方法不同，这项工作引入了现实世界中对重要节点（即具有高重要性分数的节点）偏好的潜在先验知识。**为了注入这种先验知识，提出了一种新颖的采样策略，结合对比学习，利用可用的节点重要性分数预训练节点嵌入**。还需注意的是，所提出的 LICAP 是一种**插件式方法**，可以集成到以前的 NIE 方法中，而不是一种新的特定 NIE 方法。

创新点：

1. 提升了 `embedding` 的质量

2. label 感知分组将连续分数划分成有序区间以**生成对比样本**；分箱机制，**顶层节点优先 分层采样机制**

   这么做的其中一个原因是，我们完全不需要在预训练阶段就估计节点重要性，这一任务将在下游实现；我们在预训练阶段的目的仅是为了得到更好的嵌入表

   这样一来，原本并非为回归问题设计的对比学习，现在可以采用对比样本对嵌入进行预训练。

3. 谓词 GNN
4. 通过联合最小化两个对比损失优化节点嵌入。



细节：

1. 顶层节点优先，分层采样机制

   top-bin 与 non-top-bin 根据 `important ratio`  $\gamma$ 划分，代码中是取 0.1

   ==理解一下 InfoNCE 损失函数==

2. 划分为第一级对比样本与第二级对比样本（分别对应 $L_1$ 和 $L_2$ 的 loss）

   在顶层区间中，将每个节点与顶层原型（即图 3 中的三角形）进行正对比。顶层原型是通过对所有顶层节点的嵌入进行元素级平均得到的。对于负对比，我们将从非顶层区间中随机采样的非顶层节点，作为每个顶层节点的负样本对。

   对于更细区间中的每个顶级节点（这里的更细区间是指将**顶层区间**划分为更细区间），其正对比样本被定义为该更细区间的原型（例如，图 4 中包含红色节点的更细区间的红色三角形），这同样是通过对该更细区间内节点的嵌入进行元素级平均得到的。至于负对比样本，它们分别也是其余更细区间的原型，我们设计了一种重新加权负采样机制，以保持这些更细区间之间的相对顺序。这种重新加权负采样机制基于这样一种直觉：来自距离更远区间的样本更不相似（或更不接近）。因此，我们引入一个接近系数 $\beta$ 来对负对比对进行重新加权，使得接近系数越大，相应的负样本对项在分母中的影响就越大。

   第一级对比样本和 $L_1$ 损失旨在更好地将顶层节点与非顶层节点区分开来，而第二级对比样本和 $L_2$ 损失则试图通过保持更细区间之间的相对顺序，来更好地区分顶层区间内的顶层节点。





疑惑点：

1. 这么做对比学习的目的是什么？

2. 这个接近系数 $\beta$ 到底有什么用？为什么要引入它，是用来区分什么的
   个人感觉是，正样本对都在同一层，所以正样本对之间是没有差异的，但是负样本对在不同层，要体现层与层之间的差异，用一个接近系数 $\beta$ 来衡量

3. 为什么要设计 $L_1$ 和 $L_2$ 损失，如果只是想找更优质的节点，直接降序排序得到不就行了吗？这么做的意义是什么？其更深层的原理和机制是什么。论文中提到要通过使用InfoNCE损失来结合对顶级节点偏好的先验知识来预训练节点嵌入，如何理解？我到现在还是无法理解这种损失到底是在做什么？？（还是得还之前 `deepwalk` 的账啊）

   突然之间秒懂，是要根据损失去调整嵌入特征 $\mathbf{h}_i$， 获得更优质的嵌入特征！比如获得的特征能够更好地区分 top-bin 中地节点和 non-top-bin 中的节点；同时也能更好地区分 top-bin 中其他不同层之间的节点，**通过对比学习和结合先验知识，生成更优质的节点嵌入，为后续的节点重要性估计任务提供更好的基础。**



#### 预备知识点

* 预训练范式

  * > 预训练范式（Pre-training Paradigm）是深度学习领域的一种模型训练方法，核心思想是通过**分阶段训练**来提升模型性能：
    > **先在大规模通用数据上进行无监督或自监督的预训练**，学习通用特征表示；**再在特定任务的小规模数据上进行微调**（Fine-tuning），适应下游任务。这一范式显著降低了数据标注成本，推动了自然语言处理（NLP）、计算机视觉（CV）等领域的突破。

* 对比学习，无监督学习包括对比式学习与生成式学习，**对比学习是无监督学习的一种**

  * > 在重要节点识别任务中，对比学习的主要思想是：
    >
    > 1. **构造正样本（Positive Sample）**：通常是**同一节点的不同视图**，或者**同类别的重要节点**。
    > 2. **构造负样本（Negative Sample）**：通常是**普通节点**，或者**不同类别的节点**。
    > 3. **优化目标**：让重要节点的表示尽可能靠近，而普通节点远离它们，以便更好地区分节点的重要性。
    >
    > 常见的方法包括：
    >
    > - **基于全局结构的对比**：让重要节点在全局网络中保持一致性。
    > - **基于局部子图的对比**：让重要节点的局部结构与其重要性保持一致。
    > - **基于动态变化的对比**：在动态图中，学习重要节点随时间变化的稳定性。
    
  * InfoNCE 将一个正样本与多个负样本进行区分，已成为更受欢迎的对比学习损失函数。该文章在节点重要性估计（NIE）问题中采用对比学习，通过使用 InfoNCE 损失函数，融入对重要节点（top nodes）偏好的先验知识，来预训练节点嵌入。

* 二项分布概率质量函数

#### 理解数据

##### `fe15k_rel.pk`

* `fb15k_rel.pk`：这是一个知识图谱数据，表示**图谱的结构特征**，以字典的形式保存，涵盖了 `edges: tuple`， `edge_type: tensor`, `labels: tensor`, `feature: tensor`, `invalid_masks: tensor`

  具体的形式如下
  ```python
  {'edges': (tensor([ 345, 9796,  848,  ..., 5338, 4277, 1098]),
    tensor([10667,  1985,  6425,  ..., 12077, 11101, 13898])),
   'edge_types': tensor([  0,   1,   2,  ...,   9,  26, 166]),
   'labels': tensor([ 9504., 10199.,  9030.,  ..., 41222., 31642., 44935.]),
   'features': tensor([[-0.2106,  0.3095,  0.6127,  ..., -0.0039, -0.2529,  0.1002],
           [-0.2618,  0.6114, -0.2634,  ...,  0.3543, -0.0417, -0.0791],
           [ 0.1157,  0.0885, -0.1008,  ...,  0.1172,  0.3553, -0.1994],
           ...,
           [ 0.2731,  0.4277,  0.4038,  ...,  0.4685,  0.4484,  0.4826],
           [-0.2515, -0.2345,  0.9372,  ..., -0.1647,  0.3712, -0.0693],
           [ 0.2306, -0.1211, -0.1945,  ...,  0.5564,  0.0363, -0.1344]]),
   'invalid_masks': tensor([0, 0, 0,  ..., 0, 0, 0])}
  ```

  * 节点数量为 14,951，边的数量为 592,213，谓词种类为 1,345，有将近 14,105 (94.3%) 的节点是有节点特征的（64维嵌入）
  * `edges`: `tuple` ，表示源点与汇点的索引
  * `edge_types: tensor`，表示每条边的**谓词种类**
  * `labels: tensor`，表示节点重要性（node importance）的得分，通过从维基百科对收集到的真实世界重要性值（如FB15K的页面浏览量）进行对数转换来获得。（有时间去了解一下具体是怎么收集的）
  * `features：tensor  (num_nodes, 64)`，表示每个节点的词嵌入特征 ，通过 `node2vec` 算法得到节点特征
  * `invalid_masks`, 指示哪些**节点**或边需要被忽略或排除在计算之外。经过验证 `invalid_masks` 为 `1` 的节点其 `labels` 为 `0`, 我们不使用这些数据进行训练

##### `fb_lang.pk`

* `fb_lang.pk` ：这是一个仅关于节点的数据，表示**图谱节点的语义特征**，是一个二维 `np.array` 
  具体形式如下

  ```python
  array([[-0.07588089, -0.15141135, -0.13396032, ..., -0.08600746,
          -0.17194201,  0.53592861],
         [-0.02278661, -0.29997835, -0.0073817 , ...,  0.02436821,
          -0.02998137,  0.46098486],
         [-0.15221825, -0.06115236, -0.34801662, ..., -0.12346848,
          -0.14316572,  0.39701506],
         ...,
         [ 0.01512893,  0.186928  , -0.18825455, ..., -0.17741039,
          -0.02106529,  0.62954003],
         [-0.21668366, -0.01483662, -0.10437213, ...,  0.15704495,
          -0.13830011,  0.55147076],
         [-0.05817803,  0.12917031, -0.13514727, ..., -0.02458374,
          -0.02240661,  0.38673902]])
  ```

  论文中提到其通过 Transformer-XL 生成，维度为 **128**。但是实际数据上却是768 维？语义信息通常通过自然语言处理技术（如Transformer-XL）从节点的文本描述中提取，用于补充知识图谱的结构信息（如节点间的连接关系）。例如，在电影知识图谱（TMDB5K）中，语义信息可能包括电影的剧情简介、演员或导演的文本描述；在学术知识图谱（GA16K）中，可能是论文的标题或摘要。

  **语义信息和结构信息被作为两个独立的输入通道（例如在 RGTN 模型中），共同用于节点重要性估计任务。**



#### 理解预处训练过程

* 可能需要用到的函数

```python
np.round()
np.where()		# 注意返回的是一个 `tuple` 需要使用 [0] 进行索引
np.intersect1d(arr1, arr2, assume_unique = True)	# assume_unique = True 表示假设arr1和arr2都是唯一的，可以加速计算
np.random.choice(arr, size)

torch.sort()	# 两个返回值，分别是值和索引
```



* 我将学习到交叉验证、早停机制、划分正例样本与负例样本、分桶机制

* 设置参数与超参数，设置随机种子

* 设置各种保存路径，一般都会保存在 `checkpoints` 中

* 开始进入交叉验证大循环：
  * 进入 `load_data()` 函数开始装载与提取数据（涉及三种情况（1）只提取节点语义特征（2）提取两者特征并将它们拼接（3）提取两者特征但不将它们拼接）
    * 提取出对应的特征、掩码等数据
    * 处理节点的入度：对其进行 norm 处理（归一化因子），然后将归一化因子保存到**图的节点数据中**（`g.ndata['norm'] = norm`），并使用 `g.apply_edges()` 将每一个汇点的 `norm` 添加到对应的边上。
    * 标签的对数变换
    * 数据集划分（70% for train, 10% for val, 20% for test）
      * 五折交叉验证，==**这个操作手法需要学习一下（交叉验证）**==
    * 返回 `hg, edge_types, edge_norm, rel_num, node_feats, labels, train_idx, val_idx, test_idx`

  * 进入 `find_imp_idx()` 函数寻找重要节点（这是该论文 `LICAP` 的核心概念，用于指导**对比学习**的样本生成策略）。==**这里的操作手法需要学习一下（划分正负例样本）**==

    * 这里最后刻意将 `normal` 按照节点 50 % 采样。（从代码中看， normal 节点的数量大概是 important 节点数量的两倍）

  * 进入 `et_imp_bin_idx2node_idx()` 函数，根据节点标签的数值范围，将目标节点 `target_idx` 分桶，并记录哪些桶是有效的。其主要作用是**对重要节点进行分段索引**，可能用于后续的分析或采样。==**这里的操作手法需要学习一下（分桶操作）**==

  * 进入 `return_bin_idx2bin_coeff()` 函数**计算每个 bin（桶）的权重系数**，并存储在 `bin_coeff` 字典中。其核心逻辑是使用**二项分布概率质量函数（PMF，Probability Mass Function）**来计算不同 bin 之间的权重。

    * 理解其原理，理解 $k$ 和 $n$ 是如何选取的

    * 应用场景如下：
  
      **在重要节点识别中**，可能用于**根据 bin 的位置，为不同 bin 赋予不同的加权系数** 即衡量不同 bin 之间的**相对影响力**。
  
      **在对比学习或采样任务中**，可以作为权重分布，引导模型更关注某些 bin。
  
      通过利用二项分布的 **概率质量函数** 来衡量节点间（当然这里是 bin ）的相关性和影响力，尤其在一些 **图神经网络** 或 **基于距离的加权策略** 中得到广泛应用。
  
  * 进入 `return_imp_node_coeff()` 函数，**提取重要节点对应的系数**：根据 `imp_idx` 中的节点索引，找到每个节点对应的 bin 索引，进而提取每个节点所属 bin 的系数。**拼接系数**：将所有提取的系数拼接成一个大张量，并返回。
  
  * 处理自环问题并且更新 `edge_types` ，`rel_num` `n_edges`
  
  * 开始进行 模型 的预训练
  
    * 创建三个损失模块
  
    * 隐藏层维度为8，头为8，谓词嵌入的维度为10
  
    * 进入谓词嵌入 GAT 模型 `Predicate_GAT`（==**这个一定要掌握！！**==）
  
      * `in_dim = 64`, `hidden_dim = 8`, `out_dim = 64`, `rel_dim = 10`
  
    * 进入早停模块初始化
  
    * 进入 `return_centroids()` 函数: **计算一组节点嵌入（embeddings）的质心（centroid）**,之所以被称之为质心是因为它会分别计算每个 `bin` 中对应节点特征的均值，即为该 `bin` 的质心
  
      > **计算 bin 内所有节点的质心**（均值）。
      >
      > **存入 `bin2centroids`**，用于单独查询每个 bin 的质心。
      >
      > **拼接 `imp_bin_centroids`**，形成所有 bin 质心的矩阵。
      >
      > **跳过空 bin**，避免计算错误。
  

#### 理解两个损失函数

* 自己尝试写一下，要很熟悉矩阵以及张量操作
* 太猛了，大概懂了，先从损失函数入手，在训练时处理损失函数需要的数据
* 思考一下 $\text{Loss}_2$ 中的系数设置从直觉上以及数学上设计的合理性
  * 数学上和哲学上都是合理的，首先 $\text{Loss} = -\log \frac{A}{A+B}$ 就是一个恒大于 0 的数，当 $A >> B$ 的时候，损失越接近 $0$ ，系数 $\beta_{m,n}$ 的出现使得物理上距离更远的两个 `bin` 在数值得出的 `B`相较于 `A` 会更小，符合损失函数所要传达的意思。


#### 一些工具或者技巧

* 在计算 $L_1$ 损失的时候，代码中将 `embed_important` 与 `embed_normal`  （分别对应了论文中公式（1）的 $h_i$ 与 $h_j$）进行了归一化操作，即 

  ```python
  embed_important = F.normalize(embed_important)
  embed_normal = F.normalize(embed_normal)
  ```

  这是为了防止后续在进行 `torch.exp()`  操作的同时指数爆炸，经过实验直接变成 `nan` 了（这是因为分母或分子为 `inf` 导致结果为 `nan`）

  

*  设置随机种子

* 简单早停模块

* 梯度裁剪

  * 防止梯度爆炸（**在反向传播时使用！！**），用于提高训练稳定性，常用于深度网

    * 需要在 `loss.backward()` 之后、`optimizer.step()` 之前执行，而不是在 `forward` 里执行。

  * `torch.nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = 10, norm_type = 2)`
  
    > 梯度裁剪的缩放公式如下：
    >
    > $g_i = g_i \times \frac{\text{max\_norm}}{\|\mathbf{g}\|_2}$
    >
    > 其中：
    >
    > * $g_i$ 是梯度的某个分量。
    >   
    > * $\|\mathbf{g}\|_2$ 是所有梯度的 **L2 范数**（即欧几里得范数）：
    >   
    >   $\|\mathbf{g}\|_2 = \sqrt{\sum_i g_i^2}$
    >
    > * **如果** $\|\mathbf{g}\|_2 \leq \text{max\_norm}$，则不进行缩放。
    >
    > * **如果** $\|\mathbf{g}\|_2 > \text{max\_norm}$，则所有梯度按照这个比例缩放，使得梯度范数不会超过 `max_norm`。

