# 论文与技术哲思

* 只记录一些**自己深度思考**的个人感觉很关键很核心的东西（部分可以用理论解释，部分是经验的解释）



## （论文）为什么这些来自不同语义空间的信息，能被有效聚合为有意义的嵌入向量？

* 出处论文：[Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks](https://arxiv.org/pdf/2402.12411)

针对 Heterogeneous Knowledge Aggregation（异构知识聚合这一部分）提出的疑问

与公式2~5相关

具体疑问：

1. 

==为什么这么密集的信息这种嵌入向量能够取得不错的训练效果？？？我有点难以理解？感觉是个黑盒，有没有偏数学的理论支撑？毕竟我感觉聚合的过程中，不同层 $l$ 以及不同的节点类型的信息的空间是不一样的（可以理解为语义空间的不同）。为了处理这个问题，作者是不是使用了一组共享参数的前馈神经网络如公式（2）、（4）所示，将其映射到同一语义空间来进行处理？这是不是也是常见的操作？为什么这样做是有用的？==

GPT回答：

> ## ✅ 你的理解是对的：
>
> 是的，**公式 (2)** 和 **(4)** 中确实用了**共享参数的前馈神经网络**，本质上是在把“不同层次结构知识的向量表示”投射到一个共享的语义空间中，进而统一用 soft attention 来加权。
>  这是一个常见但非常重要的 trick，被广泛用于：
>
> - Transformer（Q/K/V projection）
> - Heterogeneous GNN（如 HAN、HGT 等）
> - Cross-modal learning（图像+文本融合）
>
> ------
>
> ## 🔵 **为什么这是必要的？**（你提到的“语义空间不同”问题）
>
> 你说的没错：
>
> - 比如 **centrality embedding** 和 **similarity embedding** 的本质统计语义是不同的。
> - metapath A→P→A 和 A→P→V 的语义空间也不一样。
> - 不同节点类型（author, paper, venue）拥有不同属性域，其分布维度也不同。
>
> 👉 **因此直接拼接或加权求和这些东西会产生“语义干扰”或“空间投毒”现象。**
>
> 所以作者用 **前馈神经网络（MLP）+ attention** 的组合，起到了三重作用：
>
> ### 1. **空间统一：**
>
> 通过共享参数 MLP（例如 $W_1',W_1$），将所有类型的结构知识投射到一个 shared latent space 中，使它们具备比较和加权的前提。
>
> ### 2. **可比性建立：**
>
> attention 中 softmax 权重仅有意义的前提是不同输入具备可比较的投影维度，否则无法合理区分重要性。
>
> ### 3. **可学习加权：**
>
> 不同 metapath 或 centrality 对不同节点的价值不同，让模型学会“从众多来源中自动选择最重要的几类结构信息”。
>
> ------
>
> ## 🟢 数学支撑与理论解释？
>
> 虽然当前还没有特别严格的数学证明能直接回答“为什么 attention 融合是最优的”，但已有一些数学观点/经验可以作为支撑：
>
> ### 1. **注意力机制的归一化特性（softmax） + 残差结构（skip connections）**
>
> 这些组合在信息融合过程中可避免“低质量信息干扰整体表征”的问题。
>
> ### 2. **从最优传输角度**（SKES 后续使用 optimal transport 测距）
>
> 本质上是在 latent space 中寻找“语义分布间的 Wasserstein 距离”。只要每个 node 的 representation 能捕捉到其重要性的分布特征，模型就能通过 OT 找到全局可比较的 embedding。
>
> ### 3. **经验上 HGT/HAN/HGNN 等模型都采用类似结构**
>
> 这已被许多大规模 benchmark 实验证明是效果优越的结构设计：
> ==**局部空间对齐 → attention 融合 → 全局嵌入生成。**==
>
> ------
>
> ## 📌 举个类比帮助理解：
>
> 你可以把不同的看成是**不同专家给的意见（例如一位结构学家、一位拓扑学家、一位内容分析师）**，他们对一个节点（比如一个论文作者）的重要性理解是不一样的。
>
> 那怎么办呢？
>
> 1. 先把所有专家的观点翻译成“标准学术术语”（用 MLP 映射到同一空间）
> 2. 再用“学术委员会打分”（attention）来看哪个观点对当前评估最有帮助
> 3. 然后**融合打分意见，统一表述这个作者的综合影响力**



2. （追问），**全是干货**

==凭什么用一组FFN投影之后 它们就可以比较了。我是这样想的：A 与 B 不同 C共享；那为什么 AC 与 BC 可比较==

| 问题                              | 回答                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| “凭什么用 FFN 映射之后就可比较？” | 因为 FFN 和 Attention 是**联合训练**的，FFN 的目标就是“把原始结构嵌入映射到可被比较的表征空间”。 |
| 是不是有理论支撑？                | 这属于 representation learning 中的一个“**分布对齐**”操作，**没有严格证明**，但在 Transformer 和多模态建模中**大量验证有效**。 |
| 是否存在风险？                    | 是的，如果输入**结构过于差异、FFN 太浅或训练数据不足，会导致学不出良好的对齐空间。** |

后续自己又思考了一下，这里的可行应该是指大家都是 这么操作的！即我虽然在 $a$ 空间，你在 $b$ 空间，我们拥有不同的语义信息，但是我们被同一个投影矩阵 $C$ 作用。投影到了一个可比较的空间。这里为什么可比较我们可以这样理解：对矩阵 $C$ 的行向量通过 $a$ 或 $b$ 做了加权求和，即我们将变换的视角看成 $C$：

$aC=c_1,bC = c_2$ 虽然 $c_1$ 和 $c_2$ 可能语义信息不同（即数据的度量尺度完全不同，有的权重高，有的权重低，有可能导致最后得到的 attention 不太有信服力），但由于 $a,C,b$ 都是可学习参数，可以在训练过程中及时调整，最后得到合理的嵌入向量、投影矩阵，计算得到的注意力系数也将合理。

| 我的视角                    | 数学解释                                                    |
| --------------------------- | ----------------------------------------------------------- |
| “$a$ 对 $C$ 行向量加权求和” | $aC = \sum_ia_iC_i$：**激活语义基向量**                     |
| “变换的视角是 $C$”          | $C$ 是**语义空间的 basis**，输入是选择哪些 basis 来构成输出 |
| “语义空间统一”              | 所有输入共享 $C$，使输出具有结构可比性                      |

我们可以把 $C$ 看作是一种“**共享的语义参考系**”，不同的输入 $a$、$b$ 就像是用不同方式在这个参考系中“**激活一段解释空间**”。

所以虽然原始的 $a$ 和 $b$ 是不可比的，但只要它们都从同一个 $C$ 里提取向量，那它们的输出就可以进入 attention 层去比较了。

这也可以解释为什么：

- **$C$ 需要共享**（否则两个视角激活的是不同参考系，就不可比了）
- **$C$ 要可学习**（否则可能学不到一个合理的公共语义空间）





## （论文）从偏数学和感性理解的视角自己脑洞了一下 $\mathbf{q}^{\top} W k$ 的可解释性，有时间补充一下。

* 受到了上面思考的启发







## （论文）从前向传播/反向传播的角度理解一下循环神经网络中的短期记忆

* 看up主唐一旦聊AI的这个[视频](https://www.bilibili.com/video/BV1R4421Q7tQ?spm_id_from=333.788.videopod.sections&vd_source=56ba8a8ec52809c81ce429c827dc30ab)中关于短期记忆的介绍有感所思
* 最好（务必）搭配这个[例子](https://github.com/GenTang/regression2chatgpt/blob/zh/ch10_rnn/bptt_example.ipynb)理解一下梯度图的计算过程

我认为 up 主说的这段话有点问题，或者说表述得不好：<u>“后续对应的梯度值越小，那么它对后续信息的贡献就会更小，即数据距离越远 -> 反向传播的路径越长 -> 梯度越容易消失 -> 带来的信息量越少。比如 $h_3$ 同时会受到 $x_1, x_2,x_3$ 输入信息的影响，因为 $x_3$ 距离 $h_3$ 更近，因此 $x_3$ 的贡献最大。”</u>

我更倾向于这种表述：

> “谁对最终 loss 有更大贡献”，**本质**上是看谁在**前向传播**中更主导整个模型的输出；反向传播的梯度只是**量化**这种贡献的方式。而路径长度、非线性、权重等等只是放大或衰减这种主导性的手段。”

输入信号离得越远，其实是一种**信息衰减机制**（远的信息会进行更多层的链式累乘，当然权重得小于 1）的体现

公式如下：

$V_k = X_k \cdot W_v,\ H_i = \sum_k ^i V_k \cdot W_h^{i-k}$





## （技术）关于批处理技术运用到时序数据上的理解

* ~~我不太理解使用批处理技术来处理时序数据。 假如说我现在有5个场站的数据，这5组数据互相独立，分别记录了一年365 \* 24 = 8760条数据，那么总共有 8760 * 5 = 43800条数据。我们现在使用批处理。究竟是8760为一个批次还是以诸如32、16这种传统的 batch_size 为一个批次。若选择前者，似乎是更有效的，也更符合人直觉的（我们关注各个独立场站之间的数据）但是批次太大了。若是后者，那么我们会将一段长时间且连续的数据拆成多个批次，然后进行独立的处理（不知道是不是独立），而且还有可能和其他场站的数据混合！该如何理解这一些情况？~~（我这段内容的表示非常有问题，我在发出这个问题时根本还没有理解透彻！！见 update！！）

  * 首先理解什么是时序数据的样本：时序数据的样本**不是一个点**，而是**一段时间序列**

  * 分别展开说说上面批处理的两种典型处理方式

    * 样本 = 某一场站上的一个“时间窗口”

      这是最常见的时序建模思路，对于每一个场站，我们能够生成 `8760 - pre_len - win_size` 个样本；

      然后我们可以把这些样本随机分成若干个 `batch` ，比如 `batch_size = 32、64`；

      每个 batch 由 **同一场站**的多个时间片构成（**不跨时段**，但可以乱序采样）；

      优点：

      1. `batch_size` 由我们自己控制，不管一个场站有多少小时；
      2. 内存可控，可充分使用 GPU 进行加速
      3. 常用于训练阶段

    * 样本 = 一个完整的时间序列（整个8760条）

      这是我身为一个初学者最初始的思路：每次模型吃下整年的 8760 条数据，做一次训练更新；

      可以**按场站划分 `batch`**（如5个场站→`batch_size=5`）；

      优点：

      1. 避免破坏时间连续性；
      2. 特别适用于模型结构允许一次处理长序列（如 Transformer、TFT 等）；
      3. 适合推理或 fine-tune；

      缺点：

      1. 占内存极大，限制 batch_size；
      2. 很难shuffle，训练容易过拟合；

  * 更加推荐的做法是

    每个样本 = 某一个场站在某一时间段的滑窗片段；然后将 batch_size 设置为传统的小数，如32，64；

    **场站分开训练（最好，若数据分布差异大）**；场站一起训练（提高泛化，适合数据分布相似）

  * 理解一下为什么训练集数据可以**乱序处理**

    首先，如果我把整个序列看作一段连续的现实过程，比如电力功率、温度、流量等：

    - 样本 A：`t=0~23` → `t=24`
    - 样本 B：`t=1~24` → `t=25`

    可以看到，A 和 B 的输入 **重叠了 23 个小时**，label 相邻，仅差 1 小时。

    > 所以从**数据来源角度讲**：**A 和 B 是高度相关的！**

    也正因为如此，很多文献在做 forecasting 时特别强调“不要让验证/测试数据和训练数据时间段重叠”。

    但是我们仍能在训练阶段乱序这些样本的本质原因是：==在训练阶段，模型并不需要依赖样本之间的顺序来建模“时间演化”，它只需要学习**输入与输出之间的映射关系**。==



update: 回过头来再看这个问题，我觉得我是将 `batch` 这个维度同 `seq_len` 这个维度弄混了。。





## （技术）关于LSTM运用批处理进行训练的训练流程的思考

来自比赛[第三届世界科学智能大赛新能源赛道：新能源发电功率预测](http://competition.sais.com.cn/competitionDetail/532315/format?spm=CHANNEL-0001)

没人指导，自己和GPT对话思考得出来的结果...

* **LSTM做时间序列预测时间序列长度应该怎么设置？**

  > 具体化这个回答:就像天气预报一样，如果历史数据够多，我可以用过去到现在为止10天、一个月甚至一年的数据来预测明天**一天**的结果，这个预测效果当然是比较好的;要是数据量有限怎么办，就用今天来预测明天或者前3天预测明天等等。在数据集有限的情况下，步长越小样本越多。因此，我们需要权衡的是步长要够大，样本量也要足够学习和测试。（**这里语境下的步长指的是历史输入长度也即滑动窗口的时间跨度**）
  >
  > 这段话强调的是：**在数据有限的前提下，需要在“历史输入长度（步长）”和“训练样本数量”之间做一个权衡。**
  >
  > 我之前犯得大错误是:我理解成用3步预测5步、10步了!**实际上都是前很多步预测下一步(因为这样是最准确的)。**那么，别人的文章为什么预测的步数那么多?是采用了移动窗口的方法，真实的123预测4，真实234预测5，一直循环下去，预测出456789。预测时，每次都把真实值再带入了一次(更改输入数据)。若不用真实的数据输入转而用当前预测的，也就是说，我用真123预测4，下一步就是真23预4来预测5。**这样的效果是，误差会累计，结果会越接近上一轮值，直到没有任何趋势(直线)。**
  >
  > 
  >
  > 作者：海底月
  > 链接：https://www.zhihu.com/question/429976362/answer/1581271976
  > 来源：知乎
  > 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

  所以我们得到的经验是：若数据足够多，那么我们可以用多步来预测一步，体现在本任务上就是我们可以考虑使用一周（7 * 24 = 168h 来预测一天 24h）的数据；或者考虑使用（24h的数据来预测1h的数据）

  GPT的建议：

  > ✅ 如果你有**足够多的高质量历史数据**，并且模型（如 LSTM、Transformer 等）能够处理长序列，**建议使用「方法1」**：用一周（168h）预测一天（24h），可一次性生成完整预测，更适合当前这种**预测整天功率曲线**的任务。

  > 🔄 若数据不多、模型较简单、或者你在调试阶段，**可以从「方法2」入手**，快速实现滚动预测逻辑，再逐步扩展。

  考虑到我们的数据是以年为单位的（数据量比较多，并且我们也想捕捉到周期性）因此选择方案一应该是需要考虑的。我们首先从人类的视角直观地来理解应该选择怎样的时间步长，再进行实验（此外，针对seq_len的设置，最好设置成能够包含周期的seq_len，比如一天、一周）。**还有就是LSTM能够有效的步长（seq_len）是有限的，3000步肯定是已经失效了，即早就忘记前面的信息了**。

  这样的话，在训练时，我能构造的训练样本数量为： $N = 8760 - 168 - 24 + 1 = 8569$ 即我的 `train_loader` 中会有 8569个样本对。

* 针对我的任务，`batch_size` 该怎么设置？以及是否应该打乱即 `shuffle=True`（我还是不能理解和接受这一块，尤其是在我有场站的情况下）

  我纠结的点其实就是在于分批次训练是不是会将时序这个非独立性的属性给”独立化“掉（因为分批次并行训练的话，不同批次的数据可以看成是独立的，但并非完全独立，因为它们的损失会一起影响模型的更新，相对纯串行训练，方差是更小的，更稳定的）。所以分批次能更好地带来模型的稳定性以及训练效率。

  至于分批次能否影响时序这个属性，对模型性能产生影响以及是否应该 `shuffle` 我有以下思考。

  首先需要理解的是，如果仅仅只是分批次，那么数据本身仍然是连续的，只是在训练或处理时被分成了若干小批次（btch）来进行，这通常是为了提高训练效率或减小内存消耗。可以举一个例子来进行直观上的理解，假设我有一个连续的时间序列的数据：

  ```csharp
  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  ```

  如果用 batch_size=2 分批次训练，那么数据在训练时会被切分为：

  ```less
  Batch 1: [1, 2]
  Batch 2: [3, 4]
  Batch 3: [5, 6]
  ...
  
  ```

  但是这些数据**本身是连续的**，你只是每次喂给模型两个数据点而已。

  回到问题，虽然 `batch` 虽然看起来”打破了时序的连续性“，但是在每个 `batch` 中，样本是完全连续的，因为 `seq_len` 这一维度上的数据就是时序构建的，我们选取了一周 7  天的数据。所以 `batch_1` 可以认为是`batch_size` 个样本，每个样本是 7 天的数据，也就是 `batch_size x seq_len`， `batch_2` 以此类推，`batch`之间是独立的，但 `batch` 内部是连续的。并且需要注意的一点是：**模型的时序建模能力仍然体现在样本内部的时间维度上，而不是 `batch` 之间的顺序。**（我们可以对每个样本数据设计时序顺序来更好地保证模型质量，比如说月份，小时，季节），我们的 `seq_len` 设计的就是一周七天，再结合样本的属性，模型是能出色地预测出下一天的数据的，我们只需将这`七天 *  batch_size`的特征捕捉即可，**模型只需要在每个样本内部捕捉时序依赖，不需要跨样本记住前后信息**。）。其实一个批次里面也有挺多数据了，也够了。~~如果不使用批处理，那么整个时序太长了，模型根本记不住，效果反而可能会更差。~~（这段的理解其实是有误的，模型能否记住与批次无关，与 `seq_len` 的大小有关）

  虽然批次之间样本在时间上不连续，但**我们不期望模型学习跨样本的时间依赖**，因为我们**已经人为将序列片段分割成建模单位**，每个样本独立建模即可。这样来看的话，我们可以放心将批次打乱了（`shuffle = True`）。而 `batch_size` 的选取就是正常的从32、64、128中去选。

  而我之前的顾虑：为什么不以场站的数量为批次，每个场站的所有数据为一批进行训练（这样操作的话，`batch_size = 8569`， `seq_len = 168`（不变） ，那么 `len(train_loader) = 10`（`batch` = 10））批次太大了，将会极大降低梯度的方差，使得每次参数更新的方向变得过于“精确”，从而让模型在训练过程中更容易陷入陡峭但非全局的最小值，或者鞍点。

  **其实只要抓住本质：LSTM模型主要还是看 `seq_len` 这一维度上的数据（此处才是用来训练时序的），这里的数据不能乱！究其原因，我之前都是将 `batch` 这一维度和 `seq_len` 这一维度所充但的任务搞混了才会发出这种疑问**【针对我的任务，`batch_size` 该怎么设置？以及是否应该打乱即 `shuffle=True`（我还是不能理解和接受这一块，尤其是在我有场站的情况下）】（现在已理解）

  **因此目前我打算将前五个场站的数据合并，后五个场站的数据合并。然后 `shuffle = True` 并且设置 `batch_size` 进行训练，考虑设置为 64 或 32**，但是我还要写一份每个场站单独训练的代码，要思考并规划一下。

* 模型训练好了，我们有一组测试数据（1416 = 24 * 59），也就是说要得到1416个`target`，我们能做到test 中原本的前168个`target` 不丢失吗？是不是前 168 个 `target` 会丢失？

  是的，若按照**正常的预测过程**为：

  ```python
  x[0:167] → 预测 y[168]
  x[1:168] → 预测 y[169]
  ...
  ```

  我们需要在测试前额外准备一段历史（前 168 条）输入特征，即可完整预测 1416 个 `target`！（总数其实是1417个，我们考虑将最后一个结果剔除掉）

  若没有我们可以考虑**从训练集或验证集的末尾取出最后的 168 条输入特征数据**，拼接到测试集的前面，作为测试时的“历史上下文”。这样，模型就能用 `[train[-168:], test[:1416]]` 的滑动窗口来预测 `target[0] ~ target[1415]`。

  这种处理确实会引入轻微误差，但这是无法避免且普遍接受的做法。而**这是时序预测里业界、学界都默认接受的策略**；虽然会有误差，但这是我们可以接受的。

* 如果样本数量没有达到 `batch_size` 的整数倍，那么在训练时和预测时会怎么处理？

  默认（`drop_last = False`）都会选择继续处理小批次（`mini_batch`）模型会用这个小批次进行训练/预测并返回结果，剩余部分的样本不会被丢弃。

* 如何理解以及处理时序训练中遇到的样本之间高度重叠导致数据高度相关这一问题？？

  典型的滑动窗口采样模式：

  - 样本 A：`x=[t=0~23], y=t=24`
  - 样本 B：`x=[t=1~24], y=t=25`

  两者输入序列几乎完全重叠，label 也紧邻。这种高重叠带来的**主要问题**如下：

  1. 机器学习普遍假设训练样本是 i.i.d.（独立同分布），但这里明显不成立；导致模型“看到的是变种重复”，**学到的泛化能力弱**。
  2. 验证集/测试集与训练集严重“信息泄漏”：如果我从同一时间段中滑动生成训练集和验证集，极易产生**过拟合现象**；你以为模型学得很准，其实是“见过类似的样本”。（这个可以先分再滑处理？）
  3. 梯度冗余，训练效率低：连续样本之间梯度非常相似，更新方向重复；训练进展慢（是因为几乎只朝着一个方向更新，但是原本我们可以探索多个方向），容易陷入局部最优或早停（这是因为梯度方向高度一致，导致探索空间变窄）。

  **如果训练样本之间高度相似，模型在训练时的梯度方向高度重复，就像在“原地打圈”，无法从多角度学习数据特征。这会导致：**

  - **更新方向冗余；**
  - **模型学习缓慢；**
  - **探索空间受限；**
  - **提前停止训练但性能一般。**

  有如下策略可以缓解这些问题

  1. 增大滑动窗口的跳步 stride

     ```python
     for i in range(0, len(data) - seq_len - pre_len, stride):
         x = data[i: i + seq_len]
         y = data[i + seq_len: i + seq_len + pre_len]
     ```

  2. 按时间划分训练集和验证集，确保无泄漏
  3. 做样本去重或过滤高度相似样本: 比如设阈值，计算相邻输入的差异度（如余弦相似度）；或在构造训练集时，仅保留差异较大的窗口。



## （论文）理解与思考FFN设计的目的以及它为什么能work的原因

* 背景是此阶段看得很多相关的论文都在使用 transform ，但是一直不理解设计 `FFN` 的原因，想彻底搞清楚每一步的动机是什么
* 这篇知乎上的[文章](https://www.zhihu.com/question/622085869/answer/88824367005)醍醐灌顶！这里的回答是参考该篇文章的（建议直接看原文！！）。
* 基于下文我们可以再对 transform 的结构进行总结。Attention 重点关注序列的 L 维度（序列长度维度），可以跨空间地关注 token 之间的相互关系、依赖和交互；FFN 重点关注序列的 d 维度（特征维度）关注如何在每个 token 的**特征维度**中混合信息，得到更强的表示。

==FFN在Transformer里面主要是对**多头注意力矩阵**升维，非线性过滤，然后再降回原来的维度。这个通常的比喻是：FFN就像个人的思考空间—— Attention Layer帮助模型正确的分配注意力，然后FFN 帮助模型仔细的思考，提取更加抽象的特征。==

首先我们要先了解 transform 之前的设计结构即注意力机制的计算是这样的：
$$
Attention = \text{softmax}(\frac{QK^{\top}}{\sqrt{d_k}})V
$$
Attention 层主要的问题在于，多头注意力不会互相混合，从某种意义上依然是**泾渭分明**的对不同 $V$ 的「线性」混合。

无论括号里面的部分怎么腾挪变化，怎么再加上softmax来非线性化，如果我们把前面的部分用 $A$ 来表示，那么注意力分数说到底也就是 $AV$ ，是一个 $V$ 的线性变换。如果注意力有$h$ 个头，那么其注意力矩阵则可以标注为 $A^iV^i$，那么最后联合的时候可以写成这个形式：
$$
\begin{bmatrix}
A^1 & 0 & \ldots & 0 \\
0 & A^2 & \ldots & 0 \\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & \ldots & A^h
\end{bmatrix}

\begin{bmatrix}
V1 \\
V2 \\
\vdots \\
V^h
\end{bmatrix}
$$
**虽然每个头都有不同的 $V$,但是这个 $V$ 是来自训练矩阵 $W_v ^i$ 。换句话说，注意力的头再多，终究是在自身的子空间的投影作为权重，然后用这个权重来对 $V$ 进行线性变换罢了，注意力再多，也不改变这个本质，注意力之间也无法打破障碍进行混合。**

之后我们会用一个投影矩阵 $W$ 将这个基于多头的 `cat` 矩阵映射到同一个空间上。各个头的特征将会被“混合”，接下来的 FFN 会处理到每个头的所有特征

然后 FFN 出场。Transformers 原始论文中的解释： FNN 可以看作用 1x1 的卷积核来进行特征的升维和降维。估计是当时 Transformers 内容太多，所以作者并没有很详细的解释。

从形式上看 FFN 做了两件事1. 先升维再降维 2. 增加了非线性性质

针对1和2，**其实本身也是试图在平滑特征的空间**。（这里如何理解很关键，需要结合核方法的相关知识。我们首先需要知道，低维空间很难表示复杂特征，在图像上显示的话它们会呈现**扭打在一起的状态**，但是在升维之后，这个操作就像展开一团毛线，把它舒展开，让不同特征在更大空间中被区分、表达；）最理想的情况当然是在无限维空间去展开所有的特征，在无限维空间中，任何复杂的非线性关系都可以被完美地展开、整理和分离。但是考虑到现实性和训练的充分性、过拟合性这些问题，在升维的时候用力过猛，工程上会有很大的困难。所以最后transformer作者选择升成4倍是一个权衡的结果。

由于升维之后，特征之间的关系可以被充分地展开和重组让更加不可分的细微的特征结构也能暴露在非线性激活函数如 `ReLU` `ELU` 面前。通过非线性激活函数进行**分段处理**，最终大大增强了模型的表示能力、分离能力和对复杂语义的理解能力。至于最后又降维到之前的维度，其实就是正常的卷积网络里面的**提炼信息的过程**，相当于把之前过滤的重要特征组合通过重组

经过升维+非线性+降维之后，特征空间的局部表达能力就得到了很大的提升。

所以这么掰开了分析之后，再回过头看这个「个人思考空间」的比喻，感觉也还比较贴切，确实是相当于在「精炼」已经被注意力提取的信息。但是如果不琢磨这些细节，单看这个比喻，仿佛是什么都没说。

那么你可能会问，对 `cat` 矩阵进行操作的 $W$ 不也是对所有特征进行了混合的变化吗？为什么还需要 FFN。是的，多头注意力输出后的 线性变换 的确实现了一次跨头的线性混合。但是这个混合是线性的、局部的、浅层的，而 FFN 提供了 **非线性、高阶、跨特征**的处理，这两者不是等价的。

由于 FFN 对每个位置的feature都是无差别处理的，所以FFN泛化能力很不错，这就像是在序列的每个位置都在学习相同的特征混合器，这使得模型在学习特征交互规律时，必须提取与位置无关的普适性模式，也可以帮助模型更好地泛化到不同长度的序列。



## （技术）当我们的模型可能输出为负数而对应的目标输出在现实中应为正数（如功率、电量销售额等）这是什么原因？我们又该怎么解决

* 原因：

  * 模型没有学到非负的约束
    * 绝大多数模型默认输出是实数域（如线性回归、LSTM、Transformer等），没**有内建任何“输出≥0”的限制**。
    * 比如 LSTM 最后一层是 `Linear(hidden_dim, 1)`，自然可能输出负数。
  * 数据标准化方式问题
    * 如果对目标值（label）进行标准化（如 `StandardScaler`），在还原时可能会出现负值。（绝大部分情况都会出现负值，除非这一维上所有的值都相等）
    * 即使原始值全是正数，标准化过程中值会分布在正负之间。（这是显然的，而且个人感觉受极端值影响比较大）
  * 损失函数没有强制非负性
    * 使用如 MSE、MAE 时，**只优化误差，而不是值域限制**，导致在某些输入下预测值负偏离。
  * 训练数据中存在小值、波动大、模型未收敛等现象
    * 比如输入接近于0但实际功率是低正值，模型可能预测为小负值。
    * 小样本、特征不充分也可能导致预测不稳定。（概括起来就是模型预测不稳定）

* 解决方案：

  1. 在模型结构中加非负约束：最直接的方法是在**输出层**使用**非负激活函数**

     ```python
     import torch.nn as nn
     
     model = nn.Sequential(
         nn.LSTM(...),
         nn.Linear(...),
         nn.ReLU()  # 或者 nn.Softplus()
     )
     ```

     常用的非负激活：

     - `ReLU()`：直接将负数变0，简单粗暴
     - `Softplus()`：平滑版ReLU，输出始终 > 0，更适合回归任务

  2. 预测对数再还原（适用于**乘法性增长/分布右偏的目标变量**）

     * 补充两个概念：乘法性增长与分布偏右

       乘法性增长：乘法性增长（multiplicative growth）指的是目标变量的变化趋势是**成倍地增加或减少**的，而不是固定加减，比如每次乘2

       分布偏右：**偏分布**指的是数据大多数集中在较小的值，但存在一些非常大的值拉高了均值

       **log 变换后会趋于正态分布（常用于建模）**

     操作步骤如下：

     > 对目标进行 `log(y + ε)` 变换（ε 是一个小常数，如 `1e-6`），保证取对数安全；
     >
     > 训练模型预测 log 之后的值；
     >
     > 在预测阶段，将输出通过 `exp` 还原即可。

     ```python
     # 训练前
     train_target = np.log(train_target + 1e6)
     
     # 预测后还原
     pred = np.exp(pred_output)
     
     ```

  3. 对输出强制截断（不推荐⚠️）

     在预测阶段将负数替换为0

     ```python
     y_pred = torch.clamp(y_pred, min = 0)
     ```

     缺点：不是模型内生学习到的，可能隐藏模型预测问题

  

## （技术）下述的这张图片为什么是明显的过拟合？以及过拟合出现的原因。

![过拟合](C:\Users\5c\Desktop\Study Notes\pic\过拟合.png)

首先需要明白，过拟合最标志的一点是在训练集上表现良好，具体体现在：损失稳步下降，Score稳步提升；但是在测试集上表现较差，具体体现在：损失**并没有**稳定下降，score也**并没有稳步提升**。过拟合指的是：**模型在训练集上表现很好（低 loss、高 score），但在验证集上表现较差（高 loss、低 score）**。这说明模型学会了训练集中的“记忆”，而没有真正学到泛化的能力。

这张图正好符合了所有的点，训练集的表现情况完全符合描述，验证集的表现情况也完全符合描述：1. 没有明显的提升：一直在某个点附近上下震荡 2. 没有稳步，震荡的幅度非常的大，非常不稳定！！

那么造成过拟合的原因有哪些：

1. 模型太复杂（参数多）
2. 训练轮次多（可能未使用早停机制）
3. 验证集太小或不够代表整体分布
4. 缺乏正则化手段



## （技术）深刻理解一下使用字典/稀疏矩阵与使用矩阵存储数据的区别与优劣

这个问题是我在学习推荐系统 [基于用户的协同过滤](https://datawhalechina.github.io/fun-rec/#/ch02/ch2.1/ch2.1.1/usercf) 这一章学到的。

* 首先先理解一下稀疏矩阵的结构，以推荐系统为例，下面举一个例子

  行：用户（user）

  列：物品（item）

  元素值：评分、点击、偏好程度等（没有行为则为空）

  |       | Item A | Item B | Item C | Item D | Item E |
  | ----- | ------ | ------ | ------ | ------ | ------ |
  | User1 | 5      |        |        | 3      |        |
  | User2 |        | 4      |        |        |        |
  | User3 |        |        | 2      |        | 1      |
  | User4 | 3      |        |        |        |        |

  接下来看一下，在**代码**中稀疏矩阵的表示方法

  1. 嵌套字典（适合小规模或构造期）

     ```python
     users = {
         'user1': {'itemA': 5, 'itemD': 3},
         'user2': {'itemB': 4},
         'user3': {'itemC': 2, 'itemE': 1},
         'user4': {'itemA': 3}
     }
     ```

  2. 稀疏矩阵结构（推荐大规模使用）

     ```python
     from scipy.sparse import csr_matrix
     import numpy as np
     
     # 构造稀疏矩阵数据（需要三项）
     data = np.array([5, 3, 4, 2, 1, 3])
     row = np.array([0, 0, 1, 2, 2, 3])  # 用户索引
     col = np.array([0, 3, 1, 2, 4, 0])  # 物品索引
     
     R = csr_matrix((data, (row, col)), shape=(4, 5))
     ```

     使用 `.toarray()` 方法或者 `.A` 属性将一个稀疏矩阵对象转换成稠密矩阵 `Numpy`

     `data_matrix = R.A` `data_matrix = R.toarray()`

     稀疏矩阵适合存储大规模稀疏数据（如 10000×10000），但转为稠密矩阵会立刻占满内存。我们不妨计算一下这里大概需要多少内存，如果用 `float64`，内存占用为：`10000 × 10000 × 8 = 800,000,000 字节 ≈ 762.94 MB`

* 使用字典的优点：节省空间、适合稀疏数据

  现实中用户对物品的评分是非常稀疏的，使用稀疏矩阵或嵌套字典（如 `users = {user1: {item1: 4.5, item3: 3.0}}`）存储可以显著节省空间。

  - 在 Python 中，如果你用 NumPy 矩阵来表示一个大多数元素是 `NaN` 或 `0` 的评分表，会浪费大量内存。
  - 字典**按需存储非零值**，只记录用户确实评分过的物品，**天然适合**稀疏性。

* 字典的劣势：牺牲了部分时间效率

  用字典（特别是嵌套字典）操作时的计算效率相对较低，原因包括：

  - **缺乏向量化计算**：不像 NumPy 矩阵那样可以使用矩阵运算（如点积、余弦相似度）**直接计算**，**字典通常需要手动遍历**。
  - **随机访问性能不如数组**：虽然 Python 字典的查找是平均 O(1)，但遍历所有用户或物品的组合仍然成本高。
  - **不能直接使用线性代数库**：很多机器学习算法、推荐算法（如 SVD、ALS）依赖于稠密或稀疏矩阵格式。

*  实际应用中如何权衡？

  在真实推荐系统或机器学习任务中：

  - **数据准备阶段**：通常用字典或稀疏格式（如 `scipy.sparse.csr_matrix`）来构建数据。
  - **计算阶段**：往往将字典转成稀疏矩阵进行高效计算，比如用 `scikit-learn`, `Surprise`, `lightFM` 等库。

  我们可以把字典作为“数据预处理阶段”的中间结构，最终还是建议用专门的稀疏矩阵数据结构进行建模和计算。



## （论文与技术）在复现EASING这篇论文中，为什么解码过程中看起来错误的写法反而效果更好？

* 具体来说，源码是这样的（注意，这是一个**解码**过程中的部分）

  ```python
  qs = self.w_qs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
  ```

  而标准的多头注意力应该是这样的写法。从最低维度 `token` 的维度先分头，然后再 `permute` 将对应位置上的头对齐

  ```python
  qs = self.w_qs(s).view(s.shape[0], s.shape[1], self.heads_num, self.heads_dim).permute(0, 2, 1, 3)  # [B, H, N, D']
  ```

  为了弄清这个问题，我们考虑先从它的编码过程看，编码过程其实就是传统的 transfrom 过程，然后投影到一个同一维度（这个过程同时还具有融合多个头信息的效果）

  ```python
  self.o_linear = nn.Linear(self.out_channel, self.out_channel, bias=False)
  rst = graph.dstdata['t'] # [n_node, n_head, out_dim]
  rst = self.o_linear(rst.flatten(1)) # [n_node, o_channel]
  ```

  EASING 的解码过程增加了一个维度，其特征变成了 $N \times 2d$ ，$2d$ 即为 o_channel，$N$ 也可以理解为头，事实上根据对源码的理解，这里的 $N$ 其实是对 $2d$ 这个嵌入特征**做了一些缩放或者反转的操作**！真正需要理解的是这一步。下面是源代码的写法

  ```python
  qs = self.w_qs(s).reshape(s.shape[0],self.heads_num,s.shape[1],self.heads_dim) # [B, heads_num, N, heads_dim]
  ```

  其中，这里的 `B` 代表的是节点的数量，`heads_num * heads_dim = 2d = H * D' = D`   。为什么这样写的效果反而比：

  ```python
  qs = self.w_qs(s).view(s.shape[0], s.shape[1], self.heads_num, self.heads_dim).permute(0, 2, 1, 3)  # [B, H, N, D']
  ```

  上述这种 transform 的标准写法好？

  

  经过查阅资料，了解到，源代码的写法（非标准reshape）是一种**耦合写法**，而我给出的写法是一种**解耦写法**，具体区别如下：

  | 项目          | 解耦（标准写法）                         | 耦合（非标准 reshape）              |
  | ------------- | ---------------------------------------- | ----------------------------------- |
  | token 与 head | 完全独立，token 在所有 head 中**都参与** | **head 管 token 的某一子块**        |
  | 多头结构      | 并行注意力、子空间投影                   | 某种结构划分、特征方向上的切分      |
  | 表示能力      | 高，灵活，但可能过拟合                   | 有 inductive bias，结构受限但更稳健 |
  | 适用场景      | NLP / 通用 Transformer                   | 显式特征建模、结构推荐等            |

  我们之所以认为源码就应该按照标准写法写的原因是因为受到了编码过程的影响，我们的 $2d$ 维度是通过多个头**拼接**起来的，所以我们在解码的时候自然地就想 $2d$ 维度拆分成多个头就像是一个逆过程。但事实是：**在进入解码之前 $2d$ 维度上的嵌入特征已经融合了编码过程中所有头特征的信息**，再在 `token` 维度上的划分头其实是增加了自由度（因为token 会在所有的 head 中都参与，导致信息密度偏大）。

  $N \times 2d$ ,如果我们选择解耦揭发进行划分，那么感觉就比较杂乱，可解释性就比较差。==因为我们应该已经将 `token` 这个维度看成一个**整体了**==，再以它进行划分头是没有道理没有可解释性的，因为它”散掉“了。相反，如果我们使用耦合写法，那么可以将 $M$ 个 `token` 向量看成一个头，这样是具备可解释性的。因为**我们已经把 token 这个维度看作一个整体结构，代表某种组合后的表示或结构信息**，那我们就应该在 feature（通道）维度上进行结构划分。**每个 head 负责 token 整体表示的一部分子空间**，这样就能形成一种结构化、可解释的表示，比如：

  1. 每个 head 可能对应一个“语义方向”

  2. 每个 head 上可以单独建模 importance / uncertainty

  暂时这样理解吧。。唉



## （论文与技术）理解在 mini-batch + 半监督/伪监督图训练场景下的做法

* 在复现 EASING 这篇文章中，由于显存不够大进而学习并使用批处理的做法（感觉要花小一周）。但是这样技术在图挖掘领域还是很常见的，因此多花点时间也是值得的。

  下面的内容是 GPT 给出的：

  通常的做法是

  > **将所有目标节点（标记的和未标记的）一起传入采样器进行子图采样，**再在每个 batch 中用掩码区分 labeled / unlabeled 节点。

  这种 单 dataLoader + 掩码 是一种主流的做法。

  针对一些场景会有一些双 dataloader 的做法。
  
  目前采取的是单 dataloader 的做法，但是会发现一些问题
  
  1. 用 mini-batch 训练时， **labeled 和 unlabeled 混在一个 batch 里**，导致 labeled 样本数有时会很少，这对像 `list_loss` 这种需要较多样本做采样和排序的损失来说不太友好。
  
  目前尚未想到更好的解决办法

