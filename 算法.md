# 算法

## 聚类

### DBSCAN算法



## 图学习算法

### 图游走类算法

#### DeepWalk

理论可以看[这篇](https://zhuanlan.zhihu.com/p/56380812) 与 [这篇](https://zhuanlan.zhihu.com/p/397710211)

* 核心是要理解 **Rand-Walk**  与 **Skip-Gram** 的思想

  * Rand-Walk

  * Skip-Gram

    * 什么是**负采样（NS: Negative Sampling）思想，如何理解**

      * 负采样（Negative Sampling）是一种在训练模型时用于**处理大量类别问题**，特别是在推荐系统、词向量训练（如Word2Vec）或图神经网络中的一种优	化策略。其目的是减少计算量并加速模型训练。负采样通过从所有类别中**随机选择一部分**“负样本”，而不是每个负样本都参与计算，从而有效地降低计算量。

      * 优点：

        **减少计算量**：通过减少负样本的数量，训练过程中需要计算的点数大大减少，尤其在大规模数据集中，效果尤为显著。

        **加速收敛**：减少计算量也加速了模型的训练，使得模型可以更快地收敛。

        **有效的学习负样本特征**：负采样不仅帮助我们减小计算量，还能有效地学习负样本的特征。

      * 如何选择：
        负采样通常采用**随机选择**或者**根据某些特征选择**负样本。例如，在Word2Vec中，可以通过**词频**来调整负样本的选取概率，频率较高的词更容易被选为负样本，频率较低的词较难被选为负样本。
      * 举个例子理解：假设词汇表中有10000个词，使用传统方法，你每次训练时需要与9999个负样本（即非目标词）进行计算。而使用负采样时，你只需要与5个负样本进行计算，计算量减少了几千倍。此外，**目标词与正样本之间的关系会被强化而与负样本之间的关系将会疏远**==如何理解？怎么做到的？==
        * 充分理解对正例、负例损失函数的计算（交叉熵、人为设计！）
          负样本的损失为 $\mathcal{L}_N = -\log(1-P(neg \mid src))=-\log(1-sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{neg}))$
          正样本的损失为 $\mathcal{L}_{T}=-\log(P(pos \mid src))= -\log(sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{pos})$

    * 目标节点 `src` , 正样本 `pos`, 负样本 `neg`

      * **正样本**：指的是符合某个特定条件或目标的样本。例如，在DeepWalk中，正样本通常是目标词和上下文词之间的实际关系。

        **负样本**：指的是那些不符合特定条件的样本。例如，在DeepWalk中，负样本是目标词和随机选取的其他词之间的关系。

* 如何用向量表示网络结构？像这种**用向量表示网络**的方式，我们可以统称为 `Network Embedding`。`Network Embdding`: 在**低纬度的空间**里表示**整个网络**，并**尽可能保持网络的结构（相似）**，向量是稠密的。得到的表达向量可以用来进行下游任务，如节点分类，链接预测，可视化或重构原始图等。