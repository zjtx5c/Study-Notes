# 算法

## 聚类

### DBSCAN算法

* 理解一下流程，可以看下这个视频[DBSCAN流程](https://www.bilibili.com/video/BV17L4y147W2/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

* 算法中的各种点

  * **核心点（Core Point）**：在 `eps` 半径内至少有 `min_samples` 个数据点的点。

    **边界点（Border Point）**：在 `eps` 半径内**少于** `min_samples`，但属于某个簇的点。

    **噪声点（Noise Point）**：**既不是核心点，也不是边界点**，即它不属于任何簇，被赋值 `-1`。

## 图学习算法

### 图游走类算法

#### DeepWalk

理论可以看[这篇](https://zhuanlan.zhihu.com/p/56380812) 与 [这篇](https://zhuanlan.zhihu.com/p/397710211)

* 核心是要理解 **Rand-Walk**  与 **Skip-Gram** 的思想

  * Rand-Walk

    * 随机游走，目的是为了获取节点的**局部特征**
    * 滑动窗口采样：在游走的每一步，选择当前节点 `v` 及其前后 `w` 步的节点作为一个局部结构，生成一个训练样本对。这些节点对将用于训练模型。

  * Skip-Gram

    * 什么是**负采样（NS: Negative Sampling）思想，如何理解**

      * 负采样（Negative Sampling）是一种在训练模型时用于**处理大量类别问题**，特别是在推荐系统、词向量训练（如Word2Vec）或图神经网络中的一种优	化策略。其目的是减少计算量并加速模型训练。负采样通过从所有类别中**随机选择一部分**“负样本”，而不是每个负样本都参与计算，从而有效地降低计算量。

      * 优点：

        **减少计算量**：通过减少负样本的数量，训练过程中需要计算的点数大大减少，尤其在大规模数据集中，效果尤为显著。

        **加速收敛**：减少计算量也加速了模型的训练，使得模型可以更快地收敛。

        **有效的学习负样本特征**：负采样不仅帮助我们减小计算量，还能有效地学习负样本的特征。

      * 如何选择：
        负采样通常采用**随机选择**或者**根据某些特征选择**负样本。例如，在Word2Vec中，可以通过**词频**来调整负样本的选取概率，频率较高的词更容易被选为负样本，频率较低的词较难被选为负样本。
      * 举个例子理解：假设词汇表中有10000个词，使用传统方法，你每次训练时需要与9999个负样本（即非目标词）进行计算。而使用负采样时，你只需要与5个负样本进行计算，计算量减少了几千倍。此外，**目标词与正样本之间的关系会被强化而与负样本之间的关系将会疏远**==如何理解？怎么做到的？==
        * 充分理解对正例、负例损失函数的计算（交叉熵、人为设计！）
          负样本的损失为 $\mathcal{L}_N = -\log(1-P(neg \mid src))=-\log(1-sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{neg}))$
          正样本的损失为 $\mathcal{L}_{T}=-\log(P(pos \mid src))= -\log(sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{pos})$

    * 目标节点 `src` , 正样本 `pos`, 负样本 `neg`

      * **正样本**：指的是符合某个特定条件或目标的样本。例如，在DeepWalk中，正样本通常是目标词和上下文词之间的实际关系。

        **负样本**：指的是那些不符合特定条件的样本。例如，在DeepWalk中，负样本是目标词和随机选取的其他词之间的关系。

* 如何用向量表示网络结构？像这种**用向量表示网络**的方式，我们可以统称为 `Network Embedding`。`Network Embdding`: 在**低纬度的空间**里表示**整个网络**，并**尽可能保持网络的结构（相似）**，向量是稠密的。得到的表达向量可以用来进行下游任务，如节点分类，链接预测，可视化或重构原始图等。

  * 它本身不直接完成特定任务，而是为下游任务（如节点分类、链接预测等）提供高质量的输入特征。
  * 因此，DeepWalk 是一个**图表示学习**方法，旨在将图的结构信息**编码**为低维向量，以便后续任务使用。
    * 但是**DeepWalk 生成的节点嵌入向量的可解释性通常较差**，这是因为它是一种基于**无监督学习的表示学习方法**，其生成的嵌入是高维空间中的稠密向量，缺乏直接的语义解释。

* 需要掌握的核心思想与代码

  * 建图
  * 随机采样路径（完成）
  * `skip-gram`，滑动窗口采 `pari node` 即采样 `src` 与 `dst` 点对
    * 正例样本的采样（使用滑动窗口）	（完成）
    * 负例样本的采样（使用随机选择）
  * `skip-gram` 模型训练，也即损失函数的搭建



### 图结构的度量算法

#### PathSim

* 这个方法先以三元组（或更高）的形式来构建子图，这样能够唯一确定一种结构。之后再通过路径来度量节点之间的相似度（相当于有哪些节点做了类似的事情），很妙的设计。

> ---
>
> ### 🌐 场景：学术网络
>
> 假设我们有一个异构图，有以下类型的节点：
>
> - **A**：作者（Author）
> - **P**：论文（Paper）
> - **C**：会议（Conference）
>
> 边的关系如下：
>
> - A–P：作者写了论文  
> - P–C：论文发表在会议上  
>
> ---
>
> ### 🧩 图结构（示意）
>
> 我们构造以下数据：
>
> - **作者：** A1, A2, A3  
> - **论文：** P1, P2, P3, P4  
> - **会议：** C1, C2  
>
> 边如下：
>
> - A1 写了 P1, P2  
> - A2 写了 P3  
> - A3 写了 P4  
> - P1, P3 发表在 C1  
> - P2, P4 发表在 C2  
>
> 画图表示如下（A-P-C）：
>
> ```
> A1 - P1 - C1
> A1 - P2 - C2
> A2 - P3 - C1
> A3 - P4 - C2
> ```
>
> ---
>
> ### 🔁 元路径（Meta-path）：A–P–C–P–A
>
> 我们现在看两个作者是否“结构相似”：  
> 意思是说，“他们是否都写了发表在同一个会议上的论文”。
>
> ---
>
> ### 🧮 计算 PathSim
>
> PathSim 的定义如下：
>
> $\text{PathSim}(x, y) = \frac{2 \times |\text{Path}_{x \rightarrow y}|}{|\text{Path}_{x \rightarrow x}| + |\text{Path}_{y \rightarrow y}|}$
>
> 也就是：
>
> - 分子是：$x$ 和 $y$ 之间符合元路径的路径条数 × 2  
> - 分母是：$x$ 自己到自己、$y$ 自己到自己的路径条数之和  
>
> ---
>
> ### ✅ 例子：计算 A1 和 A2 的相似度
>
> 我们走路径 A–P–C–P–A：
>
> - A1 有一条路径：A1–P1–C1–P3–A2（通过 C1）  
> - 所以 $|\text{Path}_{A1 \rightarrow A2}| = 1$  
>
> 看自己到自己的路径数：
>
> - A1–P1–C1–P1–A1、A1–P2–C2–P2–A1 → 2条  
> - A2–P3–C1–P3–A2 → 1条  
>
> 所以：
>
> $\text{PathSim}(A1, A2) = \frac{2 \times 1}{2 + 1} = \frac{2}{3} \approx 0.667$
>
> ---
>
> ### ✅ 再看看 A1 和 A3
>
> - A1–P2–C2–P4–A3（通过 C2） → 1 条  
> - A3–P4–C2–P4–A3 → 1 条  
>
> $\text{PathSim}(A1, A3) = \frac{2 \times 1}{2 + 1} = \frac{2}{3}$
>
> 所以 A1 与 A2、A3 的结构相似度是一样的。
>
> ---
>
> ### ✅ 最后看看 A2 和 A3
>
> - 没有共同会议（A2 是 C1，A3 是 C2）→ 路径数是 0  
>
> $\text{PathSim}(A2, A3) = \frac{2 \times 0}{1 + 1} = 0$
>
> ---
>
> ### 🧠 总结
>
> - PathSim 度量的是两个节点是否在图中**扮演了类似的结构角色**。  
> - 它特别适合用于**异构图**，你可以根据不同的元路径选择不同的相似度视角。  
> - 和 node2vec 这种局部游走方法相比，PathSim 更“全局”和“语义明确”。  
>
> 



## 时序算法

### RNN与LSTM

[笔记](https://zhuanlan.zhihu.com/p/473671479)

#### 小结

* 引入：slot filling

  > Slot filling 是自然语言处理（NLP）中的一个常见任务，通常出现在 **语音识别** 和 **对话系统** 中。它的目标是从用户的输入（例如语音或文本）中提取出特定的关键信息，并填充到预定义的“槽”中。
  >
  > 这些“槽”是指模型预先定义的字段，每个槽对应一个具体的信息类型（比如日期、地点、数量等）。例如，在一个天气查询系统中，可能会有如下槽：
  >
  > - **城市**：用户提到的城市名
  > - **日期**：用户询问的日期
  > - **温度**：用户关心的温度值
  >
  > 假设用户输入："我想知道明天北京的天气"，系统的任务就是识别“明天”作为日期，并识别“北京”作为城市，并将这些信息填充到相应的槽中。
  >
  > 一个简单的示例：
  >
  > - **输入**："预定明天从北京飞往上海的航班"
  > - **槽**：
  >   - **日期**: 明天
  >   - **起点**: 北京
  >   - **终点**: 上海
  >
  > 在对话系统中，槽填充通常和 **意图识别**（intent recognition）一起使用。意图识别用于确定用户的请求类型（比如查询天气、预定航班），而槽填充则用于提取用户请求中的具体参数。

* 思考为什么激活函数使用 `tahn` 而不使用 `ReLU`

* 若 nn 存在记忆力，那么他就可以解决 input 相同，但是 output 不同这一问题（台北可以是出发地，也可以是目的地，关键看语义 ）

  * 正式因为 RNN 存在“记忆力”，拥有 store 部件，这才会使得 ==**change the sequence order will change the output**，一切都将会变得 different==，一定要重点理解清楚这一点

* 思考为什么要设计得这么复杂，搞了这么多的门？其时序的性质在哪里体现？

* 通过设计“门机制”(gate) 来控制信息的记忆和遗忘，能够长期保留信息。但是为什么引入了记忆单元和门控机制，使得模型能够**长时间记住信息**，并且能决定在什么时候 **保留信息**，在什么时候 **忘记信息**

  * 它维护了两个重要状态
    * cell State（记忆单元）：长期记忆
    * Hidden State（隐藏状态）：当前时间步的输出（也是短期记忆）
  * 但是它和 Transform 的区别还是 Transform 能够做到全局注意力，而 LSTM 则依赖顺序输入。
  * **LSTM：在一条时间轴上“记忆-忘记-输出”语义单元，靠门机制来进行语义切换。**

    **Transformer：在全局空间上“比对-关注-整合”语义块，靠注意力重分布来捕捉语义边界。**

* 通俗比喻的理解，你可以把 LSTM 想象成一个人记笔记的过程：

  - **Cell State** 是他的大脑，记录他目前记住的所有信息；
  - **遗忘门**：他决定要不要把旧的内容从脑中“忘掉”；
  - **输入门**：他决定是否把新看到的内容记下来；
  - **输出门**：他决定要不要把某些记忆说出来（用于下一步任务）。
  - 另外附上我的一个比较又洞察力的理解：==LSTM 的门机制，本质上就是在每一步判断“语义是否完成”，“是否需要更新记忆”，以及“是否要输出记忆”。==
  
* 为什么在进行了 $c'=g(z)f(z_i)+cf(z_f)$ 的计算后，需要对 $c'$ 进行 $\text{softmax}$ 操作？

  * 将计算结果转换为 **概率分布**，便于选择最优的决策。
  
    对输出进行归一化，使得不同信息的贡献合理合并。
  
    在 **分类任务** 或 **生成任务** 中，根据概率进行选择。
  
    主要还是因为马上要进入输出层了
  
* **RNN 在处理隐藏状态（hidden state）和输入（input）时，常见的操作确实包括相加、拼接等方式**。

  * 最初始的版本是相加（最简化版本），一般的都是采取拼接操作拼接（这是最常见的）
  
  * 使用 `cat` 操作更容易理解其中的记忆力机制
  
* 尝试自己写代码实现一下 `LSTM` 的核心框架

#### RNN核心提要

* 如何理解RNN中的隐藏状态

  * 我们可以将RNN中的隐藏状态（假设其维度为 $h_{dim}$）理解成在机器一个字符一个字符地阅读文本时，不断地去**更新**这段**中心文本的抽象**，即 $hidden$ 这个向量就是在描述这个**中心文本的抽象**，并且会随着阅读的递进不断地更新。我们可以根据任务需求的不同来利用它，比如我们的任务是预测下一个字，那么我们的输出维度 $out_{dim}$ 应该对应词库的大小，因此我们可以通过一个线性变换将 $h_{dim}$ 维度的数据投影到 $out_{dim}$ 的数据

* ==从~~**反向传播**~~的角度理解模型的短期记忆==（**每个时间点的输出对后续的贡献**）

  * 自己举一下例子理解
    * 可以看一下这个[例子](https://github.com/GenTang/regression2chatgpt/blob/zh/ch10_rnn/bptt_example.ipynb)
    * 自己手算模拟了一下，若权重 $W$ 与 $W_h$ 都介于 0~1 之间，其实前向传播的过程中，权重也会慢慢变小，这其实才是最本质的东西
    
  * 后续对应的梯度值越小，那么它对后续信息的贡献就会更小，即数据距离越远 -> 反向传播的路径越长 -> 梯度越容易消失 -> 带来的信息量越少。比如 $h_3$ 同时会受到 $x_1, x_2,x_3$ 输入信息的影响，因为 $x_3$ 距离 $h_3$ 更近，因此 $x_3$ 的贡献最大。

  * 从**网络结构**上来看 ，循环神经网络理论上可以处理无限长的序列的关联关系，~~但是由于**反向传播算法**的限制~~（这里的表述我认为有一些问题），它只能处理距离它比较近的数据的信息。事实上，更精确的表述是：
  
    > **“谁对最终 loss 有更大贡献”，本质上是看谁在前向传播中更主导整个模型的输出；反向传播的==梯度只是量化这种贡献的方式==。而路径长度、非线性、权重等等只是==放大或衰减这种主导性==的手段。”**
  
    这一特性在学术上称为**短期记忆**
  



#### LSTM核心提要

* 这里给出一下隐藏状态的数学表达式（跟我想得差不多）：$H_i=\sum_{k=1}^{i}V_kW_h^{i-k}$，其中 $V_k = X_kW_v$，可见随着层数的加深，$H_i$ 会逐渐趋于0，因此，如果我们想解决短期记忆的问题，我们就需要打破这样一个通过线性变换直接参与更新的这一个过程，这也是长短期记忆网络的主要创新点

* 这里先做一个前景提要

  我们最简单的神经网络是多层感知机它具有以下特点并且分别被表格右边的网络“打破”。

  | MLP的特点              | 被哪个模型进行了优化 |
  | ---------------------- | -------------------- |
  | 神经元分层一维排列     | 卷积神经网络         |
  | 跨层不连接，相邻层连接 | 残差网络             |
  | 同层神经元之间相互独立 | 循环神经网络         |
  | 神经元结构简单         | 长短期记忆网络       |

* 长-短期记忆网络的结构

  * 将单一隐藏状态分解成细胞状态和隐藏状态
  * 细胞状态和隐藏状态的更新过程更加复杂，模拟人类阅读的过程
  * 可以将细胞状态理解为长期记忆，隐藏状态理解为短期记忆

* 我们**死记硬背一些模型的组件其实是没有什么卵用**的，我们完全可以依照我们的创造性自己搭建模型。而我们需要做的就是**去理解模型组件它想达到什么目的？其设计的精髓是什么？以及我们如何高效的用代码把这个设计翻译出来。**

* 一些有关内容的激活函数设置成 `tanh` 的意义：

  * tanh为负在LSTM中对应的意义是：（1）当前输入与已有记忆冲突（2）模型希望纠正或反转之前的记忆
  * tanh具有良好的性质，适合在隐藏层使用




## 其他算法

### attention is all your need

* 大概弄懂了每个张量操作所蕴含的意义（这一点很关键）

#### 一些疑问

* 为什么归一化是除以 $\sqrt{d}$ 而不是 $d$ 或者 $d^2$

  一些都是为了让数值稳定在**合理的范围**之内（理解什么是合理的范围）

  > 1. 点积的期望值随着维度的增大而增大  
  >
  > 假设我们有两个随机的高维向量 $\mathbf{Q}$ 和 $\mathbf{K}$，它们的每个元素都是独立采样的标准正态分布（均值为 $0$，方差为 $1$）。对于这两个向量的点积 $\mathbf{Q}^T \mathbf{K}$，它的期望值为：  
  > $$
  > E[\mathbf{Q}^T \mathbf{K}] = \sum_{i=1}^{d} E[Q_i K_i] = 0
  > $$
  >
  > 因为 $Q_i$ 和 $K_i$ 是独立的标准正态分布，$E[Q_i K_i] = 0$，但是它的方差是：  
  >
  > $$
  > \text{Var}[\mathbf{Q}^T \mathbf{K}] = \sum_{i=1}^{d} \text{Var}[Q_i K_i] = d
  > $$
  >
  > 所以随着维度 $d$ 的增大，点积的方差也会增大。具体来说，点积的标准差会随着 $d$ 的增大而增加：  
  >
  > $$
  > \text{Std}[\mathbf{Q}^T \mathbf{K}] = \sqrt{d}
  > $$
  >
  > 2. 归一化的目的  
  >
  > 由于点积的标准差随着维度的增大而增加，我们需要对点积进行归一化，以避免随着维度的增加，点积值变得过大，进而影响训练的稳定性。在注意力机制中，归一化操作一般是将点积结果除以 $\sqrt{d}$，从而控制其数值范围。  
  >
  > - 如果我们直接除以 $d$ 或 $d^2$，那么归一化后的结果将变得过小，可能导致梯度更新不显著，从而影响模型的训练效果。  
  > - 如果除以 $\sqrt{d}$，则可以将点积的结果保持在一个合理的范围内，避免数值过大或过小，保证训练过程中的稳定性。  
  >
  > 3. 为什么是 $\sqrt{d}$ 而不是 $d$ 或 $d^2$  
  >
  > - **归一化 by $\sqrt{d}$** 是基于点积方差的平方根来做的。在高维空间中，向量之间的点积的方差通常是与维度成正比的。由于标准差是方差的平方根，因此使用 $\sqrt{d}$ 来缩放点积可以使得归一化后的值具有合理的分布，并且避免数值过大。  
  > - 如果直接除以 $d$ 或 $d^2$，会导致点积结果被过度缩小，从而使得计算的相似度变得过于微小，这样在 softmax 操作后会导致数值不稳定或梯度消失。  
  >

