# 算法

## 聚类

### DBSCAN算法

* 理解一下流程，可以看下这个视频[DBSCAN流程](https://www.bilibili.com/video/BV17L4y147W2/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

* 算法中的各种点

  * **核心点（Core Point）**：在 `eps` 半径内至少有 `min_samples` 个数据点的点。

    **边界点（Border Point）**：在 `eps` 半径内**少于** `min_samples`，但属于某个簇的点。

    **噪声点（Noise Point）**：**既不是核心点，也不是边界点**，即它不属于任何簇，被赋值 `-1`。

## 图学习算法

### 图游走类算法

#### DeepWalk

理论可以看[这篇](https://zhuanlan.zhihu.com/p/56380812) 与 [这篇](https://zhuanlan.zhihu.com/p/397710211)

* 核心是要理解 **Rand-Walk**  与 **Skip-Gram** 的思想

  * Rand-Walk

    * 随机游走，目的是为了获取节点的**局部特征**
    * 滑动窗口采样：在游走的每一步，选择当前节点 `v` 及其前后 `w` 步的节点作为一个局部结构，生成一个训练样本对。这些节点对将用于训练模型。

  * Skip-Gram

    * 什么是**负采样（NS: Negative Sampling）思想，如何理解**

      * 负采样（Negative Sampling）是一种在训练模型时用于**处理大量类别问题**，特别是在推荐系统、词向量训练（如Word2Vec）或图神经网络中的一种优	化策略。其目的是减少计算量并加速模型训练。负采样通过从所有类别中**随机选择一部分**“负样本”，而不是每个负样本都参与计算，从而有效地降低计算量。

      * 优点：

        **减少计算量**：通过减少负样本的数量，训练过程中需要计算的点数大大减少，尤其在大规模数据集中，效果尤为显著。

        **加速收敛**：减少计算量也加速了模型的训练，使得模型可以更快地收敛。

        **有效的学习负样本特征**：负采样不仅帮助我们减小计算量，还能有效地学习负样本的特征。

      * 如何选择：
        负采样通常采用**随机选择**或者**根据某些特征选择**负样本。例如，在Word2Vec中，可以通过**词频**来调整负样本的选取概率，频率较高的词更容易被选为负样本，频率较低的词较难被选为负样本。
      * 举个例子理解：假设词汇表中有10000个词，使用传统方法，你每次训练时需要与9999个负样本（即非目标词）进行计算。而使用负采样时，你只需要与5个负样本进行计算，计算量减少了几千倍。此外，**目标词与正样本之间的关系会被强化而与负样本之间的关系将会疏远**==如何理解？怎么做到的？==
        * 充分理解对正例、负例损失函数的计算（交叉熵、人为设计！）
          负样本的损失为 $\mathcal{L}_N = -\log(1-P(neg \mid src))=-\log(1-sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{neg}))$
          正样本的损失为 $\mathcal{L}_{T}=-\log(P(pos \mid src))= -\log(sigmoid(\mathbf{v}_{src} \cdot \mathbf{v}_{pos})$

    * 目标节点 `src` , 正样本 `pos`, 负样本 `neg`

      * **正样本**：指的是符合某个特定条件或目标的样本。例如，在DeepWalk中，正样本通常是目标词和上下文词之间的实际关系。

        **负样本**：指的是那些不符合特定条件的样本。例如，在DeepWalk中，负样本是目标词和随机选取的其他词之间的关系。

* 如何用向量表示网络结构？像这种**用向量表示网络**的方式，我们可以统称为 `Network Embedding`。`Network Embdding`: 在**低纬度的空间**里表示**整个网络**，并**尽可能保持网络的结构（相似）**，向量是稠密的。得到的表达向量可以用来进行下游任务，如节点分类，链接预测，可视化或重构原始图等。

  * 它本身不直接完成特定任务，而是为下游任务（如节点分类、链接预测等）提供高质量的输入特征。
  * 因此，DeepWalk 是一个**图表示学习**方法，旨在将图的结构信息**编码**为低维向量，以便后续任务使用。
    * 但是**DeepWalk 生成的节点嵌入向量的可解释性通常较差**，这是因为它是一种基于**无监督学习的表示学习方法**，其生成的嵌入是高维空间中的稠密向量，缺乏直接的语义解释。

* 需要掌握的核心思想与代码

  * 建图
  * 随机采样路径（完成）
  * `skip-gram`，滑动窗口采 `pari node` 即采样 `src` 与 `dst` 点对
    * 正例样本的采样（使用滑动窗口）	（完成）
    * 负例样本的采样（使用随机选择）
  * `skip-gram` 模型训练，也即损失函数的搭建



