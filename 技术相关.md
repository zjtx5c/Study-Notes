# 技术相关

## 学会调试代码

* 使用 `jupyter` 进行调试
* 使用 `pycharm` `vscode` 自带的功能进行调试
  * 断点、单步执行、查看变量、查看调用栈
  * 使用 `print` 或 `logging` 进行调试

## 学会部署代码

### 学会编写 python 命令行界面解释器

* 库： `argparse`

* 核心：`argparse.ArgumentParser` 是 Python 标准库 `argparse` 中的一个核心类，用于**命令行参数的解析**。它能帮助你写出可以通过命令行灵活传参的 Python 脚本。

  * 简单理解：

    我们可以把 `ArgumentParser` 理解成一个**命令行界面的解释器**：

    1. 它**定义**你的程序可以接受哪些参数（如 `--batch_size`, `--lr` 等）；
    2. 它**解析**用户从命令行输入的参数；
    3. 它**自动生成**帮助文档，提示用户如何使用这些参数。

  * 我们先对它**创建一个实例对象**： `parser = argparse.ArgumentParser(description = "这是一个示例脚本")`

  * 使用 `.add_argument()` 添加参数

    * `--argument_name`（添加参数名称）
    * `type = `（添加参数类型）
    * `default = `（设置参数的默认值）
    * `help = `（撰写参数的说明文档）

  * 解析命令函参数 `args = parser.parse_args()`

  * 简单的例子

    ```python
    import argparse
    
    # 创建 ArgumentParser 对象
    parser = argparse.ArgumentParser(description="这是一个示例脚本")
    
    # 添加参数
    parser.add_argument('--epoch', type=int, default=10, help='训练轮数')
    parser.add_argument('--lr', type=float, default=0.001, help='学习率')
    
    # 解析命令行参数
    args = parser.parse_args()
    
    # 使用参数
    print(f"训练轮数: {args.epoch}")
    print(f"学习率: {args.lr}")
    
    ```

  * 一些问题：

    **`args` 与 `parser` 有什么区别？**

    | 名称     | 类型                           | 作用                               | 示例                                    |
    | -------- | ------------------------------ | ---------------------------------- | --------------------------------------- |
    | `parser` | `argparse.ArgumentParser` 对象 | 定义你程序能接受哪些命令行参数     | `parser.add_argument('--lr', ...)`      |
    | `args`   | **`argparse.Namespace` 对象**  | 存储用户输入的所有参数及其对应的值 | `args.lr`, `args.epochs`, `args.device` |

    如何调出其中的 `description` 与 `help` 信息

    `python tmp.py -h` 会出现：

    ```bash
    usage: tmp.py [-h] [--epochs EPOCHS] [--lr LR]
    
    这是一个示例脚本
    
    optional arguments:
      -h, --help       show this help message and exit
      --epochs EPOCHS  训练轮次
      --lr LR          学习率
    ```

    







## 使用服务器

* 连接服务器（按照PPT的要求来），下载并安装对应的代理软件进行设置，根据要求写好对应的配置文件
* 如何使用`Slurm`，可以看一下这篇[文章](https://zhuanlan.zhihu.com/p/345387783)，这里是一篇更详细的教程[北大高性能计算平台](https://hpc.pku.edu.cn/_book/)

### UESTC BCM

#### BCM 简介

> ### **Bright Computing (BrightCM / BCM) 的含义**  
>
> #### **1. Bright Computing（BrightCM）**  
> **Bright Computing** 是一家专注于 **高性能计算（HPC, High Performance Computing）**、**人工智能（AI）** 和 **数据分析** 集群管理软件的公司，提供企业级的集群部署、监控和管理解决方案。  
>
> - **主要产品**：**Bright Cluster Manager（BCM）**  
> - **功能**：  
>   - 自动化部署和管理 **HPC/AI/大数据集群**  
>   - 支持 **Linux 集群**（包括 Slurm、Kubernetes、OpenStack 等）  
>   - 提供 **GPU 加速计算** 支持（如 NVIDIA CUDA）  
>   - 适用于 **云、本地和混合环境**  
>
> #### **2. BCM（Bright Cluster Manager）**  
> **BCM** 是 **Bright Computing** 的核心产品，全称是 **Bright Cluster Manager**，主要用于：  
>
> - **快速部署和管理计算集群**（HPC、AI、Kubernetes 等）  
> - **监控节点状态、资源使用情况**（CPU、GPU、内存、存储等）  
> - **集成 Slurm、PBS、Kubernetes 等调度器**  
> - **支持多种 Linux 发行版**（如 CentOS、RHEL、Ubuntu 等）  
>
> #### **3. 常见应用场景**  
> - **科研计算**（如气象模拟、生物信息学）  
> - **AI/深度学习训练**（管理 GPU 集群）  
> - **企业级大数据分析**（如 Hadoop/Spark 集群）  
>
> ### **总结**  
> | 缩写                 | 全称                       | 含义                                                         |
> | -------------------- | -------------------------- | ------------------------------------------------------------ |
> | **Bright Computing** | -                          | 一家提供 **HPC/AI 集群管理软件** 的公司                      |
> | **BCM**              | **Bright Cluster Manager** | Bright Computing 的 **集群管理软件**，用于自动化部署、监控和管理计算集群 |
>

#### 使用流程

> 1. **创建虚拟环境**： 创建一个虚拟环境，可以隔离你的项目依赖和系统级库，确保不同项目之间的依赖不会相互干扰。
>
>    - 使用 `conda` 创建虚拟环境：
>
>      ```bash
>      conda create --name myenv python=3.8
>      conda activate myenv
>      ```
>
>    - 使用 `virtualenv` 创建虚拟环境：
>
>      ```bash
>      python -m venv myenv
>      source myenv/bin/activate  # 激活虚拟环境
>      ```
>
> 2. **加载所需的系统模块**： 激活虚拟环境后，你可以加载一些系统级模块（例如 Anaconda、CUDA、cuDNN 等），这些模块提供了你所需的全局工具和库支持。
>
>    例如，加载 Anaconda 和 CUDA 模块：
>
>    ```bash
>    module load anaconda3  # 加载 Anaconda 环境
>    module load cuda11.8/toolkit  # 加载 CUDA 工具包
>    module load cudnn8.5-cuda11.8  # 加载 cuDNN 库
>    ```
>
>    这些模块为虚拟环境提供了 CUDA 支持、GPU 加速等功能。
>
>    装载的模块实际上是配置和加载 **系统级的工具和库**，比如 **驱动程序**、**CUDA 工具包**、**cuDNN 库**、**编译器** 等。它们并不是直接安装 Python 包，而是设置和配置你的环境，使得你可以使用这些工具在系统级别进行计算，特别是在涉及到 GPU 加速的任务时，像 **CUDA** 和 **cuDNN** 就是为了让你能够使用 GPU 来加速计算。（说实话，这一步还不是很懂要干麻..）
>
> 3. **安装项目依赖到虚拟环境中**： 然后，你可以在虚拟环境中使用 `pip` 或 `conda` 安装你的项目依赖项。例如，如果你有一个 `requirements.txt` 文件列出了所有的依赖：
>
>    ```bash
>    pip install -r requirements.txt
>    ```
>
>    或者，使用 `conda` 安装依赖：
>
>    ```bash
>    conda install --file requirements.txt
>    ```
>
>    这样，所有的依赖（如 `torch`、`numpy`、`scikit-learn` 等）都会安装到你当前激活的虚拟环境中，而不会影响到系统级的其他库。
>
> 4. **使用虚拟环境中的库**： 完成安装后，你就可以在虚拟环境中运行代码，所有需要的库都会从虚拟环境中加载。这使得你的项目可以在一个干净、隔离的环境中运行，避免版本冲突和依赖问题。
>
> ### 总结：
>
> 通过这样的操作，你不仅能将需要的库和工具安装到虚拟环境中，而且可以确保这些工具和库的版本是与你项目兼容的。虚拟环境的好处在于，它使得你可以管理每个项目的独立依赖，避免了与其他项目的依赖冲突，同时利用系统提供的模块（如 CUDA）来支持 GPU 加速和其他硬件工具。
>
> ==加载这些模块的作用==
>
> | 模块命令                        | 功能                        | 若不加载的后果                                    |
> | :------------------------------ | :-------------------------- | :------------------------------------------------ |
> | `module load anaconda3`         | 提供Python环境和conda包管理 | 无法使用Anaconda安装的库（如PyTorch）             |
> | `module load cuda11.8/toolkit`  | **激活CUDA编译器、GPU驱动** | GPU不可用，代码降级到CPU运行（GPU相关）           |
> | `module load cudnn8.5-cuda11.8` | 加载深度神经网络加速库      | 深度学习模型训练/推理速度大幅下降（深度学习加速） |
>
> **必须加载这些模块（除非我们自己配置好了环境）**，否则深度学习代码要么无法运行，要么只能以极低性能在CPU上执行。这是集群环境与个人电脑的关键区别——所有软件依赖需通过模块系统显式加载。
>
> 一般来讲，我们要先申请节点才能获得显卡，不然不能用 GPU 进行训练

#### 常用命令

##### 查看队列中的作业与查看节点状态

> 在 Slurm 作业调度系统中，你可以通过以下命令查看当前的工作队列情况（包括所有用户或你自己的作业）：
>
> ---
>
> ### **1. 查看所有运行中和排队的作业**
> ```bash
> squeue
> ```
> **输出示例：**
> ```
> JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
> 15646     defq     job1   user1  R    2:03:45      1 node02
> 15648     defq     job2   user2  R    1:45:20      1 node03
> 15651     debug     job3   user3 PD       0:00      1 (Resources)
> ```
> ==**关键列说明：**==
>
> - **JOBID**：作业ID  
> - **PARTITION**：作业所在队列（如 `defq`）  
> - **USER**：提交作业的用户  
> - **ST**：状态（`R`=运行中，`PD`=排队中，`CG`=完成中）  
> - **TIME**：已运行时间  
> - **NODELIST(REASON)**：运行的节点或排队原因（如 `Resources`=资源不足）
>
> ---
>
> ### **2. 仅查看自己的作业**
> ```bash
> squeue -u $USER
> ```
> 或指定用户名：
> ```bash
> squeue -u username
> ```
>
> ---
>
> ### **3. 按队列（分区）查看**
> ```bash
> squeue -p defq  # 查看defq队列中的作业
> ```
>
> ---
>
> ### **4. 查看更详细信息**
> ```bash
> squeue -l  # 显示更详细的列（如优先级、提交时间）
> squeue --start  # 显示预计作业开始时间（需管理员配置）
> ```
>
> ---
>
> ### **5. 查看作业排队原因**
> 如果作业处于 `PD`（排队）状态，查看具体原因：
> ```bash
> squeue -j <JOBID> -o "%i %P %T %r"  # 显示作业ID、队列、状态、原因
> ```
> **常见排队原因：**
> - `Resources`：资源不足（如GPU/CPU不够）  
> - `Priority`：优先级低于其他作业  
> - `QOSMaxWallDurationPerJobLimit`：超过了队列的最大运行时间限制  
> - `PartitionNodeLimit`：分区节点数已达上限  
>
> ---
>
> ### **6. 查看节点资源占用情况**
> ```bash
> sinfo -o "%P %N %T %C %G"  # 显示分区、节点名、状态、CPU使用、GPU使用
> ```
> **输出示例：**
> ```bash
> PARTITION NODES STATE CPUS GRES  
> defq      node02 alloc 8/8 gpu:1  
> defq      node03 idle  0/8 gpu:0  
> defq* node[01-02] mixed 37/43/0/80 gpu:8
> ```
> - **STATE**：`idle`=空闲，`alloc`=已占用，`mix`=部分占用  
> - **GRES**：GPU使用情况（如 `gpu:1` 表示已用1块GPU）  
> - ==`defq* node[01-02] mixed 37/43/0/80 gpu:8` 表示==
>   - 当前**分区**包含两个**节点**：`node01` 和 `node02` 
>   - 节点处于**混合状态**：部分资源被占用，部分空闲。
>     - `idle`：完全空闲。
>     - `alloc`：完全被占用。
>     - `drain`：节点故障或维护中。
>
>   - **CPUS（A/I/O/T）（CPU使用情况）**
>     - **`37/43/0/80`**
>       - 格式为 `Allocated/Idle/Other/Total`：
>         - **`37` (Allocated)**：已分配的 CPU 核心数（正在被作业使用，**一般一个作业一个核心**）。
>         - **`43` (Idle)**：空闲的 CPU 核心数（可用资源）。
>         - **`0` (Other)**：其他状态（如离线、保留）的 CPU 核心数。
>         - **`80` (Total)**：节点总 CPU 核心数。
>       - **计算方式**：`37 (已用) + 43 (空闲) = 80 (总数)`。
>
>   - **GRES（通用资源，这里是GPU）**
>     - **`gpu:8`**
>       - 表示每个节点（`node01` 和 `node02`）**总共**有 8 块 GPU。
>       - **注意**：这里是节点 GPU 的总数，不显示已用数量（需通过 `squeue` 或 `scontrol show node` 查看具体分配）。
>
>
> ---
>
> ### **7. 查看作业的详细配置**
> ```bash
> scontrol show job <JOBID>
> ```
> **输出内容**包括：
> - 作业的请求资源（CPU/GPU/内存）  
> - 实际分配的节点  
> - 提交时间、启动时间  
> - 环境变量和命令  
>
> ---
>
> ### **8. 实时监控队列（类似`top`）**
> ```bash
> watch -n 1 squeue -u $USER  # 每秒刷新一次
> ```
> 按 `Ctrl+C` 退出监控。
>
> ---
>
> ### **常用命令总结**
> | 命令                        | 用途               |
> | --------------------------- | ------------------ |
> | `squeue`                    | 查看所有作业       |
> | `squeue -u $USER`           | 查看自己的作业     |
> | `squeue -p defq`            | 查看特定队列的作业 |
> | `sinfo`                     | 查看节点状态       |
> | `scontrol show job <JOBID>` | 查看作业详情       |
>

##### 如何申请资源与提交作业

* 选择空闲的节点申请资源（假如 node3 节点空闲，那么我们可以使用以下命令）

  `sbatch -p defq -w node03 <my_scripy.sh>`
  `sbatch`：是 Slurm的作业提交命令，它将任务提交给调度系统

  `-p defq` ：指定作业提交到哪个**分区（Partition）**，这里提交到 `defq` 分区（或者说是队列中）

  `-w node03`：强制将作业分配到 特定的计算节点（这里是 `node03`），它将绕过 Slurm 的自动调度，直接指定运行位置。**如果不想硬编码节点名（不强制选择某个节点），可通过约束条件选择首个空闲节点（Slurm会自动调度，动态选择）**如下所示：

  `sbatch -p defq <my_scripy.sh>`

  * 可以添加一些其他参数比如，`--gres=gpu:1`， `--time=1-00:00:00`
    * 如果我们的项目需要GPU资源，那么一定得申请（**当然可以将这条命令添加到训练脚本中，更推荐**），若我们的项目没有添加这条命令，那么运行时会报错==（如 `CUDA not available`）血泪教训==
    * 我们服务器上的资源限制是无限的，所以 -`-time=1-00:00:00` 这条命令完全可以不加~~

* 提交作业的脚本，事实上我可以 **完全省略命令行参数**，将所有 Slurm 配置直接写入作业脚本中，然后直接运行脚本 `my_scripy.sh`。

  ```bash
  #!/bin/bash
  # ===== Slurm 参数 =====
  #SBATCH --partition=defq     # 指定分区
  #SBATCH --nodelist=node01    # 强制指定节点（等效于 -w node01）
  #SBATCH --gres=gpu:1         # 申请 1 块 GPU
  #SBATCH --time=1-00:00:00    # 运行时间限制（1天）（我们可以不屑）
  #SBATCH --job-name=pretrain  # 作业名称
  #SBATCH --output=slurm-%j.out # 输出日志（%j 替换为作业ID）,这个是可以自定义命名的！
  #SBATCH --error=slurm-%j.err  # 错误日志, 这个是可以自定义命名的！
  
  # ===== 实际任务命令 =====
  echo "Job started on $(hostname)"
  python my_training_script.py  # 替换为你的实际命令
  ```

  | 参数              | 作用                   | 是否必选 | 示例                    | 备注                   |
  | :---------------- | :--------------------- | :------- | :---------------------- | :--------------------- |
  | **`--partition`** | 指定作业队列（分区）   | 可选     | `--partition=defq`      | 默认使用配置的默认分区 |
  | **`--nodelist`**  | 强制指定节点           | 可选     | `--nodelist=node01`     | 等效于 `-w node01`     |
  | **`--gres`**      | 申请通用资源（如 GPU） | 按需     | `--gres=gpu:1`          | 必须显式请求 GPU       |
  | **`--time`**      | 设置作业最大运行时间   | 强烈建议 | `--time=1-00:00:00`     | 格式：`D-HH:MM:SS`     |
  | **`--job-name`**  | 作业名称               | 可选     | `--job-name=pretrain`   | 便于识别               |
  | **`--output`**    | 标准输出日志路径       | 可选     | `--output=slurm-%j.out` | `%j` 自动替换为作业 ID |
  | **`--error`**     | 错误日志路径           | 可选     | `--error=slurm-%j.err`  | 可与 `--output` 合并   |

  * 接下来运行脚本

  1. **赋予脚本执行权限**（可能不需要）

  ```bash
  chmod +x pretrain_pregat_struct_FB15k.sh
  ```

  2. **直接提交脚本**

  ```bash
  sbatch my_scripy.sh
  ```



##### 监控任务进度（`squeue` 与 `sacct` 命令）

我们通常使用squeue和sacct（是 **SLURM Accounting** 即SLURM账务系统的缩写）来监控在SLURM中的作业活动。squeue是最重要、最准确的监控工具，因为它可以直接查询SLURM控制器。sacct也可以报告之前完成的任务，但是因为它是通过查询SLURM database获取信息，因此有时候sacct查到的信息和squeue查到的信息会有一点区别。

运行在不附带arguments的情况下运行squeue会显示所有当前正在运行的任务。当使用 squeue -u yourUserName的时候，会只显示你提交的任务。

**`squeue -u shencong`**

或是查询一个特定的任务：`squeue -j <job_id>`

```bash
(licap) [shencong@admin LICAP]$ squeue -j 15651

JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
15651      defq     test    wukun  R 2-15:42:32      1 node02
```

上述信息告诉我们

> 作业 ID 为 `15651` 的作业正在 `defq` 分区中运行，名字为 `test`（对应上文的 `--job-name` 参数），由 `wukun` 用户提交，状态是正在运行（R 即 Running）。
>
> 它已经运行了 2 天 15 小时 42 分 32 秒。
>
> 作业使用了 1 个节点，即 `node02`。
>
> **补充** 这里的 `NODELIST(REASON)` 的含义是：作业运行的节点名以及相关的原因（如果有）。在这个例子中，作业正在 `node02` 上运行。如果作业**由于某种原因没有运行或排队**，它会显示相应的原因。在此，`node02` 表示作业当前正在该节点上运行。



**或者我们可以通过 `sacct` 查询特定的任务细节** `sacct -j <job_id>`

```bash
(licap) [shencong@admin LICAP]$ sacct -j 15651
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
15651              test       defq    diggers         12    RUNNING      0:0 
15651.batch       batch               diggers         12    RUNNING      0:0 
```

首先理解一下这里报告的 `15651` 与 `15651.batch` ，前者是**主作业**：就是我们通过 SLURM 提交的作业，它会执行你在脚本中指定的任务。一般情况下我们提交的整个`.sh`脚本是一个主作业，主要的任务就是运行 Python 脚本 比如 `python pretrain/pregat_pretrain_struct.py --dataset FB15k_rel_two --data_path ./datasets/fb15k_rel.pk`。而后者 `15651.batch` 是**批处理作业**：是 SLURM 在后台创建的辅助作业，用于启动、调度和管理实际的主作业。在你提交主作业时，SLURM 会在后台创建一个 `.batch` 的作业来管理作业的资源分配、环境设置等任务。这个 `.batch` 作业**不直接执行你的 Python 任务**，而是**负责管理你的主作业的调度和启动**。它们是**父子关系**

理解一下这里的 `ExitCode`

> `ExitCode` 是作业执行完毕后的退出状态码，用来表示作业的执行结果。它由两个部分组成：**主退出码** 和 **子退出码**，格式为 **`exit_code:sub_exit_code`**。
>
> **主退出码**（`exit_code`）：表示作业的最终结果。通常情况下，`0` 表示作业成功完成，非 `0` 值表示作业发生了错误或异常。
>
> **子退出码**（`sub_exit_code`）：如果作业由多个任务组成，子退出码表示每个任务的状态。如果作业只是单一任务，这个值通常是 `0`。
>
> **作业运行中**：如果作业还在运行，SLURM 在作业的状态栏中显示 `RUNNING`，但它也会给出一个默认的退出码（如 `0:0`），这仅仅是表示它开始执行时没有错误。
>
> **作业完成后**：作业结束时，SLURM 会更新退出码。如果作业正常完成，退出码为 `0:0`；如果出错，则会有非零退出码（如 `1:0` 或 `2:0` 等），具体的退出码表示作业的错误类型。



我们还可以通过使用 `sacct -u <username>`   来查看指定用户今日的资源使用情况

```bash
(licap) [shencong@admin LICAP]$ sacct -u wukun 
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
15646              test       defq    diggers         12    RUNNING      0:0 
15646.batch       batch               diggers         12    RUNNING      0:0 
15648              test       defq    diggers         12    RUNNING      0:0 
15648.batch       batch               diggers         12    RUNNING      0:0 
15651              test       defq    diggers         12    RUNNING      0:0 
15651.batch       batch               diggers         12    RUNNING      0:0
```



另外，我们还可以通过 `scancel <job-id>` 来取消任务



##### 如何进行调试

这里的 slurm 限制资源，**它禁止用户直接ssh登录计算节点，防止不申请资源直接跑计算**

交互模式，以 `srun` 命令运行或者 `salloc` 命令运行（貌似目前申请不到资源，先搁了）



##### 如何上传文件

* 使用了 `scp` 命令 `scp "filepath" username@localhost:targetpath` 发现连接超时，不知道是什么原因，不想去管了，心累了
* 法二，直接把要上传的文件托上去，并且到指定路径下使用 `ls -l filename` 来监控文件的传输情况（慢的一批）

## docker

* 理解容器：
  * 出现的由来（打包整个环境，os太大）
  * 镜像运行的实例
  * 容器不再是对操作系统的完整模拟，而是利用Linux特性实现的一种特殊**进程**
    * 可以把普通的进程（即**软件和其依赖的环境**）和操作系统进行隔离
    * 实现了更加精炼的打包
    * 一个更加精简版的虚拟机（只有软件能用到的功能），但这说法只是方便理解，并不正确
  * **特点**：
    - **隔离性**：每个容器相互隔离，拥有独立的资源和环境。
    - **轻量**：共享主机内核，启动快且资源占用少。
    - **可移植**：可在任何支持 Docker 的环境中运行。
* 理解镜像
  * 是一个包含了各种环境或者服务的模板，它只是一个文件，在它里面记录了需要运行某个**软件**所需要的所有内容（相当于菜谱）
  * 一般不能直接访问，且不能修改
  * **特点**：
    - **分层存储**：由多个只读层组成，便于共享和复用。
    - **不可变**：创建后无法修改，更新时需生成新镜像。
    - **轻量**：仅包含运行应用所需的文件，体积较小。
* 二者关系
  * **镜像**：用于创建容器，类似于“类”。
  * **容器**：镜像的运行实例，类似于“对象”。

## Git && Gibhub

> 作为个人开发者，掌握以下 Git 命令基本可以应对日常开发需求：
>
> ### 基础命令
> 1. **git init**: 初始化仓库。
> 2. **git clone**: 克隆远程仓库。
> 3. **git add**: 将文件添加到暂存区。
> 4. **git commit**: 提交更改。
> 5. **git status**: 查看仓库状态。
> 6. **git log**: 查看提交历史。
> 7. **git diff**: 查看文件差异。
>
> ### 分支管理
> 8. **git branch**: 查看、创建或删除分支。
> 9. **git checkout**: 切换分支或恢复文件。
> 10. **git merge**: 合并分支。
> 11. **git rebase**: 变基分支。
>
> ### 远程操作
> 12. **git remote**: 管理远程仓库。
> 13. **git fetch**: 从远程仓库获取更新。
> 14. **git pull**: 拉取远程仓库并合并。
> 15. **git push**: 推送本地提交到远程仓库。
>
> ### 撤销与回退
> 16. **git reset**: 回退到指定提交。
> 17. **git revert**: 撤销某次提交。
> 18. **git stash**: 暂存当前工作目录。
>
> ### 标签管理
> 19. **git tag**: 创建、查看或删除标签。
>
> ### 其他实用命令
> 20. **git config**: 配置 Git。
> 21. **gitignore**: 忽略文件。
>
> 掌握这些命令后，你可以高效地进行版本控制、分支管理和团队协作。随着项目复杂度增加，你可能需要学习更多高级命令。

[视频教程（非常详细）](https://www.bilibili.com/video/BV1w14y1C7oi/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

* 理解Git（版本控制软件）的三个概念

  * **提交 Commit**，**仓库 repository**， **分支 branch**

  * 理解 License 

    > 在GitHub中，**License**（许可证）是指项目所采用的开源许可证，它规定了他人如何使用、修改和分发你的代码。选择合适的许可证对开源项目至关重要，因为它决定了项目的使用权限和法律约束。
    >
    
  * 理解 Issues
  
    * 相当于提交反馈的论坛
      * open 与 close
  
  * 使用 git 命令下载和解压到本地有什么区别
  
    * 前者的隐藏文件中有 `.git` 文件夹，是一个**仓库**；而解压到本地只是一个单纯的文件夹
    * **`.git` 文件夹保存着你之前所有的对仓库管理的记录，它保存了仓库的所有历史记录、配置信息和元数据。这个文件夹是 Git 仓库的“数据库”，包含了管理项目版本控制所需的一切内容。**
  
  * 理解**工作区**、**暂存区**与**仓库**
  
    > - **工作区**：你直接编辑文件的地方。
    >- **暂存区**：临时保存你准备提交的更改。
    > - **仓库**：保存项目的完整历史记录。
    >
    > 理解这三者的关系可以帮助你更好地掌握 Git 的工作流程，从而更高效地管理代码版本。

### 一些疑问与解答

>* 我执行了如下命令
>
>`git branch new_fun`
>
>` git switch new_fun`
>
>` git commit`
>
>` git commit`
>
>` git merge main`
>
>做到这一步，提示我分支已经是最新的啦。这是为什么（`new_fun`是在`main`上创建的）
>
>- 当你执行 `git merge main` 时，Git 会尝试将 `main` 分支的**更改合并**到当前分支（`new_fun`）。
>- 但是，如果 `main` 分支的提交历史是 `new_fun` 分支提交历史的**子集**（即 `main` 分支没有比 `new_fun` 分支更新的提交），Git 会认为 `new_fun` 分支已经包含了 `main` 分支的所有更改，因此会提示“分支已经是最新的了”。
>- 在你的操作中：
>  - `new_fun` 分支是基于 `main` 分支创建的。
>  - 你在 `new_fun` 分支上提交了两次，而 `main` 分支没有新的提交。
>  - 因此，`main` 分支的提交历史是 `new_fun` 分支提交历史的子集，Git 认为不需要合并。

如何理解分离 `HEAD` 的作用（一般使用 `git checkout` 命令来操作 `HEAD`）

> 在 Git 中，**分离 HEAD**（Detached HEAD）是指 `HEAD` 指针直接指向某个具体的提交，而不是指向一个分支引用（例如 `refs/heads/main`）。分离 HEAD 的主要目的是为了临时检查或操作某个特定的提交，而不影响任何分支。
>
> ------
>
> ### **分离 HEAD 的常见场景**
>
> 1. **查看历史提交**：
>
>    - 当你需要查看某个历史提交的内容时，可以通过分离 HEAD 直接切换到该提交。
>
>    - 例如：
>
>      ```bash
>      git checkout <commit-hash>
>      ```
>
>      这时 `HEAD` 会直接指向该提交，而不是任何分支。
>
> 2. **测试或调试历史代码**：
>
>    - 如果你需要测试某个历史提交的代码，可以分离 HEAD 并切换到该提交，进行测试或调试。
>
>    - 例如：
>
>      ```bash
>      git checkout abc123  # 切换到某个历史提交
>      ```
>
> 3. **创建新的分支**：
>
>    - 在分离 HEAD 状态下，你可以基于当前提交创建一个新的分支，然后继续开发。
>
>    - 例如：
>
>      ```bash
>      git checkout abc123  # 分离 HEAD
>      git switch -c new-branch  # 基于当前提交创建新分支
>      ```
>
> 4. **修复历史提交的问题**：
>
>    - 如果你需要修复某个历史提交的问题，可以分离 HEAD 并切换到该提交，然后创建一个新的修复分支。
>
>    - 例如：
>
>      ```bash
>      git checkout abc123  # 分离 HEAD
>      git switch -c fix-branch  # 创建修复分支
>      ```
>
> 5. **临时实验性更改**：
>
>    - 如果你有一些实验性的更改，但不想影响任何分支，可以分离 HEAD 并直接在某个提交上进行实验。
>
>    - 例如：
>
>      ```bash
>      git checkout main~2  # 切换到 main 分支的前两个提交
>      ```

### 使用 SSH 协议推送本地代码到 GitHub 仓库

* 看下这个[视频](https://www.bilibili.com/video/BV1dV411G77N/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)
* 修改地址为：
  
  * `git remote set-url origin git@github.com:zjtx5c/仓库名` （SSH协议版本）
  *  `git remote set-url origin https://github.com/zjtx5c/仓库名   `（Http协议版本）
  
  使用 `git remote -v` 进行查看

### 基本命令

推荐在此[网站](https://learngitbranching.js.org/?locale=zh_CN)加深印象与理解

* `U`: untracked	未被git跟踪

* `M`: modified	修改的文件

* `git checkout` 是 Git 中一个多功能命令，既可以用于切换分支，也可以用于恢复文件。它的具体行为取决于你如何使用它。

==由于 `git checkout` 的功能过于复杂，Git 2.23 版本引入了两个新命令来替代它的部分功能：==

​	**`git switch`**：专门用于**切换分支。**

```bash
git switch <branch-name>
```

​	**`git restore`**：专门用于恢复文件（恢复到文件上一次提交的状态）。

```bash
git restore <file>
```

​	这两个命令让操作更加清晰，建议在新版本中使用它们。



* `git init`
  * 初始化，将一个文件夹初始化为仓库，即建立一个 `.git` 文件。这样处理之后，我们才具有**提交**、**回滚**等功能
  
* `git add -A` 或 `git add .` 
  * 将所有的文件都**提交到暂存区**
  * 一般来讲更改区即为工作区，可以通过手动添加的方式将工作区的文件提交到暂存区
  * vscode 中的 + 号
* `git commit -m "描述信息"`
  * 第一次一般是 “first commit”
  * 事实上，我们在 `vscode` 中的源代码管理器下写入 “first commit” 再按 "ctrl" + "enter" 即可完成提交。

* 描述信息规范

> Git 提交（`git commit -m "message"`）的命名通常有一定的规范，好的 commit message 可以让团队协作更高效，代码历史更清晰。以下是一些常见的命名方式和最佳实践：
>
> ------
>
> ## **1. 通用命名格式**
>
> 常见的格式是：
>
> ```
> <类型>(<范围>): <简要描述>
> ```
>
> - **`<类型>`**：表示提交的类型，例如 `feat`（新增功能）、`fix`（修复 bug）等。
> - **`<范围>`**（可选）：修改影响的模块或文件，如 `models`、`layers`、`train` 等。
> - **`<简要描述>`**：简明扼要地说明改动内容。
>
> ### **示例**
>
> ```bash
> git commit -m "feat(models): add GCN model"
> git commit -m "fix(layers): fix gradient issue in GAT"
> git commit -m "refactor(data): optimize graph loading process"
> ```
>
> ------
>
> ## **2. 常见的 `<类型>`**
>
> | 类型       | 含义                   | 示例                                                 |
> | ---------- | ---------------------- | ---------------------------------------------------- |
> | `feat`     | 新功能                 | `feat(train): add early stopping support`            |
> | `fix`      | 修复 bug               | `fix(layers): correct dropout implementation`        |
> | `docs`     | 文档更新               | `docs(readme): update installation guide`            |
> | `style`    | 代码格式（不影响功能） | `style(models): reformat code with black`            |
> | `refactor` | 代码重构（不影响功能） | `refactor(train): simplify training loop`            |
> | `perf`     | 性能优化               | `perf(layers): improve GAT attention computation`    |
> | `test`     | 添加或修改测试         | `test(utils): add unit tests for data preprocessing` |
> | `chore`    | 构建过程或工具调整     | `chore(deps): update PyTorch version`                |
> | `ci`       | CI/CD 配置             | `ci(github-actions): add lint check`                 |
>
> ------
>
> ## **3. 编写详细的 Commit Message**
>
> 如果修改内容较多，建议使用**多行提交信息**：
>
> ```bash
> git commit -m "fix(train): fix learning rate scheduler bug
> 
> Fixed an issue where the learning rate scheduler was not correctly applied
> in the training loop. The scheduler now properly updates the learning rate 
> after each epoch."
> ```
>
> 这样，在 `git log` 或 `git show` 时，可以看到详细的修改背景。
>
> ------
>
> ## **4. Git Commit 的最佳实践**
>
> ✅ **使用动词的现在时态**（避免过去式，如 "fixed"）
>  ✅ **保持简洁**（首行尽量控制在 50 个字符以内）
>  ✅ **提供背景信息**（多行 message 解释问题和解决方案）
>  ✅ **遵循团队约定**（如果团队有特定的 commit 规范，尽量遵守）
>
> ------
>
> ### **示例**
>
> ```bash
> git commit -m "feat(models): implement GraphSAGE with attention"
> git commit -m "fix(train): resolve gradient vanishing issue"
> git commit -m "refactor(utils): remove redundant tensor operations"
> ```
>
> 你可以选择最适合自己项目的格式，或者和团队协商一个固定的 commit 规范。🚀

* `git log --stat`

  * **查看提交的历史**：
  * 每一个 commit 都会有一个哈希值（唯一）

* `git checkout <filename>`

  * 在**工作区**回滚到最初版本
  * 事实上 vscode 中有 `放弃更改` 这个选项，能起到同样的功能

* `git reset HEAD <file>`

  * 将文件从**暂存区移出**，**但保留工作目录中的更改**。

  * > ### **`HEAD` 的含义**
    >
    > 在 Git 中，**`HEAD`** 是一个非常重要的概念，它表示当前所在的**提交（commit）\**或\**分支（branch）**。简单来说，`HEAD` 是一个指针，指向你**当前工作的位置**。以下是 `HEAD` 的详细解释：
    >
    > 1. **指向当前提交**
    >    - `HEAD` 通常指向当前分支的最新提交。例如，如果你在 `main` 分支上，`HEAD` 就指向 `main` 分支的最新提交。
    > 2. **指向分支或提交**
    >    - `HEAD` 可以指向一个分支（如 `refs/heads/main`），也可以直接指向一个具体的提交（在“分离头指针”状态下）。
    > 3. **表示工作目录的状态**
    >    - 工作目录中的文件内容与 `HEAD` 指向的提交一致（除非有未提交的更改）。

  * vscode 中的 - 号

  * `git reset HEAD .`

    * 可以使用 `.` 来撤销**所有暂存的文件**

* `git checkout -- file`（回退到**上一次提交的状态**）

  * ==更推荐使用 `git restore file`==
  * 这条命令的作用是将文件恢复到 **上一次提交的状态**，无论该文件是否已经被暂存。
  * 如果你想让 `file` 回到**最近一次提交的状态**（**丢弃修改**），可以使用此命令
  * 为什么需要 `--`

    > 在 Git 命令中，`--` **用于分隔命令选项和文件路径**，防止文件名与选项混淆。具体到 `git checkout -- file`，`--` 的作用是明确告诉 Git 后面的内容是文件路径，而不是命令选项。
    >
    > ### 为什么需要 `--`
    > 有些文件名可能以 `-` 开头，例如 `-file.txt`。如果不加 `--`，Git 可能会将其误认为选项，导致错误。使用 `--` 可以避免这种歧义。
    >
    > ### 示例
    > - **正常文件名**：
    >   ```bash
    >   git checkout -- file.txt
    >   ```
    >   这里的 `--` 表示 `file.txt` 是文件路径。
    >
    > - **以 `-` 开头的文件名**：
    >   ```bash
    >   git checkout -- -file.txt
    >   ```
    >   如果没有 `--`，Git 会误将 `-file.txt` 当作选项，导致错误。
    >
    > ### 总结
    > `--` 的作用是区分命令选项和文件路径，确保 Git 正确解析文件名，避免歧义。

### 分支

#### `git Merge`

在 Git 中，创建新分支是一个常见的操作，通常用于**开发新功能**、**修复问题**或**隔离实验性代码**。以下是创建新分支的详细步骤

创建分支后需切换到新分支，这是一个好习惯！！！

* `git merge branch_name`：**`git merge` 是将指定分支 `branch_name` 的更改合并到当前分支（即 `HEAD` 指向的分支）**

* 一个重要的问题 `git checkout BigFix` 和 `git merge main`  为什么要这样子操作？其意义是什么？

  > - **场景 1：保持 `BigFix` 分支与 `main` 分支同步**
  >   - 
  >     在团队协作中，`main` 分支通常是主分支，包含最新的稳定代码。
  >   - 如果你在 `BigFix` 分支上开发了一段时间，`main` 分支可能已经有了新的提交（例如其他开发者的更改）。
  >   - 为了确保 `BigFix` 分支基于最新的代码，你需要将 `main` 分支的更改合并到 `BigFix` 分支。
  >
  > * **场景 2：避免未来的冲突**
  >  - 如果 `main` 分支和 `BigFix` 分支都修改了相同的文件，将来将 `BigFix` 合并回 `main` 时可能会产生冲突。
  >   
  >  - 提前将 `main` 的更改合并到 `BigFix`，可以在本地解决冲突，减少未来的合并问题。
  >   
  >* **场景 3：测试最新代码**
  >   - 你可能需要在 `BigFix` 分支上测试 `main` 分支的最新更改，以确保你的代码与最新代码兼容。
  
* `git checkout -b <branch>`
  * **`git checkout`**
    - 用于切换分支或恢复文件。
    - 当切换分支后，`HEAD` 会指向新的分支
  * **`-b` 选项**
    - 表示**创建一个新分支**。
  * `<branch>`
    - 这是新分支的名称。
  * 相当于 `git branch <branch>` 与 `git checkout branch`

 #### `git Rebase`

> ## Git Rebase
>
> 第二种合并分支的方法是 `git rebase`。Rebase 实际上就是取出一系列的**提交记录**，“复制”它们，然后在另外一个地方逐个的放下去。
>
> Rebase 的优势就是可以**创造更线性的提交历史**，这听上去有些难以理解。如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。

* `git Rebase <main>`
  * 这将把 `HEAD` 指向的分支的提交重新应用到 `main` 分支的最新提交之后,形成一条**直线**。



### 分离 `HEAD`

* 我们有必要先学习在你项目的**提交树上前后移动**的几种方法。一旦熟悉了如何在 Git 提交树上移动，你驾驭其它命令的能力也将水涨船高！

* 我们首先看一下 “HEAD”。 HEAD 是一个对当前所在分支的符号引用 —— 也就是指向你正在其基础上进行工作的提交记录。

  HEAD 总是指向**当前分支上最近一次提交记录**。大多数修改提交树的 Git 命令都是**从改变 HEAD 的指向开始的。**

  HEAD **通常情况下是指向分支名**的（如 bugFix）。在你提交时，改变了 bugFix 的状态，这一变化通过 HEAD 变得可见。

* ==分离的 HEAD 就是让其指向了某个具体的**提交记录**而不是**分支名**。==

  * 在这个项目上 `main` 这种是分支名，`c1`，`c2`, `c3` 这种是分支名
  
  > 在命令执行之前的状态如下所示
  >
  > `HEAD -> main -> C1`
  >
  > HEAD 指向 main， main 指向 C1
  >
  > 执行 `git  checkout c1`
  >
  > 现在变成了
  >
  > `HEAD -> C1`
  

### 相对引用

* 相对引用的核心作用是**灵活地定位提交记录**，比如移动分支或者移动 `HEAD`

* 通过指定提交记录哈希值的方式在 Git 中移动不太方便。在实际应用时，并没有像本程序中这么漂亮的可视化提交树供你参考，所以你就不得不用 `git log` 来查查看提交记录的哈希值。

  并且哈希值在真实的 Git 世界中也会更长（译者注：基于 SHA-1，共 40 位）。例如前一关的介绍中的提交记录的哈希值可能是 `fed2da64c0efc5293610bdd892f82a58e8cbc5d8`。舌头都快打结了吧...

  比较令人欣慰的是，Git 对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入`fed2` 而不是上面的一长串字符。

* 正如我前面所说，通过哈希值指定提交记录很不方便，所以 Git 引入了相对引用。这个就很厉害了!

  使用相对引用的话，你就可以从一个易于记忆的地方（比如 `bugFix` 分支或 `HEAD`）开始计算。

  相对引用非常给力，这里我介绍两个简单的用法：

  - 使用 `^` 向上移动 1 个提交记录
  - 使用 `~<num>` 向上移动多个提交记录，如 `~3`

* 你也可以将 `HEAD` 作为相对引用的参照。

  * `git checkout HEAD^`
  * `git branch -f main HEAD~3`：上面的命令会将 main 分支强制指向 HEAD 的第 3 级 parent 提交。
    * `git branch`: 用于分支管理（创建、删除、查看等）
    * `-f` 表示 forces 表示强制操作

### 撤销变更

* 在 Git 里撤销变更的方法很多。和提交一样，撤销变更由底层部分（暂存区的独立文件或者片段）和上层部分（变更到底是通过哪种方式被撤销的）组成。我们这个应用主要关注的是后者。

  主要有两种方法用来撤销变更 —— 一是 `git reset`，还有就是 `git revert`。接下来咱们逐个进行讲解。

#### `git reset`

* `git reset` 通过把**分支记录**回退几个提交记录来实现撤销改动。你可以将这想象成**“改写历史”**。`git reset` 向上移动分支，原来指向的提交记录就跟从来没有提交过一样。
  * `git reset HEAD~1`： Git 把 main 分支移回到 `C1`；现在我们的本地代码库根本就不知道有 `C2` 这个提交了。
  * 在reset后， `C2` 所做的变更还在（**即工作区正常**），但是处于未加入暂存区状态。
  * 用于**回退到某个提交**，丢弃之后的提交记录。
  * 适合本地分支的撤销操作，**会改变提交历史**。
  * 常用于撤销**未推送的提交**。

#### `git Revert`

* 虽然在你的本地分支中使用 `git reset` 很方便，但是这种“改写历史”的方法对大家一起使用的**远程分支是无效的**哦！

  为了撤销更改并**分享**给别人，我们需要使用 `git revert`。

  * `git revert HEAD`
  * 用于**创建一个新的提交来撤销某次提交的更改**，保留原始提交记录。
  * 适合已经**推送到远程仓库**的撤销操作，**不会改变提交历史**。
  * 常用于**撤销已推送的提交**。

* 切记：**使用 `git revert` 之后，通常需要执行 `git push` 操作**。这是因为 `git revert` 会创建一个新的提交来撤销之前的更改，但这个新的提交只存在于你的本地仓库。

* `git revert HEAD` 撤销最新的提交。

* `git revert HEAD~` 撤销倒数第二个提交。

### 自由修改提交树

#### `git Cherry-pick`

~~鸽了~~（主要的都学得差不多了，等以后真要用上了再学习）



### `Git` 技术、技巧与贴士大集合

#### 只取一个提交记录

* 来看一个在开发中经常会遇到的情况：我正在解决某个特别棘手的 Bug，为了便于调试而在代码中添加了一些调试命令并向控制台打印了一些信息。

  这些调试和打印语句都在它们各自的提交记录里。最后我终于找到了造成这个 Bug 的根本原因，解决掉以后觉得沾沾自喜！

  最后就差把 `bugFix` 分支里的工作合并回 `main` 分支了。你可以选择通过 fast-forward 快速合并到 `main` 分支上，但这样的话 `main` 分支就会包含我这些调试语句了。你肯定不想这样，应该还有更好的方式……

### 远程操作—Push & Pull —— Git 远程仓库！

#### 第一次提交，设置上游分支

* 在第一次提交并推送到远程仓库时，需要使用 `git push --set-upstream origin main` 命令。这个命令做了两件事：

  1. **推送本地分支到远程仓库**：将本地的 `main` 分支推送到远程仓库。
  2. **设置上游分支**：将本地的 `main` 分支与远程的 `main` 分支关联，之后你就可以直接使用 `git push` 和 `git pull` 来同步数据，而无需每次都指定远程分支。

  一旦你完成了这一步，后续的推送就可以直接使用 `git push`，不需要再加 `--set-upstream`，因为已经有了上游分支的关联。

  例如，第二次推送时，你只需要运行：

#### `git clone`

* 直到现在, 教程都聚焦于**本地**仓库的操作（branch、merge、rebase 等等）。但我们现在需要学习远程仓库的操作 —— 我们需要一个配置这种环境的命令, 它就是 `git clone`。 
* 从技术上来讲，`git clone` 命令在真实的环境下的作用是在**本地**创建一个远程仓库的拷贝（比如从 github.com）。 *但在我们的教程中使用这个命令会有一些不同 —— 它会在远程创建一个你本地仓库的副本。显然这和真实命令的意思刚好相反，但是它帮咱们把本地仓库和远程仓库关联到了一起.*

#### 远程分支

* 你可能注意到的第一个事就是在我们的本地仓库多了一个名为 `o/main` 的分支, 这种类型的分支就叫**远程**分支。由于远程分支的特性导致其拥有一些特殊属性。远程分支反映了远程仓库(在你上次和它通信时)的**状态**。这会有助于你理解本地的工作与公共工作的差别 —— 这是你与别人分享工作成果前至关重要的一步.远程分支有一个**特别的属性**，在你切换到远程分支时，**自动进入分离 HEAD 状态**。Git 这么做是出于不能直接在这些分支上进行操作的原因, 你必须在别的地方完成你的工作, （更新了远程分支之后）再用远程分享你的工作成果。

* > 我的思路：意思就是说你现在打算操作远程，将HEAD分离是为了防止操作到本地仓库是吗？
  >
  > 你的理解基本正确，但需要稍微澄清一下细节。Git 在切换到远程分支时自动进入 **分离 HEAD 状态**，并不是为了防止操作本地仓库，而是因为 **远程分支是只读的**，Git 的设计不允许直接在这些分支上进行操作。
  >
  > 你的理解基本正确，但需要稍微澄清一下细节。Git 在切换到远程分支时自动进入 **分离 HEAD 状态**，并不是为了防止操作本地仓库，而是因为 **远程分支是只读的**，Git 的设计不允许直接在这些分支上进行操作。以下是对这段话的详细解释：
  >
  > ---
  >
  > ### 1. **远程分支是只读的**
  > - 远程分支（例如 `origin/feature`）是 Git 用来跟踪远程仓库状态的指针。
  > - 这些分支是只读的，你不能直接在远程分支上进行提交或修改。
  > - Git 的设计理念是：**远程分支只能通过从远程仓库拉取（fetch/pull）来更新，而不能直接修改**。
  >
  > ---
  >
  > ### 2. **切换到远程分支时进入分离 HEAD 状态**
  > - 当你运行 `git checkout origin/feature` 时，Git 会将 HEAD 指向远程分支 `origin/feature` 所指向的提交。
  > - 由于远程分支是只读的，Git 不会让你直接在这个分支上进行操作，因此它会进入 **分离 HEAD 状态**。
  > - 在分离 HEAD 状态下，HEAD 直接指向一个提交，而不是一个本地分支。
  >
  > ---
  >
  > ### 3. **为什么 Git 要这么做？**
  > - **防止误操作**：远程分支是只读的，Git 通过进入分离 HEAD 状态来明确提醒你，你不能直接在这些分支上工作。
  > - **明确工作流程**：Git 希望你基于远程分支创建一个本地分支，然后在本地分支上进行开发。完成工作后，再将本地分支推送到远程仓库。
  > - **分离 HEAD 状态是一个临时状态**：它适合查看代码或调试，但不适合长期开发。
  >
  > ---
  >
  > ### 4. **正确的操作流程**
  > 如果你想基于远程分支进行开发，应该按照以下步骤操作：
  > 1. **创建本地分支并跟踪远程分支**：
  >    
  >    ```bash
  >    git checkout -b feature origin/feature
  >    ```
  >    这会创建一个本地分支 `feature`，并让它跟踪远程分支 `origin/feature`。
  >    
  > 2. **在本地分支上开发**：
  >    在 `feature` 分支上进行修改和提交。
  >
  > 3. **将本地分支推送到远程仓库**：
  >    
  >    ```bash
  >    git push origin feature
  >    ```
  >
  > ---
  >
  > ### 5. **总结**
  > - Git 在切换到远程分支时**自动进入**分离 HEAD 状态，是因为远程分支是只读的，不能直接操作。
  > - 分离 HEAD 状态是一个临时状态，提醒你不能直接在远程分支上工作。
  > - 正确的做法是基于远程分支创建一个本地分支，然后在本地分支上开发，最后将更改推送到远程仓库。
  > - 这种设计是为了保证工作流程的清晰性和安全性，而不是为了防止操作本地仓库。

#### `git fetch`

* Git 远程仓库相当的操作实际可以归纳为两点：向远程仓库传输数据以及从远程仓库获取数据。既然我们能与远程仓库同步，那么就可以分享任何能被 Git 管理的更新（因此可以分享代码、文件、想法、情书等等）。

  本节课我们将学习如何从远程仓库获取数据 —— 命令如其名，它就是 `git fetch`。

  你会看到当我们从远程仓库获取数据时, 远程分支也会更新以反映最新的远程仓库。

* `get fetch` 做了些什么

  * `git fetch` 完成了仅有的但是很重要的两步:

    - 从远程仓库下载本地仓库中缺失的提交记录
    - 更新远程分支指针(如 `o/main`)

  * `git fetch` 实际上将本地仓库中的远程分支更新成了远程仓库相应分支最新的状态。

    如果你还记得上一节课程中我们说过的，远程分支反映了远程仓库在你**最后一次与它通信时**的状态，`git fetch` 就是你与远程仓库通信的方式了！希望我说的够明白了，你已经了解 `git fetch` 与远程分支之间的关系了吧。

  * `git fetch` 通常通过互联网（使用 `http://` 或 `git://` 协议) 与远程仓库通信。

* `get fetch` 不会做的事

  * `git fetch` **并不会改变你本地仓库的状态**。它不会更新你的 `main` 分支，也不会修改你磁盘上的文件。
  * 理解这一点很重要，因为许多开发人员误以为执行了 `git fetch` 以后，他们本地仓库就与远程仓库同步了。它可能已经将进行这一操作所需的所有数据都下载了下来，但是**并没有**修改你本地的文件。我们在后面的课程中将会讲解能完成该操作的命令 
  * 所以, 你可以将 `git fetch` 的理解为单纯的下载操作。

#### `git pull`

* 既然我们已经知道了如何用 `git fetch` 获取远程的数据, 现在我们学习如何将这些变化更新到我们的工作当中。
* 其实有很多方法的 —— 当远程分支中有新的提交时，你可以像合并本地分支那样来合并远程分支。也就是说就是你可以执行以下命令:
  - `git cherry-pick o/main`
  - `git rebase o/main`
  - `git merge o/main`
  - 等等
* 实际上，由于**先抓取更新再合并到本地分支**这个流程很常用，因此 Git 提供了一个专门的命令来完成这两个操作。它就是我们要讲的 `git pull`。
* `git pull` 就是 `git fetch` 和 `git merge` 的缩写！

#### 模拟团队合作

* 直接去看题吧，理解一下这个过程

#### `git push`

* 嗯，上传自己分享内容与下载他人的分享刚好相反，那与 `git pull` 相反的命令是什么呢？`git push`！
* `git push` 负责将**你的**变更上传到指定的远程仓库，并在远程仓库上合并你的新提交记录。一旦 `git push` 完成, 你的朋友们就可以从这个远程仓库下载你分享的成果了！
* 你可以将 `git push` 想象成发布你成果的命令。它有许多应用技巧，稍后我们会了解到，但是咱们还是先从基础的开始吧……
* *注意 —— `git push` 不带任何参数时的行为与 Git 的一个名为 `push.default` 的配置有关。它的默认值取决于你正使用的 Git 的版本，但是在教程中我们使用的是 `upstream`。 这没什么太大的影响，但是在你的项目中进行推送之前，最好检查一下这个配置。*

* **好好理解这段话**
  * ==过去了, 远程仓库接收了 `C2`，远程仓库中的 `main` 分支也被更新到指向 `C2` 了，我们的远程分支 (o/main) 也同样被更新了。**所有的分支都同步了！**==
* 做了一下题，突然感觉 `o/main` 在本地的存在貌似是为了**记录下**我们之前获取的远程仓库的信息！一定是这样！

#### 偏离的提交历史

* 现在我们已经知道了如何从其它地方 `pull` 提交记录，以及如何 `push` 我们自己的变更。看起来似乎没什么难度，但是为何还会让人们如此困惑呢？

* 困难来自于远程库提交历史的**偏离**。
* 假设你周一克隆了一个仓库，然后开始研发某个新功能。到周五时，你新功能开发测试完毕，可以发布了。但是 —— 天啊！你的同事这周写了一堆代码，还改了许多你的功能中使用的 API，这些变动会导致你新开发的功能变得不可用。但是他们已经将那些提交推送到远程仓库了，因此你的工作就变成了基于项目**旧版**的代码，与远程仓库最新的代码不匹配了。
* 这种情况下, `git push` 就不知道该如何操作了。如果你执行 `git push`，Git 应该让远程仓库回到星期一那天的状态吗？还是直接在新代码的基础上添加你的代码，亦或由于你的提交已经过时而直接忽略你的提交？
* 因为这情况（历史偏离）有许多的不确定性，Git 是不会允许你 `push` 变更的。实际上**它会强制你先合并远程最新的代码**，然后才能分享你的工作。

* ==看见了吧？什么都没有变，因为命令失败了！`git push` 失败是因为你最新提交的 `C3` 基于远程分支中的 `C1`。而远程仓库中该分支已经更新到 `C2` 了，所以 Git 拒绝了你的推送请求。==（很妙的设计）
* 那该如何解决这个问题呢？很简单，你需要做的就是**使你的工作基于最新的远程分支**。
  * 有许多方法做到这一点呢，不过最直接的方法就是通过 rebase 调整你的工作。咱们继续，看看怎么 rebase！







## 如何做复现

* 步骤

  * （1）找自己方向的有开源代码的顶会顶刊论文，没有开源代码的论文一律视为路边一条，看都别看一眼（血泪教训）。

  * （2）将论文 `git` 或者解压到本地，尝试跑通，如果跑不通或者跑通了但是和论文实验结果差距明显过大，也视为路边一条，放弃该篇论文。若跑通且与实验结果差不多，那么就可以精读论文正文了。

    * 如何跑通实验

      * 0）读 `README.md` 文件（一般会教你创建一个虚拟环境，安装依赖，运行哪个文件教你跑通代码）

        1）创建一个新的虚拟环境：在终端（确保的你的 conda 命令可行）进入到你想安装各种包的路径下，创建一个新的虚拟环境，顺便可以将 `python` 版本安装，命令格式为 `conda create --prefix your_path\env_name python=3..` 举例：
        `conda create --prefix D:\Users\5c\anaconda3\envs\licap python=3.7.13 ` 表明我在 `D:\Users\5c\anaconda3\envs\` 这个路径下创建了 `licap` 这个虚拟环境，并顺便安装了 `python = 3.7.13` 

        2）安装依赖，可以直接在终端或者idea（比如 vscode）中安装。但是需要进入到项目的文件地址目录下（一般该目录下会有一个 `requirements.txt`） 文件。执行 `pip install -r requirements.txt` 命令安装依赖

      * 按照 `README.md` 的要求进行实验

* 其他补充

  * 如何在 `jupyter` 中使用指定的虚拟环境

    * 0）将虚拟环境添加为 Jupyter 内核： 在虚拟环境中，使用 `ipykernel` 来将该环境添加为一个 Jupyter 内核。`python -m ipykernel install --user --name=your_env_name --display-name "Python (your_env_name)"`

      * `your_env_name` 是我们虚拟环境名称。

        `--display-name` 是希望在 Jupyter 中看到的内核名称（您可以根据需要进行修改）。

      1）选择虚拟环境作为内核： 启动 Jupyter 后，创建一个新笔记本或打开现有笔记本。在 Jupyter 界面的菜单中选择 **Kernel** -> **Change Kernel**，然后选择您添加的虚拟环境内核（例如“Python (your_env_name)”）。
