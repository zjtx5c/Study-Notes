# 数学相关

## 基础

### 求导与梯度

视频推荐看[李沐老师](https://www.bilibili.com/video/BV1eZ4y1w7PY/?spm_id_from=333.1387.collection.video_card.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)，评论区下方有知乎链接

* 标量导数（高中常识）
* 亚导数
  * 将导数拓展到不可微的函数（分类讨论）

#### 向量与标量之间的求导

* 将导数拓展到向量（核心是要**搞对形状**）

* 有一个约定俗称（分子布局）

  > 在许多机器学习和优化领域，**默认约定是**：
  >
  > - **标量对向量求导，结果是行向量**（即 $1 \times n$）（这就是梯度方向）。
  >   - **标量对向量求导**通常被定义为行向量，因为它可以直接与后续的矩阵乘法操作兼容，避免了额外的转置操作，使得在计算时更简洁。
  > - **向量对标量求导，结果是列向量**（即 $n \times 1$）。
  >
  > 这样设计的原因有
  >
  > （1）**确保梯度与雅可比矩阵的统一**
  >
  > 在多变量微积分中，向量或矩阵的导数通常遵循 **雅可比矩阵（Jacobian Matrix）** 的定义：
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  > \begin{bmatrix} 
  > \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  > \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  > \vdots & \vdots & \ddots & \vdots \\ 
  > \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  > \end{bmatrix} 
  > \in \mathbb{R}^{m \times n}
  > $$
  > 这个定义确保：
  >
  > - 如果 $\mathbf{y}$ 是一个**标量**（即 $m=1$），那么结果是行向量。
  > - 如果 $\mathbf{x}$ 是一个**列向量**，那么结果符合矩阵乘法规则。
  >
  > （2）**使得链式法则自然成立**
  >
  > 在链式法则中，如果 
  > $$\mathbf{z} = f(\mathbf{y})$$
  > 且 
  > $$\mathbf{y} = g(\mathbf{x})$$
  > 则：
  >
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
  > $$
  >
  > 其中：
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \text{ 是 } k \times m \text{（雅可比矩阵）}
  > $$
  >
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \text{ 是 } m \times 1 \text{（列向量）}
  > $$
  >
  > 矩阵乘法要求中间维度对齐，因此 
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} \text{ 必须是 } k \times 1 \text{（列向量）}
  > $$
  > （3）在**神经网络反向传播**中，梯度计算更自然（无需转置）。
  >
  > 反向传播的本质是沿着计算图按链式法则逐层传递梯度。当采用：
  >
  > 标量损失对参数矩阵的梯度 $\frac{\partial L}{\partial W}$ 与参数同形（即若 $W$ 是 $m \times n$，则梯度也是 $m \times n$）
  >
  > 中间变量的梯度 $\frac{\partial z}{\partial x}$ 按雅可比矩阵形式传播
  >
  > 这种约定能保证：
  >
  > - **维度自动对齐**：在链式法则的矩阵乘法中，中间维度天然匹配，无需手动插入转置操作。
  > - **编程一致性**：框架（如PyTorch）的 `autograd` 会统一处理张量的梯度形状，与数学约定解耦。

#### 向量与向量之间的求导

* 雅可比矩阵：可以这样理解，将 $\mathbf{y}$ 中的每个元素（标量）分别对 $\mathbf{x}$ 求导，每个元素得到一个行向量，最终得到 $m$ 个行向量！

  $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  \begin{bmatrix} 
  \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  \vdots & \vdots & \ddots & \vdots \\ 
  \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  \end{bmatrix} 
  \in \mathbb{R}^{m \times n}$

#### 有关矩阵的求导

* 有空再补



## 信息论相关（常涉及一些损失函数）

#### InfoNCE loss

初见：[Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs](https://arxiv.org/pdf/2402.17791)

它基于信息论的思想，通过比较**正样本和负样本的相似性**来学习模型参数，从而提高特征的区分度。

其一般形式为：
$$
\text{InfoNCE Loss} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{A^+}{A^+ + A^-}
$$
首先，分式部分一定是介于(0,1)之间的，而log在（0，1）之间是单增的且函数值小于0

在损失优化过程中，我们希望达成的结果是 $A^+$ 尽可能大，也就是正样本之间的距离尽可能尽，其实也隐含着与负样本之间的相似度尽可能低，距离尽可能远。从公式上来看，**我们在最小化loss的过程中，需要让公式接近0，也就是让log内部的分式接近1，要达到这个效果，应该使 $A^+ >> A^-$，可以发现跟我们的训练思路是吻合的，这就达到了对于查询向量而言，推近它和正样本之间的距离，拉远它和负样本的距离**。

#### 对比学习

* [视频链接](https://www.bilibili.com/video/BV1uf4y1A7KC/?spm_id_from=333.337.top_right_bar_window_history.content.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

* 小结一下

  * 使用**随机失活来构造相似的正例样本**这个 `trick` 比较妙

  * 理解损失函数以及如何度量嵌入向量的好坏

    * 理解Alignment（对齐）与Uniformity（均匀性）

      > 在机器学习和深度学习中，**alignment** 和 **uniformity** 这两个术语常常被用来描述模型或表示学习的质量，尤其是在图嵌入、对比学习、以及其他优化任务中。它们通常涉及到如何对齐或保持表示的分布特性。我们可以分别来看这两个概念。
      >
      > 1. **Alignment（对齐）**
      >
      > **Alignment** 通常指的是模型学习到的表示在某种目标或参考标准上的一致性或对齐。在对比学习、迁移学习等任务中，alignment 可以指：
      >
      > - **正样本的对齐**：当学习表示时，模型应该使得相似的样本在嵌入空间中距离较近，即使相似的实例对齐。例如，在图像对比学习中，正样本对（相似图片）应该被映射到表示空间中的同一位置或相邻位置。
      > - **目标一致性**：在一些任务中，我们希望模型学习到的表示能够对齐到某个先验目标（如类别标签）。例如，在分类任务中，模型的输出表示应该能够对齐到目标标签空间，使得同一类别的样本在表示空间中尽量接近。
      >
      > 2. **Uniformity（均匀性）**$\log \mathbb{E} \  e ^{-2||f(x)-f(y||^{2})}$
      >
      > **Uniformity** 强调的是嵌入空间中的表示分布的均匀性。在许多任务中，尤其是在图嵌入或对比学习中，期望表示空间中的点分布尽量均匀：
      >
      > - **表示的均匀分布**：在对比学习中，我们通常希望所有样本（无论是正样本还是负样本）都能够在表示空间中均匀地分布，而不是某些区域过于密集或空白。这有助于提高模型的泛化能力和鲁棒性。
      >
      > - **避免聚集**：如果表示过于聚集在某个区域，可能会导致模型无法充分利用所有的表示空间，从而影响到其性能。比如在某些推荐系统中，我们希望用户和物品的表示尽量均匀分布，这样能避免模型陷入局部最优。
      >
      > - **当损失较小时**，意味着正负样本对之间的距离得到良好的区分，相似样本的距离较小，不相似样本的距离较大。**表示空间的嵌入通常更加均匀**，模型学习到的表示能够较好地反映样本之间的相似度和差异性。
      >
      >   **当损失较大时**，意味着正负样本的距离区分不明显，嵌入空间可能过于集中，导致表示的不均匀，导致较差的uniformity表现。
      >
      > 总结
      >
      > - **Alignment** 关注的是表示是否与目标对齐，强调相似或相关样本的嵌入应该相近。
      > - **Uniformity** 关注的是表示的整体分布，强调表示空间的均匀性，避免某些区域过于密集或空白。
      >
      > 在许多任务中，尤其是在对比学习和图嵌入中，通常会追求在 **alignment** 和 **uniformity** 之间找到一个平衡点：既保证相似样本的对齐，又要保持嵌入空间的均匀分布。

  * 理解什么是词向量中的各项异性问题，以及为什么**对比学习能够解决这个问题**

* ![对比学习1](C:\Users\5c\Desktop\Study Notes\pic\对比学习1.jpg)

  ![对比学习2](C:\Users\5c\Desktop\Study Notes\pic\对比学习2.jpg)

  ![对比学习3](C:\Users\5c\Desktop\Study Notes\pic\对比学习3.jpg)

  ![对比学习4](C:\Users\5c\Desktop\Study Notes\pic\对比学习4.jpg)

  ![对比学习5](C:\Users\5c\Desktop\Study Notes\pic\对比学习5.jpg)

* ==对论文中解释的理解（全是干货与精华）==

> 1. **$h_i$ 的归一化和 $WW^T$**:
>    - 文中提到对 $h_i$ 进行归一化，意味着每个向量 $h_i$（可能是某种特征或嵌入向量）已经进行了缩放，以使得某些特性成立。
>    - $WW^T$ 的对角线元素都是 1，因此 $WW^T$ 的迹（对角线元素之和，也就是矩阵的特征值之和）是一个常数。
>
> 2. **Merikoski (1984) 的结果**:
>    - 引用了 Merikoski（1984）的结果，表示如果 $WW^T$ 中的所有元素都是正数，那么 $WW^T$ 的元素总和是 $WW^T$ 最大特征值的上界。
>
> 3. **优化和对比学习**:
>    - 文中提到优化公式中的第二项（公式 6，但没有显示在图片中）。该目标是最小化 $WW^T$ 的**最大特征值**，从而"平滑"嵌入空间的奇异谱。简单来说，就是让嵌入向量在空间中更加均匀分布，不会过度集中在某些区域。
>    - **最大特征值**指的是矩阵 $WW^T$ 中最大的奇异值。如果最大特征值非常大，说明矩阵在某个方向上对嵌入空间的影响非常强，而其他方向可能没有得到充分的利用，这可能导致表示空间的"退化"。
>      通过减少最大特征值，我们试图让所有方向的奇异值更加均匀，使得**嵌入空间的表示更加平滑**。这种做法可以有效避免模型学习到过度集中的表示，进而改善**表示的多样性和均匀性**。
>
> 4. **表示退化问题**:
>    - **表示退化**指的是嵌入向量可能会崩塌成一个不太有用或者过于狭窄的空间（例如，很多嵌入都非常相似，导致模型的泛化能力差）。通过减少 $WW^T$ 的最大特征值，对比学习有助于避免这个问题。
>
> 5. **句子嵌入的均匀性**:
>    - 对比学习的一个目标是**提高句子嵌入的均匀性**。也就是说，我们希望确保句子在嵌入空间中能够均匀分布，而不是过于集中在某个区域，这样能提高嵌入的质量和模型的泛化能力。
>
> **数学表达式解析**
>
> 最后的数学公式是：
> $E \left[ \log \left( E_{x \sim \text{data}} \left[ e^{f(x)^T f(x') / \tau} \right] \right) \right] \geq \frac{1}{\tau m^2} \sum_{i=1}^m \sum_{j=1}^m h_i^T h_j = \frac{1}{\tau m^2} \text{Sum}(WW^T)$
>
> 这个公式表示了数据上的期望对数似然和矩阵 $WW^T$ 之间的关系。这里涉及到 $f(x)^T f(x')$ 这一项，表示不同样本（例如正样本对或句子嵌入）之间的相似度，$\tau$ 是一个温度参数，用来控制模型对相似度的敏感度。
>
> - 公式右侧的 $\frac{1}{\tau m^2} \sum_{i=1}^m \sum_{j=1}^m h_i^T h_j$ 表示的是嵌入向量之间的内积和，==反映了向量之间的**相似度**==。==这部分涉及了矩阵 $WW^T$ 的和，也与**嵌入的特征值结构相关**。**通过减少这个和，可以"平滑"嵌入空间，避免过度集中**。==

* 损失的代码实现


## 统计

### **Spearman Rank Correlation Coefficient**（斯皮尔曼等级相关系数）

初见：[MultiImport: Inferring Node Importance in a Knowledge Graph from Multiple Input Signals](https://arxiv.org/pdf/2006.12001)

是一种衡量两个变量之间关系的统计量。它评估的是两个变量的**单调关系**，即变量之间是否存在一种方向上的一致性，而不要求是线性关系。这个系数是通过比较两个变量的排名来计算的，而不是直接使用它们的原始值。

* 主要特点

  * 优点：

    **非参数统计**：与皮尔逊相关系数不同，Spearman 相关系数不要求数据服从正态分布，适用于任何形式的单调关系（无论是线性还是非线性）。

    **衡量单调关系，可以识别非线性相关**：Spearman 关注的是两个变量值的相对变化趋势，而不是它们的绝对值。即使它们的关系不是线性的，只要是单调递增或递减的关系，Spearman 仍然能反映这种趋势。比如说 $y = x^2(x \ge0)$ 皮尔逊相关系数可能会给出来一个 0.7，但使用 Spearman 可能会给出来一个 1.0

    **对==排名==进行处理，对数据不敏感**：在计算过程中，Spearman 先将数据转换成排名，然后再计算排名之间的差异。故对异常值不敏感。

  * 局限：

    **大量同值会不准**：对同值会取平均

    **非线性识别单一**：若碰到先减后增（先增后减）的情况，可能会为 0 

  

  **简而言之就是只看相对数值，衡量单调关系**

* 计算方法

  * 假设有两个变量 $X$ 和 $Y$，每个变量有 $n$ 个数据点。首先，对每个变量的值进行排名。如果有重复值（tie），排名是**通过给重复值赋予相同的平均排名**来处理的。（即可能会出现1.5这一说法）

    Spearman 相关系数 $\rho$ 的计算公式如下（**这个公式只适用于同值的情况下**！！！）：
    $$
    \rho = 1- \frac{6\sum d^2_i}{n(n^2-1)}
    $$
    其中：

    - $d_i$ 是第 $i$ 个数据点的排名差（即 $X_i$ 和 $Y_i$ 排名的差）。
    - $n$ 是数据点的总数。
    
  * **基于秩的公式**：
    $$
    \rho = \frac{cov(R(X),R(Y))}{\sigma_{R(X)}\sigma_{R(Y)}}
    $$
    可以发现，和常见的计算相关系数的区别就在于这个公式是基于**秩**来计算的

* 解释

  * **$\rho = 1$**：表示完全正相关，即两个变量有完全一致的单调递增关系。

    **$\rho = -1$**：表示完全负相关，即两个变量有完全一致的单调递减关系。

    **$\rho = 0$**：表示没有任何单调关系，变量之间不相关。

    介于 $-1$ 和 $1$ 之间的值表示两者之间存在某种程度的单调关系，正值表示正相关，负值表示负相关。

* 优点

  * 不要求数据满足正态分布，适用于各种类型的数据。

    对异常值（outliers）较为鲁棒，因为它是基于排名而非原始数据的值进行计算。

* 练习：手搓一个简易的计算 `Speaman` 的方法
  自己尝试写了写，应该是对的（经验证，值大致是对的，只是精度稍微差一点..小数点后 6 位）

* ```python
  def get_Spearmanr(X, Y):
      '''
      X: Tensor   (N,)
      Y: Tensor   (N,)
      '''
      def rank_data(arr):
          '''
          arr: Numpy
  
          returns: arr
          '''
          val, cnt = np.unique(arr, return_counts = True)    
          cur_rank = 1
          ranks = {}
          for v, c in zip(val, cnt):
              r = cur_rank + c - 1
              ranks[v] = (cur_rank + r) / 2
              cur_rank = r + 1
          return np.array([ranks[key] for key in arr])
      
      X_rank = torch.tensor(rank_data(X.numpy()))
      Y_rank = torch.tensor(rank_data(Y.numpy()))
  
      # 计算协方差
      mean_X = torch.mean(X_rank)
      mean_Y = torch.mean(Y_rank)
  
      cov_XY = torch.mean((X_rank - mean_X) * (Y_rank - mean_Y))
      std_X = torch.std(X_rank, unbiased=False)
      std_Y = torch.std(Y_rank, unbiased=False)
  
      rho = cov_XY / (std_X * std_Y)  # 计算 Spearman 相关系数
      return rho
  ```
  



### 二项分布概率质量函数

* 初见：[Label Informed Contrastive Pretraining for Node  Importance Estimation on Knowledge Graphs](https://arxiv.org/pdf/2402.17791)
* 

## 实验相关与经验

### 数值稳定性问题（repeat）

* [课程](https://www.bilibili.com/video/BV1u64y1i75a/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)（收获满满！）

* 要求自己手动复盘！（注意，文中这里的 $\mathbf{h}^{t}$ 看作的是**向量**而并非矩阵）

  * 熟练掌握反向传播的表达式子（会使用矩阵求导）

  * 理解梯度爆炸与梯度消失的原因以及带来的影响

  * 重点理解合理的权重初始化和激活函数的选取为什么能让数值稳定

    * 让每层的方差是一个常数可以让训练更加稳定（将每一层的输入和输出都看成一个**随机变量**）

      * > 如果每一层的输出方差太小或太大，梯度就会迅速变得非常小（梯度消失）或者非常大（梯度爆炸）。
        >
        > 例如，假设我们使用的是 Sigmoid 或 Tanh 激活函数，它们的梯度在输入非常大或非常小的时候会趋近于 0，这就会导致梯度消失问题，使得训练变慢，甚至停滞不前。
        >
        > 如果每一层的输出**方差保持恒定**（例如通过适当的初始化方法或者批归一化），那么每层的激活值和梯度**都在合理的范围内**，有助于避免梯度消失或爆炸问题，从而保证训练的稳定性。

      * 那么索性就不妨假设每层**输出**的期望为 $0$ ，这样做下去看看

        * 首先考虑**权重初始化**
        * 假设权重 $\mathbf{W}$ 各元素之间独立同分布，且权重 $\mathbf{W}$ 与 输入 $\mathbf{h}$ 独立，在假设每层 $\mathbf{h}$ 的方差都一样的情况下进行推演
        * 反向传播的均值和方差应该怎么计算

  * 思考，为什么**激活函数**最好应当在源点附近时为一个 `Identity function`，这样训练效果就好。`ReLU` 和 `tanh` **训练效果好可能与此有关**！ 
  
  * 最终我们可以得出结论：合理的权重初始值和激活函数的选取可以提升数值的稳定性
  
* 一些细节需要注意，进行反向传播分析的时候，要关注求导的那个目标可以“辐射”出哪些路径，这些路径上的点都是链式法则的一部分；此外，还要关注形状，形状很重要，不能想当然！
  $$
  \sum_{j}\mathbf{E} (\frac{\partial \mathcal{L}}{\partial {h^t_j}} \cdot \mathbf{W}^{t}_{j,i}) ^ {2}
  $$
  这里有一个小问题

  这该怎么计算呢？只有假设括号中的两项独立才能得到视频中的计算结果，但是感觉这两项不是独立的。GPT也认为由于梯度和权重是相关的，**假设独立并不成立**。

![数值稳定性1](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性1.JPG)

![数值稳定性2](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性2.JPG)

![数值稳定性3](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性3.JPG)

![数值稳定性4](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性4.JPG)

![数值稳定性5](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性5.JPG)

<img src="C:\Users\5c\Desktop\Study Notes\pic\数值稳定性6.png" alt="数值稳定性6" style="zoom:50%;" />

