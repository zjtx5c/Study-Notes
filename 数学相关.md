# 数学相关

## 基础

### 求导与梯度

视频推荐看[李沐老师](https://www.bilibili.com/video/BV1eZ4y1w7PY/?spm_id_from=333.1387.collection.video_card.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)，评论区下方有知乎链接

* 标量导数（高中常识）
* 亚导数
  * 将导数拓展到不可微的函数（分类讨论）

#### 向量与标量之间的求导

* 将导数拓展到向量（核心是要**搞对形状**）

* 有一个约定俗称（分子布局）

  > 在许多机器学习和优化领域，**默认约定是**：
  >
  > - **标量对向量求导，结果是行向量**（即 $1 \times n$）（这就是梯度方向）。
  >   - **标量对向量求导**通常被定义为行向量，因为它可以直接与后续的矩阵乘法操作兼容，避免了额外的转置操作，使得在计算时更简洁。
  > - **向量对标量求导，结果是列向量**（即 $n \times 1$）。
  >
  > 这样设计的原因有
  >
  > （1）**确保梯度与雅可比矩阵的统一**
  >
  > 在多变量微积分中，向量或矩阵的导数通常遵循 **雅可比矩阵（Jacobian Matrix）** 的定义：
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  > \begin{bmatrix} 
  > \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  > \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  > \vdots & \vdots & \ddots & \vdots \\ 
  > \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  > \end{bmatrix} 
  > \in \mathbb{R}^{m \times n}
  > $$
  > 这个定义确保：
  >
  > - 如果 $\mathbf{y}$ 是一个**标量**（即 $m=1$），那么结果是行向量。
  > - 如果 $\mathbf{x}$ 是一个**列向量**，那么结果符合矩阵乘法规则。
  >
  > （2）**使得链式法则自然成立**
  >
  > 在链式法则中，如果 
  > $$\mathbf{z} = f(\mathbf{y})$$
  > 且 
  > $$\mathbf{y} = g(\mathbf{x})$$
  > 则：
  >
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
  > $$
  >
  > 其中：
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \text{ 是 } k \times m \text{（雅可比矩阵）}
  > $$
  >
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \text{ 是 } m \times 1 \text{（列向量）}
  > $$
  >
  > 矩阵乘法要求中间维度对齐，因此 
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} \text{ 必须是 } k \times 1 \text{（列向量）}
  > $$
  > （3）在**神经网络反向传播**中，梯度计算更自然（无需转置）。
  >
  > 反向传播的本质是沿着计算图按链式法则逐层传递梯度。当采用：
  >
  > 标量损失对参数矩阵的梯度 $\frac{\partial L}{\partial W}$ 与参数同形（即若 $W$ 是 $m \times n$，则梯度也是 $m \times n$）
  >
  > 中间变量的梯度 $\frac{\partial z}{\partial x}$ 按雅可比矩阵形式传播
  >
  > 这种约定能保证：
  >
  > - **维度自动对齐**：在链式法则的矩阵乘法中，中间维度天然匹配，无需手动插入转置操作。
  > - **编程一致性**：框架（如PyTorch）的 `autograd` 会统一处理张量的梯度形状，与数学约定解耦。

#### 向量与向量之间的求导

* 雅可比矩阵：可以这样理解，将 $\mathbf{y}$ 中的每个元素（标量）分别对 $\mathbf{x}$ 求导，每个元素得到一个行向量，最终得到 $m$ 个行向量！

  $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  \begin{bmatrix} 
  \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  \vdots & \vdots & \ddots & \vdots \\ 
  \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  \end{bmatrix} 
  \in \mathbb{R}^{m \times n}$

#### 有关矩阵的求导

* 有空再补



## 统计

### **Spearman Rank Correlation Coefficient**（斯皮尔曼等级相关系数）

初见：[MultiImport: Inferring Node Importance in a Knowledge Graph from Multiple Input Signals](https://arxiv.org/pdf/2006.12001)

是一种衡量两个变量之间关系的统计量。它评估的是两个变量的**单调关系**，即变量之间是否存在一种方向上的一致性，而不要求是线性关系。这个系数是通过比较两个变量的排名来计算的，而不是直接使用它们的原始值。

* 主要特点

  * 优点：

    **非参数统计**：与皮尔逊相关系数不同，Spearman 相关系数不要求数据服从正态分布，适用于任何形式的单调关系（无论是线性还是非线性）。

    **衡量单调关系，可以识别非线性相关**：Spearman 关注的是两个变量值的相对变化趋势，而不是它们的绝对值。即使它们的关系不是线性的，只要是单调递增或递减的关系，Spearman 仍然能反映这种趋势。比如说 $y = x^2(x \ge0)$ 皮尔逊相关系数可能会给出来一个 0.7，但使用 Spearman 可能会给出来一个 1.0

    **对==排名==进行处理，对数据不敏感**：在计算过程中，Spearman 先将数据转换成排名，然后再计算排名之间的差异。故对异常值不敏感。

  * 局限：

    **大量同值会不准**：对同值会取平均

    **非线性识别单一**：若碰到先减后增（先增后减）的情况，可能会为 0 

  

  **简而言之就是只看相对数值，衡量单调关系**

* 计算方法

  * 假设有两个变量 $X$ 和 $Y$，每个变量有 $n$ 个数据点。首先，对每个变量的值进行排名。如果有重复值（tie），排名是**通过给重复值赋予相同的平均排名**来处理的。（即可能会出现1.5这一说法）

    Spearman 相关系数 $\rho$ 的计算公式如下（**这个公式只适用于同值的情况下**！！！）：
    $$
    \rho = 1- \frac{6\sum d^2_i}{n(n^2-1)}
    $$
    其中：

    - $d_i$ 是第 $i$ 个数据点的排名差（即 $X_i$ 和 $Y_i$ 排名的差）。
    - $n$ 是数据点的总数。
    
  * **基于秩的公式**：
    $$
    \rho = \frac{cov(R(X),R(Y))}{\sigma_{R(X)}\sigma_{R(Y)}}
    $$
    可以发现，和常见的计算相关系数的区别就在于这个公式是基于**秩**来计算的

* 解释

  * **$\rho = 1$**：表示完全正相关，即两个变量有完全一致的单调递增关系。

    **$\rho = -1$**：表示完全负相关，即两个变量有完全一致的单调递减关系。

    **$\rho = 0$**：表示没有任何单调关系，变量之间不相关。

    介于 $-1$ 和 $1$ 之间的值表示两者之间存在某种程度的单调关系，正值表示正相关，负值表示负相关。

* 优点

  * 不要求数据满足正态分布，适用于各种类型的数据。

    对异常值（outliers）较为鲁棒，因为它是基于排名而非原始数据的值进行计算。

* 练习：手搓一个简易的计算 `Speaman` 的方法
  自己尝试写了写，应该是对的（经验证，值大致是对的，只是精度稍微差一点..小数点后 6 位）

* ```python
  def get_Spearmanr(X, Y):
      '''
      X: Tensor   (N,)
      Y: Tensor   (N,)
      '''
      def rank_data(arr):
          '''
          arr: Numpy
  
          returns: arr
          '''
          val, cnt = np.unique(arr, return_counts = True)    
          cur_rank = 1
          ranks = {}
          for v, c in zip(val, cnt):
              r = cur_rank + c - 1
              ranks[v] = (cur_rank + r) / 2
              cur_rank = r + 1
          return np.array([ranks[key] for key in arr])
      
      X_rank = torch.tensor(rank_data(X.numpy()))
      Y_rank = torch.tensor(rank_data(Y.numpy()))
  
      # 计算协方差
      mean_X = torch.mean(X_rank)
      mean_Y = torch.mean(Y_rank)
  
      cov_XY = torch.mean((X_rank - mean_X) * (Y_rank - mean_Y))
      std_X = torch.std(X_rank, unbiased=False)
      std_Y = torch.std(Y_rank, unbiased=False)
  
      rho = cov_XY / (std_X * std_Y)  # 计算 Spearman 相关系数
      return rho
  ```
  



## 实验相关与经验

### 数值稳定性问题（repeat）

* [课程](https://www.bilibili.com/video/BV1u64y1i75a/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)（收获满满！）

* 要求自己手动复盘！（注意，文中这里的 $\mathbf{h}^{t}$ 看作的是**向量**而并非矩阵）

  * 熟练掌握反向传播的表达式子（会使用矩阵求导）

  * 理解梯度爆炸与梯度消失的原因以及带来的影响

  * 重点理解合理的权重初始化和激活函数的选取为什么能让数值稳定

    * 让每层的方差是一个常数可以让训练更加稳定（将每一层的输入和输出都看成一个**随机变量**）

      * > 如果每一层的输出方差太小或太大，梯度就会迅速变得非常小（梯度消失）或者非常大（梯度爆炸）。
        >
        > 例如，假设我们使用的是 Sigmoid 或 Tanh 激活函数，它们的梯度在输入非常大或非常小的时候会趋近于 0，这就会导致梯度消失问题，使得训练变慢，甚至停滞不前。
        >
        > 如果每一层的输出**方差保持恒定**（例如通过适当的初始化方法或者批归一化），那么每层的激活值和梯度**都在合理的范围内**，有助于避免梯度消失或爆炸问题，从而保证训练的稳定性。

      * 那么索性就不妨假设每层**输出**的期望为 $0$ ，这样做下去看看

        * 首先考虑**权重初始化**
        * 假设权重 $\mathbf{W}$ 各元素之间独立同分布，且权重 $\mathbf{W}$ 与 输入 $\mathbf{h}$ 独立，在假设每层 $\mathbf{h}$ 的方差都一样的情况下进行推演
        * 反向传播的均值和方差应该怎么计算

  * 思考，为什么**激活函数**最好应当在源点附近时为一个 `Identity function`，这样训练效果就好。`ReLU` 和 `tanh` **训练效果好可能与此有关**！ 
  
  * 最终我们可以得出结论：合理的权重初始值和激活函数的选取可以提升数值的稳定性
  
* 一些细节需要注意，进行反向传播分析的时候，要关注求导的那个目标可以“辐射”出哪些路径，这些路径上的点都是链式法则的一部分；此外，还要关注形状，形状很重要，不能想当然！
  $$
  \sum_{j}\mathbf{E} (\frac{\partial \mathcal{L}}{\partial {h^t_j}} \cdot \mathbf{W}^{t}_{j,i}) ^ {2}
  $$
  这里有一个小问题

  这该怎么计算呢？只有假设括号中的两项独立才能得到视频中的计算结果，但是感觉这两项不是独立的。GPT也认为由于梯度和权重是相关的，**假设独立并不成立**。

![数值稳定性1](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性1.JPG)

![数值稳定性2](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性2.JPG)

![数值稳定性3](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性3.JPG)

![数值稳定性4](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性4.JPG)

![数值稳定性5](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性5.JPG)

<img src="C:\Users\5c\Desktop\Study Notes\pic\数值稳定性6.png" alt="数值稳定性6" style="zoom:50%;" />

