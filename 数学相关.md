# 数学相关

## 基础

### 求导与梯度

视频推荐看[李沐老师](https://www.bilibili.com/video/BV1eZ4y1w7PY/?spm_id_from=333.1387.collection.video_card.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)，评论区下方有知乎链接

* 标量导数（高中常识）
* 亚导数
  * 将导数拓展到不可微的函数（分类讨论）

#### 向量与标量之间的求导

* 将导数拓展到向量（核心是要**搞对形状**）

* 有一个约定俗称（分子布局）

  > 在许多机器学习和优化领域，**默认约定是**：
  >
  > - **标量对向量求导，结果是行向量**（即 $1 \times n$）（这就是梯度方向）。
  >   - **标量对向量求导**通常被定义为行向量，因为它可以直接与后续的矩阵乘法操作兼容，避免了额外的转置操作，使得在计算时更简洁。
  > - **向量对标量求导，结果是列向量**（即 $n \times 1$）。
  >
  > 这样设计的原因有
  >
  > （1）**确保梯度与雅可比矩阵的统一**
  >
  > 在多变量微积分中，向量或矩阵的导数通常遵循 **雅可比矩阵（Jacobian Matrix）** 的定义：
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  > \begin{bmatrix} 
  > \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  > \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  > \vdots & \vdots & \ddots & \vdots \\ 
  > \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  > \end{bmatrix} 
  > \in \mathbb{R}^{m \times n}
  > $$
  > 这个定义确保：
  >
  > - 如果 $\mathbf{y}$ 是一个**标量**（即 $m=1$），那么结果是行向量。
  > - 如果 $\mathbf{x}$ 是一个**列向量**，那么结果符合矩阵乘法规则。
  >
  > （2）**使得链式法则自然成立**
  >
  > 在链式法则中，如果 
  > $$\mathbf{z} = f(\mathbf{y})$$
  > 且 
  > $$\mathbf{y} = g(\mathbf{x})$$
  > 则：
  >
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
  > $$
  >
  > 其中：
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \text{ 是 } k \times m \text{（雅可比矩阵）}
  > $$
  >
  > $$
  > \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \text{ 是 } m \times 1 \text{（列向量）}
  > $$
  >
  > 矩阵乘法要求中间维度对齐，因此 
  > $$
  > \frac{\partial \mathbf{z}}{\partial \mathbf{x}} \text{ 必须是 } k \times 1 \text{（列向量）}
  > $$
  > （3）在**神经网络反向传播**中，梯度计算更自然（无需转置）。
  >
  > 反向传播的本质是沿着计算图按链式法则逐层传递梯度。当采用：
  >
  > 标量损失对参数矩阵的梯度 $\frac{\partial L}{\partial W}$ 与参数同形（即若 $W$ 是 $m \times n$，则梯度也是 $m \times n$）
  >
  > 中间变量的梯度 $\frac{\partial z}{\partial x}$ 按雅可比矩阵形式传播
  >
  > 这种约定能保证：
  >
  > - **维度自动对齐**：在链式法则的矩阵乘法中，中间维度天然匹配，无需手动插入转置操作。
  > - **编程一致性**：框架（如PyTorch）的 `autograd` 会统一处理张量的梯度形状，与数学约定解耦。

#### 向量与向量之间的求导

* 雅可比矩阵：可以这样理解，将 $\mathbf{y}$ 中的每个元素（标量）分别对 $\mathbf{x}$ 求导，每个元素得到一个行向量，最终得到 $m$ 个行向量！

  $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  
  \begin{bmatrix} 
  \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ 
  \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
  \vdots & \vdots & \ddots & \vdots \\ 
  \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} 
  \end{bmatrix} 
  \in \mathbb{R}^{m \times n}$

#### 有关矩阵的求导

* 有空再补



## 信息论相关（常涉及一些损失函数）

#### InfoNCE loss

初见：[Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs](https://arxiv.org/pdf/2402.17791)

它基于信息论的思想，通过比较**正样本和负样本的相似性**来学习模型参数，从而提高特征的区分度。

其一般形式为：
$$
\text{InfoNCE Loss} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{A^+}{A^+ + A^-}
$$
首先，分式部分一定是介于(0,1)之间的，而log在（0，1）之间是单增的且函数值小于0

在损失优化过程中，我们希望达成的结果是 $A^+$ 尽可能大，也就是正样本之间的距离尽可能尽，其实也隐含着与负样本之间的相似度尽可能低，距离尽可能远。从公式上来看，**我们在最小化loss的过程中，需要让公式接近0，也就是让log内部的分式接近1，要达到这个效果，应该使 $A^+ >> A^-$，可以发现跟我们的训练思路是吻合的，这就达到了对于查询向量而言，推近它和正样本之间的距离，拉远它和负样本的距离**。

#### 对比学习

* [视频链接](https://www.bilibili.com/video/BV1uf4y1A7KC/?spm_id_from=333.337.top_right_bar_window_history.content.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

* 小结一下

  * 使用**随机失活来构造相似的正例样本**这个 `trick` 比较妙

  * 理解损失函数以及如何度量嵌入向量的好坏

    * 理解Alignment（对齐）与Uniformity（均匀性）

      > 在机器学习和深度学习中，**alignment** 和 **uniformity** 这两个术语常常被用来描述模型或表示学习的质量，尤其是在图嵌入、对比学习、以及其他优化任务中。它们通常涉及到如何对齐或保持表示的分布特性。我们可以分别来看这两个概念。
      >
      > 1. **Alignment（对齐）**
      >
      > **Alignment** 通常指的是模型学习到的表示在某种目标或参考标准上的一致性或对齐。在对比学习、迁移学习等任务中，alignment 可以指：
      >
      > - **正样本的对齐**：当学习表示时，模型应该使得相似的样本在嵌入空间中距离较近，即使相似的实例对齐。例如，在图像对比学习中，正样本对（相似图片）应该被映射到表示空间中的同一位置或相邻位置。
      > - **目标一致性**：在一些任务中，我们希望模型学习到的表示能够对齐到某个先验目标（如类别标签）。例如，在分类任务中，模型的输出表示应该能够对齐到目标标签空间，使得同一类别的样本在表示空间中尽量接近。
      >
      > 2. **Uniformity（均匀性）**$\log \mathbb{E} \  e ^{-2||f(x)-f(y||^{2})}$
      >
      > **Uniformity** 强调的是嵌入空间中的表示分布的均匀性。在许多任务中，尤其是在图嵌入或对比学习中，期望表示空间中的点分布尽量均匀：
      >
      > - **表示的均匀分布**：在对比学习中，我们通常希望所有样本（无论是正样本还是负样本）都能够在表示空间中均匀地分布，而不是某些区域过于密集或空白。这有助于提高模型的泛化能力和鲁棒性。
      >
      > - **避免聚集**：如果表示过于聚集在某个区域，可能会导致模型无法充分利用所有的表示空间，从而影响到其性能。比如在某些推荐系统中，我们希望用户和物品的表示尽量均匀分布，这样能避免模型陷入局部最优。
      >
      > - **当损失较小时**，意味着正负样本对之间的距离得到良好的区分，相似样本的距离较小，不相似样本的距离较大。**表示空间的嵌入通常更加均匀**，模型学习到的表示能够较好地反映样本之间的相似度和差异性。
      >
      >   **当损失较大时**，意味着正负样本的距离区分不明显，嵌入空间可能过于集中，导致表示的不均匀，导致较差的uniformity表现。
      >
      > 总结
      >
      > - **Alignment** 关注的是表示是否与目标对齐，强调相似或相关样本的嵌入应该相近。
      > - **Uniformity** 关注的是表示的整体分布，强调表示空间的均匀性，避免某些区域过于密集或空白。
      >
      > 在许多任务中，尤其是在对比学习和图嵌入中，通常会追求在 **alignment** 和 **uniformity** 之间找到一个平衡点：既保证相似样本的对齐，又要保持嵌入空间的均匀分布。

  * 理解什么是词向量中的各项异性问题，以及为什么**对比学习能够解决这个问题**

* ![对比学习1](C:\Users\5c\Desktop\Study Notes\pic\对比学习1.jpg)

  ![对比学习2](C:\Users\5c\Desktop\Study Notes\pic\对比学习2.jpg)

  ![对比学习3](C:\Users\5c\Desktop\Study Notes\pic\对比学习3.jpg)

  ![对比学习4](C:\Users\5c\Desktop\Study Notes\pic\对比学习4.jpg)

  ![对比学习5](C:\Users\5c\Desktop\Study Notes\pic\对比学习5.jpg)

* ==对论文中解释的理解（全是干货与精华）==

> 1. **$h_i$ 的归一化和 $WW^T$**:
>    - 文中提到对 $h_i$ 进行归一化，意味着每个向量 $h_i$（可能是某种特征或嵌入向量）已经进行了缩放，以使得某些特性成立。
>    - $WW^T$ 的对角线元素都是 1，因此 $WW^T$ 的迹（对角线元素之和，也就是矩阵的特征值之和）是一个常数。
>
> 2. **Merikoski (1984) 的结果**:
>    - 引用了 Merikoski（1984）的结果，表示如果 $WW^T$ 中的所有元素都是正数，那么 $WW^T$ 的元素总和是 $WW^T$ 最大特征值的上界。
>
> 3. **优化和对比学习**:
>    - 文中提到优化公式中的第二项（公式 6，但没有显示在图片中）。该目标是最小化 $WW^T$ 的**最大特征值**，从而"平滑"嵌入空间的奇异谱。简单来说，就是让嵌入向量在空间中更加均匀分布，不会过度集中在某些区域。
>    - **最大特征值**指的是矩阵 $WW^T$ 中最大的奇异值。如果最大特征值非常大，说明矩阵在某个方向上对嵌入空间的影响非常强，而其他方向可能没有得到充分的利用，这可能导致表示空间的"退化"。
>      通过减少最大特征值，我们试图让所有方向的奇异值更加均匀，使得**嵌入空间的表示更加平滑**。这种做法可以有效避免模型学习到过度集中的表示，进而改善**表示的多样性和均匀性**。
>
> 4. **表示退化问题**:
>    - **表示退化**指的是嵌入向量可能会崩塌成一个不太有用或者过于狭窄的空间（例如，很多嵌入都非常相似，导致模型的泛化能力差）。通过减少 $WW^T$ 的最大特征值，对比学习有助于避免这个问题。
>
> 5. **句子嵌入的均匀性**:
>    - 对比学习的一个目标是**提高句子嵌入的均匀性**。也就是说，我们希望确保句子在嵌入空间中能够均匀分布，而不是过于集中在某个区域，这样能提高嵌入的质量和模型的泛化能力。
>
> **数学表达式解析**
>
> 最后的数学公式是：
> $E \left[ \log \left( E_{x \sim \text{data}} \left[ e^{f(x)^T f(x') / \tau} \right] \right) \right] \geq \frac{1}{\tau m^2} \sum_{i=1}^m \sum_{j=1}^m h_i^T h_j = \frac{1}{\tau m^2} \text{Sum}(WW^T)$
>
> 这个公式表示了数据上的期望对数似然和矩阵 $WW^T$ 之间的关系。这里涉及到 $f(x)^T f(x')$ 这一项，表示不同样本（例如正样本对或句子嵌入）之间的相似度，$\tau$ 是一个温度参数，用来控制模型对相似度的敏感度。
>
> - 公式右侧的 $\frac{1}{\tau m^2} \sum_{i=1}^m \sum_{j=1}^m h_i^T h_j$ 表示的是嵌入向量之间的内积和，==反映了向量之间的**相似度**==。==这部分涉及了矩阵 $WW^T$ 的和，也与**嵌入的特征值结构相关**。**通过减少这个和，可以"平滑"嵌入空间，避免过度集中**。==

* 损失的代码实现


## 统计

### **Spearman Rank Correlation Coefficient**（斯皮尔曼等级相关系数）

初见：[MultiImport: Inferring Node Importance in a Knowledge Graph from Multiple Input Signals](https://arxiv.org/pdf/2006.12001)

是一种衡量两个变量之间关系的统计量。它评估的是两个变量的**单调关系**，即变量之间是否存在一种方向上的一致性，而不要求是线性关系。这个系数是通过比较两个变量的排名来计算的，而不是直接使用它们的原始值。

* 主要特点

  * 优点：

    **非参数统计**：与皮尔逊相关系数不同，Spearman 相关系数不要求数据服从正态分布，适用于任何形式的单调关系（无论是线性还是非线性）。

    **衡量单调关系，可以识别非线性相关**：Spearman 关注的是两个变量值的相对变化趋势，而不是它们的绝对值。即使它们的关系不是线性的，只要是单调递增或递减的关系，Spearman 仍然能反映这种趋势。比如说 $y = x^2(x \ge0)$ 皮尔逊相关系数可能会给出来一个 0.7，但使用 Spearman 可能会给出来一个 1.0

    **对==排名==进行处理，对数据不敏感**：在计算过程中，Spearman 先将数据转换成排名，然后再计算排名之间的差异。故对异常值不敏感。

  * 局限：

    **大量同值会不准**：对同值会取平均

    **非线性识别单一**：若碰到先减后增（先增后减）的情况，可能会为 0 

  

  **简而言之就是只看相对数值，衡量单调关系**

* 计算方法

  * 假设有两个变量 $X$ 和 $Y$，每个变量有 $n$ 个数据点。首先，对每个变量的值进行排名。如果有重复值（tie），排名是**通过给重复值赋予相同的平均排名**来处理的。（即可能会出现1.5这一说法）

    Spearman 相关系数 $\rho$ 的计算公式如下（**这个公式只适用于同值的情况下**！！！）：
    $$
    \rho = 1- \frac{6\sum d^2_i}{n(n^2-1)}
    $$
    其中：

    - $d_i$ 是第 $i$ 个数据点的排名差（即 $X_i$ 和 $Y_i$ 排名的差）。
    - $n$ 是数据点的总数。
    
  * **基于秩的公式**：
    $$
    \rho = \frac{cov(R(X),R(Y))}{\sigma_{R(X)}\sigma_{R(Y)}}
    $$
    可以发现，和常见的计算相关系数的区别就在于这个公式是基于**秩**来计算的

* 解释

  * **$\rho = 1$**：表示完全正相关，即两个变量有完全一致的单调递增关系。

    **$\rho = -1$**：表示完全负相关，即两个变量有完全一致的单调递减关系。

    **$\rho = 0$**：表示没有任何单调关系，变量之间不相关。

    介于 $-1$ 和 $1$ 之间的值表示两者之间存在某种程度的单调关系，正值表示正相关，负值表示负相关。

* 优点

  * 不要求数据满足正态分布，适用于各种类型的数据。

    对异常值（outliers）较为鲁棒，因为它是基于排名而非原始数据的值进行计算。

* 练习：手搓一个简易的计算 `Speaman` 的方法
  自己尝试写了写，应该是对的（经验证，值大致是对的，只是精度稍微差一点..小数点后 6 位）

* ```python
  def get_Spearmanr(X, Y):
      '''
      X: Tensor   (N,)
      Y: Tensor   (N,)
      '''
      def rank_data(arr):
          '''
          arr: Numpy
  
          returns: arr
          '''
          val, cnt = np.unique(arr, return_counts = True)    
          cur_rank = 1
          ranks = {}
          for v, c in zip(val, cnt):
              r = cur_rank + c - 1
              ranks[v] = (cur_rank + r) / 2
              cur_rank = r + 1
          return np.array([ranks[key] for key in arr])
      
      X_rank = torch.tensor(rank_data(X.numpy()))
      Y_rank = torch.tensor(rank_data(Y.numpy()))
  
      # 计算协方差
      mean_X = torch.mean(X_rank)
      mean_Y = torch.mean(Y_rank)
  
      cov_XY = torch.mean((X_rank - mean_X) * (Y_rank - mean_Y))
      std_X = torch.std(X_rank, unbiased=False)
      std_Y = torch.std(Y_rank, unbiased=False)
  
      rho = cov_XY / (std_X * std_Y)  # 计算 Spearman 相关系数
      return rho
  ```
  



### 二项分布概率质量函数

* 初见：[Label Informed Contrastive Pretraining for Node  Importance Estimation on Knowledge Graphs](https://arxiv.org/pdf/2402.17791)
* 

## 实验相关与经验

### 数值稳定性问题（repeat）

* [课程](https://www.bilibili.com/video/BV1u64y1i75a/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)（收获满满！）

* 要求自己手动复盘！（注意，文中这里的 $\mathbf{h}^{t}$ 看作的是**向量**而并非矩阵）

  * 熟练掌握反向传播的表达式子（会使用矩阵求导）

  * 理解梯度爆炸与梯度消失的原因以及带来的影响

  * 重点理解合理的权重初始化和激活函数的选取为什么能让数值稳定

    * 让每层的方差是一个常数可以让训练更加稳定（将每一层的输入和输出都看成一个**随机变量**）

      * > 如果每一层的输出方差太小或太大，梯度就会迅速变得非常小（梯度消失）或者非常大（梯度爆炸）。
        >
        > 例如，假设我们使用的是 Sigmoid 或 Tanh 激活函数，它们的梯度在输入非常大或非常小的时候会趋近于 0，这就会导致梯度消失问题，使得训练变慢，甚至停滞不前。
        >
        > 如果每一层的输出**方差保持恒定**（例如通过适当的初始化方法或者批归一化），那么每层的激活值和梯度**都在合理的范围内**，有助于避免梯度消失或爆炸问题，从而保证训练的稳定性。

      * 那么索性就不妨假设每层**输出**的期望为 $0$ ，这样做下去看看

        * 首先考虑**权重初始化**
        * 假设权重 $\mathbf{W}$ 各元素之间独立同分布，且权重 $\mathbf{W}$ 与 输入 $\mathbf{h}$ 独立，在假设每层 $\mathbf{h}$ 的方差都一样的情况下进行推演
        * 反向传播的均值和方差应该怎么计算

  * 思考，为什么**激活函数**最好应当在源点附近时为一个 `Identity function`，这样训练效果就好。`ReLU` 和 `tanh` **训练效果好可能与此有关**！ 
  
  * 最终我们可以得出结论：合理的权重初始值和激活函数的选取可以提升数值的稳定性
  
* 一些细节需要注意，进行反向传播分析的时候，要关注求导的那个目标可以“辐射”出哪些路径，这些路径上的点都是链式法则的一部分；此外，还要关注形状，形状很重要，不能想当然！
  $$
  \sum_{j}\mathbf{E} (\frac{\partial \mathcal{L}}{\partial {h^t_j}} \cdot \mathbf{W}^{t}_{j,i}) ^ {2}
  $$
  这里有一个小问题

  这该怎么计算呢？只有假设括号中的两项独立才能得到视频中的计算结果，但是感觉这两项不是独立的。GPT也认为由于梯度和权重是相关的，**假设独立并不成立**。

![数值稳定性1](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性1.JPG)

![数值稳定性2](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性2.JPG)

![数值稳定性3](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性3.JPG)

![数值稳定性4](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性4.JPG)

![数值稳定性5](C:\Users\5c\Desktop\Study Notes\pic\数值稳定性5.JPG)

<img src="C:\Users\5c\Desktop\Study Notes\pic\数值稳定性6.png" alt="数值稳定性6" style="zoom:50%;" />



### 批归一化与层归一化

* repeat

* 可以去看一下这个[视频](https://www.bilibili.com/video/BV1L2421N7jQ/?spm_id_from=333.337.search-card.all.click&vd_source=56ba8a8ec52809c81ce429c827dc30ab)

  * 视频里面的两道题做对了基本就懂了，后面还有题

    * 第一次看云里雾里，现在看已经悟了！
    * 第四个题错了。。。
      * 后来想透了，关键是把我**在哪个维度进行的操作**，然后观察**这一个批次/层次中有多少个“元素”**，元素的数量即为仿射变换参数的数量

  * 标准的 `normalization` 公式： $y = \frac{x- E[x]}{\sqrt{Var[x] + \varepsilon}} * \alpha + \beta$ 需要注意以下几点：

    * 在进行标准化之后还要进行**仿射变换**（这里的 $\alpha$ 与 $\beta$  是**可学习参数**）

    * 为什么标准化一般要**在激活函数**之前

      **缓解梯度消失/爆炸问题**： 标准化可以让每一层的输入分布具有相似的均值和方差（通常是均值为 0，方差为 1），这有助于避免梯度消失（对于 sigmoid、tanh 等激活函数）或梯度爆炸（对于 ReLU 等激活函数）问题。这使得反向传播时，梯度的传播更加稳定。

      **提高训练速度**： 标准化可以加速模型的收敛，因为它保持了每层输入的均衡，不会因为某些特征的分布不均而导致训练过程中的参数更新非常不稳定。通过规范化输入的分布，激活函数能够在一个合适的区域内工作，从而提高优化效率。

      **激活函数的性质**： 例如，ReLU 激活函数对于输入为负数的值输出为零，而对于正数则线性增长。如果输入的分布没有经过标准化，可能会导致很多激活输出为零（死亡神经元问题）。标准化保证了激活函数的输入分布有一定的均衡，从而使得神经网络能够在训练中更好地发挥作用。

    * 标准化一般在**激活函数之前**，**若不经过仿射变化**，那么我们的数据分布在均值为0，方差为1的区域，而**在这个区域，数据一般是很线性的**，这样的话激活函数就无法对其起到非线性的作用

    * 标准化是在执行过程中动态地调整数据分布，使得训练效果更好，直接针对输出/输入的数据；而权重初始化则是针对网络的权重

  * 自己实现一下，其中 $\varepsilon$ 取 `1e-05`，$\alpha$ 初始值设为 1， $\beta$ 初值设为 0， 无偏设置为 False

    * 有一些小细节需要格外注意与理解，比如 

      ==`torch.mean(x, dim = [0, 1, 2])` 和 `torch.mean(x, dim = -1)` 是不一样的！！！==（知识又被加深了！！！）

* 自己的理解是对的

  > 对于形状为 $$(B, H, N, D)$$ 的数据，批归一化和层归一化的作用维度是不同的。
  >
  > 1. **批归一化（Batch Normalization）**
  >
  > 批归一化是作用于 **批次维度**（通常是第一个维度，即 $$B$$），它计算的是 **整个批次（$$B$$）上每个特征的均值和方差**。因此，对于形状为 $$(B, H, N, D)$$ 的数据，批归一化会对 $$B$$ 维度上的样本进行归一化。
  >
  > - **维度作用：** `dim=0`，即对 $$B$$ 维度进行归一化。
  >
  > **例子：** 如果我们有一个形状为 $$(B, H, N, D)$$ 的数据集，批归一化会对每个 $$H, N, D$$ 的特征进行标准化，使得每个批次（$$B$$）中的每个样本的特征（$$D$$）都被归一化。
  >
  > 2. **层归一化（Layer Normalization）**
  >
  > 层归一化是作用于 **特征维度**（通常是最后一个维度 $$D$$），它对每个样本在该层的所有特征进行归一化。因此，在形状为 $$(B, H, N, D)$$ 的数据中，层归一化会对每个样本（$$B$$）在每个位置（$$H, N$$）的所有特征（$$D$$）进行归一化。
  >
  > - **维度作用：** `dim=-1`，即对 $$D$$ 维度进行归一化。
  >
  > **例子：** 对于 $$(B, H, N, D)$$ 的数据，层归一化会对每个样本的每个位置（$$H, N$$）的特征维度（$$D$$）进行标准化，而与批次维度 $$B$$ 无关。
  >
  > 总结：
  >
  > - **批归一化**：作用在 $$B$$ 维度上，即 `dim=0`。
  > - **层归一化**：作用在 $$D$$ 维度上，即 `dim=-1`。
  >
  > 这就是批归一化和层归一化在不同维度上的应用方式。

* 自己手写去感受一下（repeat）

### The Learning-To-Rank (LTR) loss

* 这个损失通常作用在与排名相关的任务中，与数值是否吻合相比，更加关注排名情况。

* 在 `RGTN` 这篇论文中，LTR损失与RMSE损失互相结合使用。前者衡量（相对排名，如果排错那么给的惩罚力度会比较大），后者衡量数据的拟合程度（有一点需要注意，越吻合不一定排名也越吻合）

* 计算方式其实就是分别对预测值和真实值使用 $\text{softmax}$，然后对结果使用做交叉熵运算。
  $$
  s'_v = \frac{\exp(s_v)}{\sum_{j \in \mathcal{N}_v^{(r)}} \exp(s_j)}, \quad
  s_v^{*'} = \frac{\exp(s_v^*)}{\sum_{j \in \mathcal{N}_v^{(r)}} \exp(s_j^*)},
  $$

  $$
  \mathcal{L}_v^{(r)} = - \sum_{i \in \mathcal{N}_v^{(r)}} s_i' \log(s_i^{*'})
  $$
  
  其中 $s'_v$ 是真实值，$s^{*}_v$ 是预测值。我们可以来看一下效果
  
  ```python
  def LTRloss(y_pred, y_true, eps = 1e-10):
      y_pred = F.softmax(y_pred, dim = -1)
      y_true = F.softmax(y_true, dim = -1)
      log_pred = torch.log(y_pred + eps)
      loss = -torch.sum(log_pred * y_true, dim = -1)
      return loss
  
  mse_loss = nn.MSELoss()
  y_true = torch.tensor([3., 2., 1.])
  y_pred1 = torch.tensor([2.5, 2.9, 1.1]) # (前两项排位错误，但是rmse应该较小)
  
  loss1 = mse_loss(y_pred1, y_true)
  loss2 = LTRloss(y_pred1, y_true)
  print(f'mse_loss is {loss1:.4f}')
  print(f'LTR loss is {loss2:.4f}')
  print(f'total loss is {(loss1 + loss2):.4f}')
  
  '''
  mse_loss is 0.3567
  LTR loss is 1.0355
  total loss is 1.3922
  '''
  ```
  
  ```python
  y_pred2 = torch.tensor([3.5, 2.9, 1.1])   # (排名全部正确，rmse一致)
  loss1 = mse_loss(y_pred2, y_true)
  loss2 = LTRloss(y_pred2, y_true)
  print(f'mse_loss is {loss1:.4f}')
  print(f'LTR loss is {loss2:.4f}')
  print(f'total loss is {(loss1 + loss2):.4f}')
  
  '''
  mse_loss is 0.3567
  LTR loss is 0.8573
  total loss is 1.2140
  '''
  ```
  
  可以发现，在 rmse 相同的情况下，**排名错误的损失要大**，且其**惩罚力度明显要高于** mse_loss。
  
  
  
  但是根据后续的实验我们可以发现，如果数值比较大的情况下 LTR损失的作用就比较小了，尽管有排序错误。
  
  ```python
  def LTRloss(y_pred, y_true, eps = 1e-10):
      y_pred = F.softmax(y_pred, dim = -1)
      y_true = F.softmax(y_true, dim = -1)
      log_pred = torch.log(y_pred + eps)
      loss = -torch.sum(log_pred * y_true, dim = -1)
      return loss
  
  mse_loss = nn.MSELoss()
  y_true = torch.tensor([3., 2., 1.]) * 10
  y_pred1 = torch.tensor([2.5, 2.9, 1.1]) * 10 # (前两项排位错误，但是数值比较大)
  
  loss1 = mse_loss(y_pred1, y_true)
  loss2 = LTRloss(y_pred1, y_true)
  print(f'mse_loss is {loss1:.4f}')
  print(f'LTR loss is {loss2:.4f}')
  print(f'total loss is {(loss1 + loss2):.4f}')
  
  '''
  mse_loss is 35.6667
  LTR loss is 4.0180
  total loss is 39.6846
  '''
  ```
  
  这种情况下可能起不到纠正排名的错误。可能无法达到**跳出GNN节点嵌入的局部性，从而从全局角度学习节点重要性的分布的效果**。
  
  这是因为 Softmax 的 “数值极化” 的效应。因为 softmax 本质上是对指数数据进行归一化，显然当 logits 中的值大于 1 时，经过 $\exp$ 的处理，数值就会被放得很大。尤其时对最大值来说，$\text{softmax}$ 会将最大值放大为几乎为1，其他压缩为0！
  
  例子如下：
  
  ```python
  y_pred2 = torch.tensor([25., 29., 11.])
  res = F.softmax(y_pred2, dim=-1)
  
  print([f'{x:.4f}' for x in res.numpy()])
  '''
  ['0.0180', '0.9820', '0.0000']
  '''
  ```
  
  可以发现确实如此。也就是说，**预测排名分布已经非常“自信”了**：它确信第二个是第一，其他都是垃圾。可事实上却是第一个是第一。由于 softmax 之后**变成了接近 one-hot 的分布，交叉熵就不再“纠正”得动了**。所以此时我们的 `LTRloss` 在这种高数值下变得“迟钝”，甚至 **失效**，损失值变化不明显。
  
  当我们画图的话，会发现：
  
  - MSE 是平滑上升的：分数越偏离，loss 越大。
  - **LTRloss 是非线性拐点型的：一旦 softmax 输出接近 one-hot，loss 就变得极小**。（可能有误）
  
  那么有没有什么解决办法呢？
  
  有的，兄弟，自然是有的。对输入 $\text{softmax}$ 前进行归一化/标准化。（现在终于理解了）。进行归一化不仅是为了所谓的数值稳定性，它对损失函数的“敏感性”和“优化方向”也有直接影响。我们可以从两个角度来理解这个：
  
  > **1. 数值稳定性（传统理解）**
  >
  > 在 deep learning 中，归一化常用于防止数值爆炸，特别是：
  >
  > - 输入进入 softmax 前范围太大时，容易导致 softmax 输出接近 one-hot（梯度变稀疏）；
  > - 非归一化输入可能让某些损失函数（如交叉熵、KL、LTR loss）结果非常大或非常小，不利于收敛（梯度爆炸或者梯度消失）；
  > - 防止 `torch.log(y_pred)` 中的 `y_pred` 太小导致 `NaN` 或 `-inf`。
  >
  > ------
  >
  > **2. 对损失值和优化过程的影响（关键）**
  >
  > 💡 以 LTRloss 为例（你用的是 KL/交叉熵变体）：
  >
  > ```python
  > def LTRloss(y_pred, y_true):
  >     y_pred = F.softmax(y_pred, dim=-1)
  >     y_true = F.softmax(y_true, dim=-1)
  >     log_pred = torch.log(y_pred + eps)
  >     loss = -torch.sum(log_pred * y_true)
  > ```
  >
  > 当 `y_pred` 和 `y_true` 原始值范围非常大时，比如都乘了个 10，虽然排序没变，但：
  >
  > - softmax 输出变得更尖锐（接近 one-hot）
  > - 损失函数对“排序错误”的容忍变高，导致梯度更小
  > - 训练会**更不关注排序错误，只关注最大值位置对没对**
  >
  >  **也就是说：**
  >  在没有归一化的情况下，如果你 `y_pred = [30, 20, 10]` 和 `y_true = [20, 30, 10]`，虽然排序不一样，softmax 后都变得非常极端（接近 one-hot），KL loss 也很小，**LTRloss 就体现不出排序的错误程度**。
